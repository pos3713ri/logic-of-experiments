% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{scrbook}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={The Logic of Experiments},
  pdfauthor={Carlisle Rainey},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{The Logic of Experiments}
\author{Carlisle Rainey}
\date{}

\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

These are notes on the logic of experiments.

Understanding the logic of experiments helps one clearly understand:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{why randomization is powerful} and
\item
  \emph{what randomization \textbf{does} and \textbf{does not} get you}.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{ATE}\label{ate}

In this chapter, we define the \textbf{average treatment effect} or the
\textbf{ATE}. This is the key quantity of interest that we want to
estimate with an experiment. But to define (and understand) the ATE, we
need to define several preliminary concepts.

\section{Potential Outcomes}\label{potential-outcomes}

The fundamental problem of causal inference is that we can never observe
what \emph{would have happened} to the same individual if we had instead
assigned them to a different condition. We can observe the outcome
under---at most---one condition.

To formalize this, we use the \textbf{potential outcomes framework}.

For each individual \(i\) in our study, we define two potential
outcomes:

\begin{itemize}
\tightlist
\item
  \(Y_i(1)\): the outcome individual \(i\) \emph{would} experience if
  assigned to treatment (\(D_i = 1\))
\item
  \(Y_i(0)\): the outcome individual \(i\) \emph{would} experience if
  assigned to control (\(D_i = 0\))
\end{itemize}

These are called ``potential'' outcomes because we can observe no more
than one of them for any given individual. If individual \(i\) receives
treatment, we observe \(Y_i(1)\) but not \(Y_i(0)\). If individual \(i\)
is in control, we observe \(Y_i(0)\) but not \(Y_i(1)\).

The unobserved potential outcome for each individual is called the
\textbf{counterfactual}. The counterfactual represents what \emph{would
have} happened in the hypothetical scenario in which the individual was
assigned to a \emph{different condition}.

To make this concrete, consider a hypothetical experiment with 10
respondents. The table below shows both potential outcomes for each
respondent---something we could never actually observe in reality, but
which helps us understand the framework:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 \\
2 & 6 & 5 \\
3 & 5 & 5 \\
4 & 3 & 9 \\
5 & 7 & 10 \\
6 & 5 & 4 \\
7 & 4 & 8 \\
8 & 6 & 7 \\
9 & 5 & 3 \\
10 & 4 & 6 \\
\end{longtable}

Importantly, though, we cannot observe both potential outcomes. But for
now, let's ignore that difficulty.

\section{The Individual-Level Treatment
Effect}\label{the-individual-level-treatment-effect}

Once we have defined potential outcomes, we can define the
\textbf{individual-level treatment effect} for individual \(i\) as
\(\tau_i = Y_i(1) - Y_i(0)\).

This represents the causal effect of treatment for that specific
individual. It's simply the difference between what would happen if they
were assigned to treatment versus what would happen if they were
assigned to control.

If we knew all the potential outcomes, then we could compute the
individual-level treatment effect for each respondent.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 & 7 - 4 = 3 \\
2 & 6 & 5 & 5 - 6 = -1 \\
3 & 5 & 5 & 5 - 5 = 0 \\
4 & 3 & 9 & 9 - 3 = 6 \\
5 & 7 & 10 & 10 - 7 = 3 \\
6 & 5 & 4 & 4 - 5 = -1 \\
7 & 4 & 8 & 8 - 4 = 4 \\
8 & 6 & 7 & 7 - 6 = 1 \\
9 & 5 & 3 & 3 - 5 = -2 \\
10 & 4 & 6 & 6 - 4 = 2 \\
\end{longtable}

Again, there's a fundamental problem: we can never observe \(\tau_i\)
directly because we never observe both potential outcomes for the same
individual. This is sometimes called the \textbf{fundamental problem of
causal inference}. We observe either \(Y_i(1)\) or \(Y_i(0)\), but never
both. But again, let's continue to ignore this pesky problem.

\section{The Average Treatment
Effect}\label{the-average-treatment-effect}

We already know that we cannot observe the individual-level treatment
effects. So instead, let's cleverly \emph{sidestep} the problem. Let's
define the \textbf{average treatment effect} or \textbf{ATE}, defined as

\[
\begin{align}
\text{ATE} &= \frac{1}{N}\sum_{i=1}^{N} [Y_i(1) - Y_i(0)] \\
&= \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) \\
&= \text{avg} \left( Y_i(1) \right) - \text{avg} \left( Y_i(0) \right) \\ 
&= \text{avg. of ALL p.o. under treatment } - \text{avg. of ALL p.o. under control}
\end{align}
\]

The ATE represents the average causal effect of treatment across all
individuals in the population. It tells us: on average, how much does
treatment change outcomes?

Here's another way to think about. Suppose we assigned \emph{everyone}
to treatment and computed the average outcome. Suppose we also assigned
\emph{everyone} to control and computed the average outcome. (We can't
assign \emph{everyone} to treatment \emph{and everyone} to control, but
suppose we could). The ATE is the difference between this two average.

Again, we cannot observe the ATE, but it turns out that this quantity
can be estimated. In later sections, we'll show how we can use
randomized experiments to estimate the ATE. But for now, let's focus on
the meaning of the concept.

To make this concrete, we can compute the ATE for the potential outcomes
above.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 \\
2 & 6 & 5 \\
3 & 5 & 5 \\
4 & 3 & 9 \\
5 & 7 & 10 \\
6 & 5 & 4 \\
7 & 4 & 8 \\
8 & 6 & 7 \\
9 & 5 & 3 \\
10 & 4 & 6 \\
\end{longtable}

Using this table, we can compute the ATE by finding the average of all
potential outcomes under control and the average of all potential
outcomes under treatment:

\[
\begin{align}
\text{ATE} &= \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) \\
&= \frac{1}{10}(7 + 5 + 5 + 9 + 10 + 4 + 8 + 7 + 3 + 6) - \frac{1}{10}(4 + 6 + 5 + 3 + 7 + 5 + 4 + 6 + 5 + 4) \\
&= \frac{64}{10} - \frac{49}{10} \\
&= 6.4 - 4.9 \\
&= 1.5
\end{align}
\] The ATE is 1.5, meaning that \emph{on average}, treatment increases
outcomes by 1.5 units. (We could average the \(\tau_i\) in the table
above to obtain the identical value.)

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, breakable, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

When we write about the ``average treatment effect,'' it should be
understood as something very specific. It should be understood as the
difference between two quantities: the hypothetical difference between
(1) the average value of the outcome if we assigned \emph{everyone} to
treatment and (2) the average value of the outcome if we assigned
\emph{everyone} to control.

\section{Special Cases}\label{special-cases}

\subsection{The Sharp Null Hypothesis}\label{the-sharp-null-hypothesis}

The \textbf{sharp null hypothesis} is a particularly strong claim about
individual treatment effects. The sharp null hypothesis states that

\[\text{sharp null hypothesis}: Y_i(1) = Y_i(0) \text{ for all } i.\]

This hypothesis states that treatment has \emph{exactly zero effect} for
every single individual in the study. That is, for each individual in
the experiment, the outcome is identical regardless of whether it is
assigned to treatment or control. Under the sharp null, the treatment
effect \(\tau_i = 0\) for all \(i\).

This hypothesis is helpful because it completely specifies all potential
outcomes. If we assume the sharp null hypothesis holds, then we can
compute the counterfactual outcome from the observed outcome.

\begin{itemize}
\tightlist
\item
  Suppose we observe \(Y_i(0)\) for a control individual, we know that
  \(Y_i(1) = Y_i(0)\).
\item
  Similarly, suppose we observe \(Y_i(1)\) for a treated individual, we
  know that \(Y_i(0) = Y_i(1)\).
\end{itemize}

While we never know whether the sharp null hypothesis holds---and it
seems implausible that it \emph{ever} would hold---it can sometimes be
convenient to check what would happen \emph{if it did hold}.

The table below shows a set of potential outcomes where the sharp null
holds.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 4 & 4 - 4 = 0 \\
2 & 6 & 6 & 6 - 6 = 0 \\
3 & 5 & 5 & 5 - 5 = 0 \\
4 & 3 & 3 & 3 - 3 = 0 \\
5 & 7 & 7 & 7 - 7 = 0 \\
6 & 5 & 5 & 5 - 5 = 0 \\
7 & 4 & 4 & 4 - 4 = 0 \\
8 & 6 & 6 & 6 - 6 = 0 \\
9 & 5 & 5 & 5 - 5 = 0 \\
10 & 4 & 4 & 4 - 4 = 0 \\
\end{longtable}

Under the sharp null hypothesis, \(Y_i(1) = Y_i(0)\) for all individuals
\(i\), which means \(\tau_i = 0\) for all individuals. We can verify
this by computing the ATE:

\[
\begin{align}
\text{ATE} &= \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) \\
&= \frac{1}{10}(4 + 6 + 5 + 3 + 7 + 5 + 4 + 6 + 5 + 4) - \frac{1}{10}(4 + 6 + 5 + 3 + 7 + 5 + 4 + 6 + 5 + 4) \\
&= \frac{49}{10} - \frac{49}{10} \\
&= 4.9 - 4.9 \\
&= 0
\end{align}
\]

The ATE is exactly 0. This follows from the fact that all of the
individual-level effects are exactly zero as well.

\subsection{Constant Treatment
Effects}\label{constant-treatment-effects}

Another implausible but sometimes-helpful hypothesis is the
\textbf{constant treatment effects} assumption that

\[\tau_i = \tau \text{ for all individuals } i\]

This assumes that while the individual-level treatment effect may be
different from zero, that effect is \emph{exactly the same} for every
individual so that \[Y_i(1) - Y_i(0) = \tau \text{ for all } i\]. Here,
\(\tau\) is a fixed number like 2 or -4.2 that is the same for all the
individuals.

As before, we never know whether individual-level treatment effects are
constant---and it seems implausible they \emph{ever} would be. But it
can sometimes be convenient to check what would happen \emph{if they
were constant}.

Similar to the sharp null hypothesis, the constant effects hypothesis is
helpful because it completely specifies all potential outcomes. It
allows us to compute the counterfactual outcome from the observed
outcome.

\begin{itemize}
\tightlist
\item
  Suppose we observe \(Y_i(0)\) for a control individual, we know that
  \(Y_i(1)\) is 2 points larger than that so that
  \(Y_i(1) = Y_i(0) + 2\).
\item
  Similarly, suppose we observe \(Y_i(1)\) for a treated individual, we
  know that \(Y_i(0)\) is 2 points less than that so that
  \(Y_i(0) = Y_i(1) - 2\).
\end{itemize}

The table below shows a set of potential outcomes where treatment
effects are constant at \(\tau = 2\) for all individuals.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 6 & 6 - 4 = 2 \\
2 & 6 & 8 & 8 - 6 = 2 \\
3 & 5 & 7 & 7 - 5 = 2 \\
4 & 3 & 5 & 5 - 3 = 2 \\
5 & 7 & 9 & 9 - 7 = 2 \\
6 & 5 & 7 & 7 - 5 = 2 \\
7 & 4 & 6 & 6 - 4 = 2 \\
8 & 6 & 8 & 8 - 6 = 2 \\
9 & 5 & 7 & 7 - 5 = 2 \\
10 & 4 & 6 & 6 - 4 = 2 \\
\end{longtable}

\[
\begin{align}
\text{ATE} &= \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) \\
&= \frac{1}{10}(6 + 8 + 7 + 5 + 9 + 7 + 6 + 8 + 7 + 6) - \frac{1}{10}(4 + 6 + 5 + 3 + 7 + 5 + 4 + 6 + 5 + 4) \\
&= \frac{69}{10} - \frac{49}{10} \\
&= 6.9 - 4.9 \\
&= 2
\end{align}
\]

The ATE is exactly 2. This follows from the fact that all of the
individual-level effects are exactly 2 as well. When treatment effects
are constant, the ATE perfectly captures the treatment effect for every
individual.

\section{Rasinski's Question-Wording
Experiment}\label{rasinskis-question-wording-experiment}

In a famous survey experiment, Rasinski (1989) was curious about how
question wording affects public support for government
spending.\footnotemark{}

Respondents could be assigned to one of two conditions: treatment or
control. The two conditions featured an identical question stem, but
varied in how they describe the spending.

\textbf{Question stem} (identical for both conditions): \emph{``Are we
spending too much, too little, or about the right amount on\ldots{}''}

\textbf{Treatment condition}: \emph{``\ldots assistance to the poor?''}

\textbf{Control condition}: \emph{``\ldots welfare?''}

\textbf{Response options}:

\begin{itemize}
\tightlist
\item
  Too little = 1
\item
  About right = 2
\item
  Too much = 3
\end{itemize}

Thus each respondent had two potential outcomes:

\begin{itemize}
\tightlist
\item
  \(Y_i(1)\): the response respondent \(i\) \emph{would} give if
  assigned to the treatment condition (``assistance to the poor'')
\item
  \(Y_i(0)\): the response respondent \(i\) \emph{would} give if
  assigned to the control condition (``welfare'')
\end{itemize}

As before, we can never observe both potential outcomes for the same
respondent. We can observe their response if the program is described as
``assistance to the poor'' or as ``welfare'', but \emph{not both}.

\subsection{Hypothetical Potential
Outcomes}\label{hypothetical-potential-outcomes}

To illustrate the concepts, suppose we could magically observe both
potential outcomes for a sample of 10 respondents. The table below shows
what each respondent would answer under each condition:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3016}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3492}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1587}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Respondent
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(0)\) (``welfare'')
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(1)\) (``assistance'')
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tau_i\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 3 (Too much) & 1 (Too little) & 1 - 3 = -2 \\
2 & 3 (Too much) & 2 (About right) & 2 - 3 = -1 \\
3 & 2 (About right) & 1 (Too little) & 1 - 2 = -1 \\
4 & 3 (Too much) & 1 (Too little) & 1 - 3 = -2 \\
5 & 2 (About right) & 1 (Too little) & 1 - 2 = -1 \\
6 & 3 (Too much) & 2 (About right) & 2 - 3 = -1 \\
7 & 2 (About right) & 1 (Too little) & 1 - 2 = -1 \\
8 & 3 (Too much) & 1 (Too little) & 1 - 3 = -2 \\
9 & 2 (About right) & 2 (About right) & 2 - 2 = 0 \\
10 & 3 (Too much) & 1 (Too little) & 1 - 3 = -2 \\
\end{longtable}

Notice that for most respondents, the treatment effect \(\tau_i\) is
negative. This means that asking about ``assistance to the poor''
produces \emph{lower} numeric responses than asking about ``welfare.''
In this case, lower numbers indicate more support for spending (``too
little'' = 1), these negative treatment effects actually represent
\emph{increased support} for spending when using the ``assistance to the
poor'' wording.

\subsection{Computing the ATE}\label{computing-the-ate}

We can compute the ATE for this hypothetical data:

\[
\begin{align}
\text{ATE} &= \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) \\
&= \frac{1}{10}(1 + 2 + 1 + 1 + 1 + 2 + 1 + 1 + 2 + 1) - \frac{1}{10}(3 + 3 + 2 + 3 + 2 + 3 + 2 + 3 + 2 + 3) \\
&= \frac{13}{10} - \frac{26}{10} \\
&= 1.3 - 2.6 \\
&= -1.3
\end{align}
\]

The ATE is -1.3. Since lower values on our 1-2-3 scale indicate more
support for spending, this negative ATE means that the ``assistance to
the poor'' wording increases support for spending by an average of 1.3
scale points compared to the ``welfare'' wording.

\section{Key Terms}\label{key-terms}

\begin{itemize}
\tightlist
\item
  Potential outcomes
\item
  Potential outcomes framework
\item
  Counterfactual
\item
  Individual-level treatment effect
\item
  Fundamental problem of causal inference
\item
  Average treatment effect (ATE)
\item
  Sharp null hypothesis
\item
  Constant treatment effects
\end{itemize}

\section{Exercises}\label{exercises}

\subsection{Conceptual Understanding}\label{conceptual-understanding}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Defining Potential Outcomes}: In your own words, explain what
  \(Y_i(1)\) and \(Y_i(0)\) represent in the context of Rasinski's
  question-wording experiment. Why can we never observe both potential
  outcomes for the same respondent?
\end{enumerate}

Solution

In Rasinski's question-wording experiment, each respondent has two
potential outcomes:

\begin{itemize}
\item
  \(Y_i(1)\) is the response that respondent \(i\) \emph{would} give if
  asked about spending on ``assistance to the poor.'' This is the
  potential outcome under the treatment condition.
\item
  \(Y_i(0)\) is the response that respondent \(i\) \emph{would} give if
  asked about spending on ``welfare.'' This is the potential outcome
  under the control condition.
\end{itemize}

We can never observe both potential outcomes for the same respondent
because each respondent can only answer \emph{one version} of the
question. Once respondent \(i\) sees the question with ``welfare'' and
gives their answer, we cannot rewind time and show them the ``assistance
to the poor'' version as if they had never seen the first one. At most,
we observe one potential outcome per respondent---either \(Y_i(1)\) or
\(Y_i(0)\), but never both.

This is the fundamental problem of causal inference.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{The Fundamental Problem}: Explain why we call it the
  ``fundamental problem of causal inference.'' What makes this problem
  ``fundamental'' rather than just ``difficult''?
\end{enumerate}

Solution

We call it the ``fundamental'' problem of causal inference---rather than
just a ``difficult'' problem---because it cannot be solved, even in
principle.

A ``difficult'' problem is one we might overcome with better technology,
more data, or cleverer methods. The fundamental problem is different: it
is a logical impossibility. At any given moment, an individual either
receives treatment or does not. We cannot observe the same person in two
different states of the world at the same time. No amount of resources
or ingenuity can change this fact.

This is why we need research designs---like randomized
experiments---that allow us to \emph{estimate} causal effects at the
group level, even though we can never directly \emph{observe}
individual-level causal effects.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{ATE Interpretation}: A researcher finds an ATE of 3.2 in an
  experiment testing the effect of a new study technique on exam scores
  (measured 0-100). Write two sentences explaining what this means in
  plain language that a non-statistician could understand.
\end{enumerate}

Solution

An ATE of 3.2 means that, on average, students who use the new study
technique score about 3.2 points higher on the exam compared to what
they would have scored without using the technique.

Another way to say this: if we could magically give the study technique
to all students and then also observe what would have happened if none
of them used it, the average difference between these two scenarios
would be 3.2 points.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Sharp Null vs.~Zero ATE}: Is it possible to have an ATE of
  zero even when the sharp null hypothesis does not hold? Explain your
  answer and provide an example using a simple table of potential
  outcomes for 4 individuals.
\end{enumerate}

Solution

Yes, it is possible to have an ATE of zero even when the sharp null
hypothesis does not hold.

The sharp null hypothesis is a strong claim: it requires that
\(\tau_i = 0\) for \emph{every single individual}. That is,
\(Y_i(1) = Y_i(0)\) for all \(i\).

The ATE, on the other hand, is just the \emph{average} of the individual
treatment effects:

\[\text{ATE} = \frac{1}{N} \sum_{i=1}^{N} \tau_i\]

If some individuals have positive treatment effects and others have
negative treatment effects, these can cancel out, producing an ATE of
zero even though the treatment affected every individual.

Here is a concrete example with 4 individuals:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Individual & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i = Y_i(1) - Y_i(0)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 5 & 8 & \(8 - 5 = 3\) \\
2 & 7 & 4 & \(4 - 7 = -3\) \\
3 & 6 & 8 & \(8 - 6 = 2\) \\
4 & 8 & 6 & \(6 - 8 = -2\) \\
\end{longtable}

Computing the ATE:

\[
\begin{align}
\text{ATE} &= \frac{1}{4} \sum_{i=1}^{4} \tau_i \\
&= \frac{1}{4}(3 + (-3) + 2 + (-2)) \\
&= \frac{0}{4} \\
&= 0
\end{align}
\]

The ATE equals zero, but the sharp null does \emph{not} hold. The
treatment affected every single individual---individuals 1 and 3
experienced positive effects of 3 and 2, while individuals 2 and 4
experienced negative effects of -3 and -2. The effects simply cancel out
when we average them.

\subsection{Computational Practice}\label{computational-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Computing Individual Treatment Effects}: Consider the
  following potential outcomes for 6 respondents in a hypothetical
  experiment:

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Respondent & \(Y_i(0)\) & \(Y_i(1)\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 5 & 8 \\
  2 & 7 & 6 \\
  3 & 4 & 7 \\
  4 & 6 & 9 \\
  5 & 5 & 5 \\
  6 & 8 & 7 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Calculate the individual-level treatment effect (\(\tau_i\)) for
    each respondent.
  \item
    Calculate the ATE.
  \item
    Which respondents experienced a positive treatment effect? A
    negative treatment effect? No treatment effect?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} To find each individual treatment effect, we compute
\(\tau_i = Y_i(1) - Y_i(0)\):

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i = Y_i(1) - Y_i(0)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 5 & 8 & \(8 - 5 = 3\) \\
2 & 7 & 6 & \(6 - 7 = -1\) \\
3 & 4 & 7 & \(7 - 4 = 3\) \\
4 & 6 & 9 & \(9 - 6 = 3\) \\
5 & 5 & 5 & \(5 - 5 = 0\) \\
6 & 8 & 7 & \(7 - 8 = -1\) \\
\end{longtable}

\textbf{Part b.} We can compute the ATE by averaging the individual
treatment effects:

\[
\begin{align}
\text{ATE} &= \frac{1}{6} \sum_{i=1}^{6} \tau_i \\
&= \frac{1}{6}(3 + (-1) + 3 + 3 + 0 + (-1)) \\
&= \frac{7}{6} \\
&\approx 1.167
\end{align}
\]

We can verify this by computing the ATE as the difference between the
average potential outcome under treatment and the average potential
outcome under control:

\[
\begin{align}
\text{ATE} &= \frac{1}{6}\sum_{i=1}^{6} Y_i(1) - \frac{1}{6}\sum_{i=1}^{6} Y_i(0) \\
&= \frac{1}{6}(8 + 6 + 7 + 9 + 5 + 7) - \frac{1}{6}(5 + 7 + 4 + 6 + 5 + 8) \\
&= \frac{42}{6} - \frac{35}{6} \\
&= 7 - 5.833 \\
&\approx 1.167
\end{align}
\]

Both methods give the same answer.

\textbf{Part c.} Classifying the treatment effects:

\begin{itemize}
\tightlist
\item
  \textbf{Positive treatment effect} (\(\tau_i > 0\)): Respondents 1, 3,
  and 4 (with effects of 3, 3, and 3)
\item
  \textbf{Negative treatment effect} (\(\tau_i < 0\)): Respondents 2 and
  6 (with effects of -1 and -1)
\item
  \textbf{No treatment effect} (\(\tau_i = 0\)): Respondent 5
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Verifying the Sharp Null}: Consider the following potential
  outcomes:

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Respondent & \(Y_i(0)\) & \(Y_i(1)\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 12 & 12 \\
  2 & 8 & 8 \\
  3 & 15 & 15 \\
  4 & 10 & 10 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Does this satisfy the sharp null hypothesis? How do you know?
  \item
    Calculate the ATE.
  \item
    Under the sharp null hypothesis, what can you always say about the
    ATE?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Yes, this satisfies the sharp null hypothesis.

The sharp null hypothesis requires that \(Y_i(1) = Y_i(0)\) for
\emph{all} individuals. We can check each respondent:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(Y_i(1) = Y_i(0)\)? \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 12 & 12 & Yes \\
2 & 8 & 8 & Yes \\
3 & 15 & 15 & Yes \\
4 & 10 & 10 & Yes \\
\end{longtable}

For every individual, the potential outcome under treatment equals the
potential outcome under control. The sharp null hypothesis holds.

\textbf{Part b.} Computing the ATE:

\[
\begin{align}
\text{ATE} &= \frac{1}{4}\sum_{i=1}^{4} Y_i(1) - \frac{1}{4}\sum_{i=1}^{4} Y_i(0) \\
&= \frac{1}{4}(12 + 8 + 15 + 10) - \frac{1}{4}(12 + 8 + 15 + 10) \\
&= \frac{45}{4} - \frac{45}{4} \\
&= 11.25 - 11.25 \\
&= 0
\end{align}
\]

\textbf{Part c.} Under the sharp null hypothesis, the ATE is
\emph{always} exactly zero.

This follows directly from the definitions. If \(Y_i(1) = Y_i(0)\) for
all \(i\), then:

\[\tau_i = Y_i(1) - Y_i(0) = 0 \text{ for all } i\]

The ATE is the average of the individual treatment effects:

\[\text{ATE} = \frac{1}{N} \sum_{i=1}^{N} \tau_i = \frac{1}{N} \sum_{i=1}^{N} 0 = 0\]

The average of zeros is always zero.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Constant Treatment Effects}: Create a table of potential
  outcomes for 5 individuals where the constant treatment effects
  assumption holds with \(\tau = -2.5\) for all individuals. Show that
  the ATE equals -2.5.
\end{enumerate}

Solution

The constant treatment effects assumption requires that
\(\tau_i = \tau\) for all individuals. With \(\tau = -2.5\), we need
\(Y_i(1) - Y_i(0) = -2.5\) for every individual.

We can choose any values for \(Y_i(0)\) and then compute
\(Y_i(1) = Y_i(0) + (-2.5) = Y_i(0) - 2.5\):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3733}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Individual
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(1) = Y_i(0) - 2.5\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tau_i = Y_i(1) - Y_i(0)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 10 & 7.5 & \(7.5 - 10 = -2.5\) \\
2 & 8 & 5.5 & \(5.5 - 8 = -2.5\) \\
3 & 12 & 9.5 & \(9.5 - 12 = -2.5\) \\
4 & 6 & 3.5 & \(3.5 - 6 = -2.5\) \\
5 & 9 & 6.5 & \(6.5 - 9 = -2.5\) \\
\end{longtable}

Now we verify that the ATE equals -2.5:

\[
\begin{align}
\text{ATE} &= \frac{1}{5}\sum_{i=1}^{5} Y_i(1) - \frac{1}{5}\sum_{i=1}^{5} Y_i(0) \\
&= \frac{1}{5}(7.5 + 5.5 + 9.5 + 3.5 + 6.5) - \frac{1}{5}(10 + 8 + 12 + 6 + 9) \\
&= \frac{32.5}{5} - \frac{45}{5} \\
&= 6.5 - 9 \\
&= -2.5
\end{align}
\]

When treatment effects are constant at \(\tau\) for all individuals, the
ATE equals \(\tau\). This makes sense: the ATE is the average of the
individual treatment effects, and the average of a constant is just that
constant.

\subsection{Application to Rasinski's
Experiment}\label{application-to-rasinskis-experiment}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Understanding the Conditions}:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    In Rasinski's experiment, what exactly differs between the treatment
    and control conditions? What stays the same?
  \item
    Why might the word ``welfare'' produce different responses than
    ``assistance to the poor'' even though both refer to the same
    government programs?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.}

\emph{What differs between the conditions}: Only the label used to
describe the government programs. The treatment condition uses
``assistance to the poor'' while the control condition uses ``welfare.''

\emph{What stays the same}: Everything else is identical:

\begin{itemize}
\tightlist
\item
  The question stem: ``Are we spending too much, too little, or about
  the right amount on\ldots{}''
\item
  The response options: Too little, About right, Too much
\item
  The numeric coding: 1, 2, 3
\item
  The underlying concept being asked about (government spending on
  programs for low-income people)
\end{itemize}

\textbf{Part b.} The words ``welfare'' and ``assistance to the poor''
may produce different responses because they carry different
connotations, even though they refer to the same government programs.

``Welfare'' has acquired negative associations in American political
discourse. It is often linked to stereotypes about dependency, fraud, or
undeserving recipients. Many people have strong negative reactions to
the word itself.

``Assistance to the poor'' is more neutral and descriptive. It focuses
attention on the humanitarian purpose of helping people in need, without
triggering the same negative associations.

These different framings can activate different considerations in
respondents' minds, leading them to express different levels of support
even though the underlying programs are identical. This is exactly what
Rasinski's experiment was designed to detect.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  \textbf{Interpreting Rasinski's Results}: In the hypothetical Rasinski
  data presented in the notes, the ATE was -1.3.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Explain what this negative ATE means substantively. Does it indicate
    increased or decreased support for spending?
  \item
    Why is the sign (positive or negative) of the ATE potentially
    confusing in this example?
  \item
    If we had instead coded responses as: Too little = 3, About right =
    2, Too much = 1, what would the ATE have been? Would your
    substantive interpretation change?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The ATE of -1.3 means that, on average, respondents
gave responses that were 1.3 points \emph{lower} on the 1-2-3 scale when
asked about ``assistance to the poor'' compared to when asked about
``welfare.''

But what does ``lower'' mean substantively? Recall the coding:

\begin{itemize}
\tightlist
\item
  Too little = 1 (more support for spending)
\item
  About right = 2
\item
  Too much = 3 (less support for spending)
\end{itemize}

Lower numbers indicate \emph{more} support for spending. So a negative
ATE indicates \emph{increased} support for spending when using the
``assistance to the poor'' wording. The treatment (changing ``welfare''
to ``assistance to the poor'') made people more supportive of government
spending.

\textbf{Part b.} The sign is potentially confusing because the coding
scheme runs in the opposite direction from what we might intuitively
expect. We often associate ``positive'' with ``more'' and ``negative''
with ``less.'' But here, a \emph{negative} ATE means \emph{more} support
for spending.

This is an artifact of how the response options were numbered. The
researcher could have coded the responses differently.

\textbf{Part c.} If we reversed the coding:

\begin{itemize}
\tightlist
\item
  Too little = 3 (more support for spending)
\item
  About right = 2
\item
  Too much = 1 (less support for spending)
\end{itemize}

Then each potential outcome would transform:

\begin{itemize}
\tightlist
\item
  Original values of 1 become 3
\item
  Original values of 2 stay 2
\item
  Original values of 3 become 1
\end{itemize}

Using the hypothetical data from the chapter, the new ATE would be:

\[
\begin{align}
\text{ATE} &= \frac{1}{10}(3 + 2 + 3 + 3 + 3 + 2 + 3 + 3 + 2 + 3) - \frac{1}{10}(1 + 1 + 2 + 1 + 2 + 1 + 2 + 1 + 2 + 1) \\
&= \frac{27}{10} - \frac{14}{10} \\
&= 2.7 - 1.4 \\
&= +1.3
\end{align}
\]

The ATE becomes \(+1.3\) instead of \(-1.3\).

The \emph{substantive} interpretation does not change: the ``assistance
to the poor'' wording still increases support for spending compared to
``welfare.'' The sign of the ATE simply reflects our arbitrary choice of
how to code the responses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  \textbf{Rasinski's Potential Outcomes}: Consider respondent 9 in the
  hypothetical Rasinski data, who answered ``About right'' (2) under
  both conditions.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    What is this respondent's individual treatment effect (\(\tau_9\))?
  \item
    Does this mean the question wording had no effect on this
    respondent? Explain.
  \item
    Can you construct an example with different potential outcomes where
    the treatment has a large effect for every individual, but the ATE
    is still zero?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Respondent 9 answered ``About right'' (coded as 2)
under both conditions:

\begin{itemize}
\tightlist
\item
  \(Y_9(1) = 2\) (response if asked about ``assistance to the poor'')
\item
  \(Y_9(0) = 2\) (response if asked about ``welfare'')
\end{itemize}

The individual treatment effect is:

\[\tau_9 = Y_9(1) - Y_9(0) = 2 - 2 = 0\]

\textbf{Part b.} For this particular respondent, the question wording
had no effect on their \emph{observed categorical response}. They gave
the same answer---``About right''---regardless of which version of the
question they received.

However, this does not necessarily mean the wording had no psychological
effect whatsoever. It could be that:

\begin{itemize}
\tightlist
\item
  The respondent holds consistent, moderate views on this issue that are
  not affected by framing
\item
  The two framings activated different considerations, but those
  considerations happened to produce the same response category
\item
  The respondent gave a default ``middle'' response without processing
  the question carefully
\end{itemize}

All we can conclude is that the treatment did not change this
respondent's response on this particular three-category scale.

\textbf{Part c.} Yes. Here is an example where treatment has a large
effect (\(|\tau_i| = 3\)) for every individual, but the ATE is zero:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Individual & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 5 & \(5 - 2 = 3\) \\
2 & 6 & 3 & \(3 - 6 = -3\) \\
3 & 4 & 7 & \(7 - 4 = 3\) \\
4 & 8 & 5 & \(5 - 8 = -3\) \\
\end{longtable}

Every individual experiences a treatment effect of either \(+3\) or
\(-3\). The treatment clearly matters for each person. But the ATE is:

\[
\begin{align}
\text{ATE} &= \frac{1}{4}(3 + (-3) + 3 + (-3)) \\
&= \frac{0}{4} \\
&= 0
\end{align}
\]

The positive and negative effects cancel perfectly, producing an ATE of
zero despite large individual effects.

\subsection{Critical Thinking}\label{critical-thinking}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\item
  \textbf{Beyond the Average}: In the original hypothetical example
  (with 10 respondents and ATE = 1.5), we saw that treatment effects
  varied across individuals, with some positive, some negative, and some
  zero.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Why might this variation in individual treatment effects matter for
    policy or practical decisions?
  \item
    What are the limitations of focusing only on the ATE when individual
    treatment effects vary substantially?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Variation in individual treatment effects matters for
policy and practical decisions because:

\emph{Targeting interventions}: If treatment helps some people but harms
others, we might want to identify who benefits and offer the treatment
only to them. A positive ATE could mask the fact that a substantial
group is being harmed.

\emph{Equity concerns}: Even if the average effect is positive, we might
care about \emph{who} benefits. If benefits are concentrated among
people who are already well-off, while disadvantaged groups see little
improvement, this raises fairness concerns that the ATE alone cannot
reveal.

\emph{Individual decision-making}: A person deciding whether to undergo
a treatment cares about the effect on \emph{them}, not the average
effect across all people. If effects are highly variable, the ATE may be
a poor guide for any particular individual.

\emph{Understanding mechanisms}: Variation in effects can help us
understand \emph{why} a treatment works. If effects are larger for
certain subgroups, this may point to underlying mechanisms or suggest
how to improve the treatment.

\textbf{Part b.} The ATE has several limitations when individual
treatment effects vary substantially:

\begin{itemize}
\item
  The ATE can hide important heterogeneity. An ATE of zero could mean
  ``the treatment has no effect on anyone'' or ``the treatment has large
  positive effects for half the population and large negative effects
  for the other half.'' These are very different situations with very
  different policy implications.
\item
  The ATE does not tell us who benefits and who is harmed.
\item
  The ATE treats all individuals equally. But we might care more about
  effects on certain groups, such as those who are most disadvantaged or
  most at risk.
\item
  The ATE is a single summary number. Like any average, it can be
  misleading if the underlying distribution is skewed or bimodal.
\item
  For individual decision-making, knowing the ATE does not tell any
  specific person whether the treatment will help or harm \emph{them}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\item
  \textbf{Assumption Plausibility}:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    For what kinds of treatments or contexts---if any---might the
    constant treatment effects assumption be approximately reasonable?
  \item
    For what kinds of treatments would this assumption be clearly
    implausible? Provide a specific example and explain why treatment
    effects would likely vary across individuals.
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The constant treatment effects assumption might be
\emph{approximately} reasonable in some contexts:

\emph{Simple, additive interventions}: A treatment that mechanically
adds a fixed amount might have roughly constant effects. For example, if
a government program gives every recipient exactly \$500, the effect on
``dollars received'' is constant at \$500 for everyone.

\emph{Highly standardized settings with homogeneous units}: In carefully
controlled laboratory experiments with nearly identical subjects (such
as certain agricultural experiments with genetically identical plants),
treatment effects might be relatively uniform.

\emph{Very small effects}: When treatment effects are small relative to
natural variation in outcomes, the assumption of constant effects might
not matter much practically, even if it's technically false.

Even in these cases, truly constant effects are unlikely. But the
assumption might be close enough to be useful for certain purposes.

\textbf{Part b.} Constant treatment effects would be clearly implausible
for treatments where individual circumstances strongly influence how
people respond.

\emph{Example: A job training program's effect on earnings}

Treatment effects would likely vary substantially across individuals
because:

\begin{itemize}
\item
  \emph{Baseline skills}: Someone with no prior job skills might benefit
  enormously from training, while someone who already has strong skills
  might gain little or nothing.
\item
  \emph{Local labor markets}: The value of new skills depends on whether
  employers in the person's area are hiring for jobs that use those
  skills. A person in a booming job market might see large earnings
  gains; a person in a depressed area might see none.
\item
  \emph{Personal circumstances}: People differ in their ability to apply
  what they learn, their job search networks, the discrimination they
  face, their health, their family responsibilities, and countless other
  factors that affect how training translates into earnings.
\item
  \emph{Motivation and engagement}: Some participants engage deeply with
  training while others do not, leading to different outcomes.
\end{itemize}

For a job training program, we would expect some participants to see
large earnings gains, others to see modest gains, and some to see no
gain or even losses (if, say, the time spent in training caused them to
miss other opportunities). Assuming constant effects would ignore this
important variation.

\bookmarksetup{startatroot}

\chapter{Estimating the ATE}\label{estimating-the-ate}

In the previous chapter, we defined the average treatment effect (ATE)
as the difference in average potential outcomes under treatment and
control. This is simply the \emph{hypothetical} difference between the
average outcome if we assigned everyone to treatment and the average
outcome if we assigned everyone to contrl.

But we also acknowledged a fundamental problem: we can never observe
both potential outcomes for the same individual. So how can we design a
procedure to \emph{estimate} the ATE from data?

In this chapter, we develop a a simple and intuitive procedure and
estimator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, randomly assign individuals to treatment or control.
\item
  Second, compute the difference in average outcomes between the
  treatment and control groups
\end{enumerate}

We show that this is a \emph{good procedure} because it is an unbiased
estimator of the ATE. Importantly, this result relies only on the
\emph{design} of the experiment (the random assignment), not on any
assumptions about how outcomes are generated.

\section{The Problem; A Solution?}\label{the-problem-a-solution}

\subsection{Problem: We Can't Observe the
ATE}\label{problem-we-cant-observe-the-ate}

Recall from the previous chapter that we define the ATE as the
difference between two averages: the average outcome if we assigned
\emph{everyone} to treatment minus the average outcome if we assigned
\emph{everyone} to control.

But here's the problem: we can't assign everyone to treatment \emph{and}
everyone to control. Each individual can only be in one condition. If we
assign someone to treatment, we see their outcome under treatment (but
not their outcome under control). If we assign someone to control, we
see their outcome under control (but not their outcome under treatment).

This means we can never directly compute the ATE. So how do we get
around this?

\subsection{Solution?: Random
Assignment}\label{solution-random-assignment}

The key insight is that we don't need to observe \emph{everyone's}
outcome under treatment to learn about the average outcome under
treatment. If we randomly assign individuals to treatment, then the
individuals who happen to be assigned to treatment are---on
average---just like the individuals who happen to be assigned to
control. They are a simple random sample from the same population.

This is the logic of a \textbf{completely randomized experiment}. We
take our group of individuals and randomly divide them into two groups:
some go to treatment, some go to control.

\begin{itemize}
\tightlist
\item
  The \textbf{treated individuals} are a random sample of the full
  group. So the average outcome we observe among treated individuals
  should be a good estimate of what the average outcome \emph{would have
  been} if we had assigned everyone to treatment.
\item
  Similarly, the average outcome among \textbf{control individuals}
  should be a good estimate of what the average outcome \emph{would have
  been} if we had assigned everyone to control.
\end{itemize}

In other words, random assignment lets us \emph{estimate} the two
quantities we need---the average outcome under treatment and the average
outcome under control---even though we can't observe them directly for
everyone.

The difference between these two estimates gives us an \emph{estimate}
of the ATE. It won't be perfect. Any particular randomization might, by
chance, put more higher-outcome individuals in one group than the other.
But on average, across all possible randomizations, this approach gets
it exactly right.

\section{The Observed Data}\label{the-observed-data}

In a completely randomized experiment, we have \(N\) individuals. We
randomly assign \(N_t\) individuals to treatment and the remaining
\(N_c = N - N_t\) individuals to control, where \(1 < N_t < N\) (so at
least one individual is in each group).

\marginnote{\begin{footnotesize}

In practice, we tend to use a \emph{balanced} design where
\(N_t = N_c = \dfrac{N}{2}\), so that half of the respondents go into
the treatment group and an equal number go into the control group.

We use the indicator variable \(D_i\) to denote the treatment assignment
for individual \(i\).

\begin{itemize}
\tightlist
\item
  \(D_i = 1\) if individual \(i\) is assigned to treatment
\item
  \(D_i = 0\) if individual \(i\) is assigned to control
\end{itemize}

Thus we can write that

\[
N_t = \sum_{i=1}^{N} D_i \quad \text{and} \quad N_c = \sum_{i=1}^{N} (1 - D_i).
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Notation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The big sigma (\(\sum\)) just means ``add up.'' The expression
\(\sum_{i=1}^{N} D_i\) says this: start with individual 1, then
individual 2, then individual 3, and so on up to individual \(N\), and
add up all their \(D_i\) values.

Since each \(D_i\) is either 0 (control) or 1 (treatment), adding them
up counts how many 1's there are---that is, how many individuals are in
the treatment group. That's \(N_t\).

The expression \((1 - D_i)\) just flips the indicator. If \(D_i = 1\),
then \(1 - D_i = 0\). If \(D_i = 0\), then \(1 - D_i = 1\). So adding up
all the \((1 - D_i)\) values counts how many 0's there are in the
original \(D_i\)'s---that is, how many individuals are in the control
group. That's \(N_c\).

\textbf{Example.} Suppose we have \(N = 5\) individuals with the
following treatment assignments:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Individual \(i\) & \(D_i\) & \(1 - D_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 0 \\
2 & 0 & 1 \\
3 & 1 & 0 \\
4 & 0 & 1 \\
5 & 1 & 0 \\
\end{longtable}

To count the treated individuals, we add up the \(D_i\) column:
\(N_t = 1 + 0 + 1 + 0 + 1 = 3\).

To count the control individuals, we add up the \(1 - D_i\) column:
\(N_c = 0 + 1 + 0 + 1 + 0 = 2\).

Three individuals are treated; two are in control.

For each individual, we observe only one potential outcome. We call this
the \textbf{observed outcome}, denoted \(Y_i^{\text{obs}}\), so that

\[
Y_i^{\text{obs}} = Y_i(D_i) =
\begin{cases}
Y_i(0) & \text{if } D_i = 0 \\
Y_i(1) & \text{if } D_i = 1
\end{cases}.
\]

The other potential outcome---the one we don't observe---is the
\textbf{missing outcome}, denoted \(Y_i^{\text{mis}}\), so that

\[
Y_i^{\text{mis}} = Y_i(1 - D_i) =
\begin{cases}
Y_i(1) & \text{if } D_i = 0 \\
Y_i(0) & \text{if } D_i = 1
\end{cases}.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Notation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The notation \(Y_i(D_i)\) means ``plug the value of \(D_i\) into the
potential outcome function.'' Since \(D_i\) is either 0 or 1, this gives
us either \(Y_i(0)\) or \(Y_i(1)\)---whichever one actually happened.

The curly brace notation is just a compact way of writing ``if-then''
rules. It says: look at \(D_i\), and depending on its value, pick the
corresponding potential outcome.

For the observed outcome \(Y_i^{\text{obs}}\), we get the potential
outcome that matches the treatment the individual actually received. For
the missing outcome \(Y_i^{\text{mis}}\), we get the \emph{other}
potential outcome---the one we can't see.

\textbf{Example.} Suppose we have \(N = 5\) individuals with the
following potential outcomes and treatment assignments:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1928}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2410}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2410}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Individual \(i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(1)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(D_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i^{\text{obs}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i^{\text{mis}}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 & 1 & 7 & 4 \\
2 & 6 & 5 & 0 & 6 & 5 \\
3 & 5 & 8 & 1 & 8 & 5 \\
4 & 3 & 6 & 0 & 3 & 6 \\
5 & 7 & 9 & 1 & 9 & 7 \\
\end{longtable}

For individual 1, \(D_1 = 1\) (treated), so we observe
\(Y_1^{\text{obs}} = Y_1(1) = 7\) and the missing outcome is
\(Y_1^{\text{mis}} = Y_1(0) = 4\).

For individual 2, \(D_2 = 0\) (control), so we observe
\(Y_2^{\text{obs}} = Y_2(0) = 6\) and the missing outcome is
\(Y_2^{\text{mis}} = Y_2(1) = 5\).

\subsection{What the Data Look Like}\label{what-the-data-look-like}

To make this concrete, consider again our hypothetical experiment with
10 respondents. Suppose we know all the potential outcomes:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 & 3 \\
2 & 6 & 5 & -1 \\
3 & 5 & 5 & 0 \\
4 & 3 & 9 & 6 \\
5 & 7 & 10 & 3 \\
6 & 5 & 4 & -1 \\
7 & 4 & 8 & 4 \\
8 & 6 & 7 & 1 \\
9 & 5 & 3 & -2 \\
10 & 4 & 6 & 2 \\
\end{longtable}

We computed in the previous chapter that \(\text{ATE} = 1.5\) for these
data.

Now suppose we run an experiment with \(N_t = 5\) and \(N_c = 5\). After
randomization, respondents 1, 4, 5, 7, and 10 are assigned to treatment,
while respondents 2, 3, 6, 8, and 9 are assigned to control. What do we
actually observe?

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & ? & 7 & 1 & 7 \\
2 & 6 & ? & 0 & 6 \\
3 & 5 & ? & 0 & 5 \\
4 & ? & 9 & 1 & 9 \\
5 & ? & 10 & 1 & 10 \\
6 & 5 & ? & 0 & 5 \\
7 & ? & 8 & 1 & 8 \\
8 & 6 & ? & 0 & 6 \\
9 & 5 & ? & 0 & 5 \\
10 & ? & 6 & 1 & 6 \\
\end{longtable}

Notice that for each respondent, we observe exactly one potential
outcome. The question marks represent the counterfactual outcomes we can
never observe. We cannot compute \(\tau_i = Y_i(1) - Y_i(0)\) for any
individual because we never have both values.

But here's the key insight: even though we can't compute individual
treatment effects, we \emph{can} compute averages within the treatment
and control groups.

\section{The Difference-in-Means
Estimator}\label{the-difference-in-means-estimator}

A natural approach is to estimate the ATE by comparing the average
outcome among treated individuals to the average outcome among control
individuals. We define \(\overline{Y}^{\text{obs}}_{t}\) and
\(\overline{Y}^{\text{obs}}_{c}\) as

\[
\overline{Y}^{\text{obs}}_{t} = \frac{1}{N_t} \sum_{i:D_i=1} Y^{\text{obs}}_{i} \quad \text{and} \quad \overline{Y}^{\text{obs}}_{c} = \frac{1}{N_c} \sum_{i:D_i=0} Y^{\text{obs}}_{i}.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Notation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The bar over \(Y\) (as in \(\overline{Y}\)) means ``average.'' So
\(\overline{Y}^{\text{obs}}_{t}\) is the average observed outcome in the
treatment group, and \(\overline{Y}^{\text{obs}}_{c}\) is the average
observed outcome in the control group.

The subscript \(i:D_i=1\) under the summation sign means ``only add up
the individuals where \(D_i = 1\)''---that is, only the treated
individuals. Similarly, \(i:D_i=0\) means ``only the control
individuals.''

So the formula says this: add up the observed outcomes for all treated
individuals, then divide by \(N_t\) (the number of treated individuals)
to get the average. Do the same for control individuals, dividing by
\(N_c\).

\textbf{Example.} Suppose we have \(N = 5\) individuals:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Individual \(i\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 7 \\
2 & 0 & 6 \\
3 & 1 & 8 \\
4 & 0 & 3 \\
5 & 1 & 9 \\
\end{longtable}

There are \(N_t = 3\) treated individuals (individuals 1, 3, and 5) and
\(N_c = 2\) control individuals (individuals 2 and 4).

For the treatment group, we add up only the outcomes where \(D_i = 1\),
so that

\[
\overline{Y}^{\text{obs}}_{t} = \frac{1}{3}(7 + 8 + 9) = \frac{24}{3} = 8.
\]

For the control group, we add up only the outcomes where \(D_i = 0\), so
that

\[
\overline{Y}^{\text{obs}}_{c} = \frac{1}{2}(6 + 3) = \frac{9}{2} = 4.5.
\]

These are the average observed outcomes in the treatment group and the
control group, respectively.

The \textbf{difference-in-means estimator} for the ATE is

\[
\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Notation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The ``hat'' over ATE (as in \(\widehat{\text{ATE}}\)) means ``estimate
of.'' We can't compute the true ATE because we don't observe all
potential outcomes, so we estimate it using the data we have.

The formula says: take the average outcome among treated individuals and
subtract the average outcome among control individuals. That's it---just
a difference of two averages.

\textbf{Example.} Using the same 5 individuals from before:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Individual \(i\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 7 \\
2 & 0 & 6 \\
3 & 1 & 8 \\
4 & 0 & 3 \\
5 & 1 & 9 \\
\end{longtable}

We already computed \(\overline{Y}^{\text{obs}}_{t} = 8\) and
\(\overline{Y}^{\text{obs}}_{c} = 4.5\).

Thus \(\widehat{\text{ATE}} = 8 - 4.5 = 3.5\). Our estimate is that the
treatment increases outcomes by 3.5 units on average.

\subsection{Computing the Estimate}\label{computing-the-estimate}

Using our example data, we can compute

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{5}(7 + 9 + 10 + 8 + 6) = \frac{40}{5} = 8
\end{align}
\]

and

\[
\begin{align}
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{5}(6 + 5 + 5 + 6 + 5) = \frac{27}{5} = 5.4
\end{align}.
\]

Therefore \(\widehat{\text{ATE}} = 8 - 5.4 = 2.6\). But recall that the
true ATE (which we computed using all potential outcomes) is 1.5. So our
estimate is off by 1.1 units.

Is this a problem? Not necessarily. Any particular estimate will differ
from the true ATE due to the randomness in treatment assignment. The
question is whether, \emph{on average} across all possible
randomizations, our estimator gets it right.

\section{Why This Estimator?}\label{why-this-estimator}

Why is
\(\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\)
a sensible estimator of the ATE?

Recall that we define the ATE as

\[
\text{ATE} = \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0) = \overline{Y}(1) - \overline{Y}(0).
\]

The ATE is the difference between two \emph{population} averages: the
average of \emph{all} potential outcomes under treatment and the average
of \emph{all} potential outcomes under control.

Our estimator replaces these population averages with sample averages:

\begin{itemize}
\tightlist
\item
  Instead of averaging \(Y_i(1)\) over all individuals, we average
  \(Y_i^{\text{obs}}\) over individuals assigned to treatment
\item
  Instead of averaging \(Y_i(0)\) over all individuals, we average
  \(Y_i^{\text{obs}}\) over individuals assigned to control
\end{itemize}

\marginnote{\begin{footnotesize}

This ``plug-in'' approach---replacing population quantities with their
sample counterparts---is a fundamental strategy in statistics. Under
random assignment, the treated individuals are a random sample from the
population, so \(\overline{Y}^{\text{obs}}_{t}\) should be a good
estimate of \(\overline{Y}(1)\). Similarly, the control individuals are
a random sample, so \(\overline{Y}^{\text{obs}}_{c}\) should be a good
estimate of \(\overline{Y}(0)\).

\section{Design-Based Inference}\label{design-based-inference}

Before proving that the estimator \(\widehat{\text{ATE}}\) is unbiased,
we need to understand

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{that} the estimate \widehat{\text{ATE}}\$ is a noisy and random
  estimate and\\
\item
  \emph{what} makes this estimate random in the first place.
\end{enumerate}

In typical statistical settings, we imagine that data are drawn from
some distribution \(Y_i \sim f(\theta)\). The randomness comes from
imagining that the data could have been different if we had drawn a
different sample from the population.

But in the potential outcomes framework for randomized experiments, we
take a different view. The potential outcomes \(Y_i(0)\) and \(Y_i(1)\)
are treated as \emph{fixed} values, not random variables. They are
simply the outcomes each individual \emph{would} experience under each
condition---there's nothing random about the \emph{potential} outcomes.

The only randomness in the experiment comes from the treatment
assignment \(D_i\). And because we designed the randomization mechanism,
we know exactly how this randomness works.

This is important: \textbf{because we designed the randomization
mechanism, we know exactly how this randomness works.}

This is called \textbf{design-based inference} in which all uncertainty
comes from the design of the experiment (i.e., the random assignment in
this case), not from any model of how outcomes are generated.

To make this explicit, we can rewrite the estimator to highlight what is
random and what is fixed, so that

\[
\widehat{\text{ATE}} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{D_i \cdot Y_i(1)}{N_t/N} - \frac{(1 - D_i) \cdot Y_i(0)}{N_c/N} \right).
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Notation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

This formula looks intimidating, but it's just a clever rewriting of the
difference-in-means estimator that separates the random part (\(D_i\))
from the fixed parts (potential outcomes).

Let's break it down piece by piece:

\begin{itemize}
\item
  \(D_i \cdot Y_i(1)\): This equals \(Y_i(1)\) when individual \(i\) is
  treated (\(D_i = 1\)) and equals 0 when individual \(i\) is in control
  (\(D_i = 0\)). So it ``turns on'' the treated potential outcome only
  for treated individuals.
\item
  \((1 - D_i) \cdot Y_i(0)\): This does the opposite---it equals
  \(Y_i(0)\) when \(D_i = 0\) and equals 0 when \(D_i = 1\). It ``turns
  on'' the control potential outcome only for control individuals.
\item
  \(N_t/N\): This is the fraction of individuals assigned to treatment
  (the treatment probability).
\item
  Dividing by \(N_t/N\) in the first term and \(N_c/N\) in the second
  term rescales each contribution so that the formula averages correctly
  over each group.
\end{itemize}

The key insight is that when you work through the algebra, this formula
gives exactly the same answer as
\(\overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\). But
writing it this way makes clear that \(D_i\) is the only random
quantity---the potential outcomes \(Y_i(1)\) and \(Y_i(0)\) are fixed
numbers.

\textbf{Example.} Suppose \(N = 4\) with \(N_t = 2\) and \(N_c = 2\) and
the following potential outcomes and randomization:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Individual \(i\) & \(Y_i(0)\) & \(Y_i(1)\) & \(D_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 & 1 \\
2 & 6 & 5 & 0 \\
3 & 3 & 8 & 1 \\
4 & 5 & 6 & 0 \\
\end{longtable}

For individual 1 (\(D_1 = 1\)): the contribution is
\(\frac{1 \cdot 7}{2/4} - \frac{0 \cdot 4}{2/4} = \frac{7}{0.5} - 0 = 14\).

For individual 2 (\(D_2 = 0\)): the contribution is
\(\frac{0 \cdot 5}{2/4} - \frac{1 \cdot 6}{2/4} = 0 - \frac{6}{0.5} = -12\).

For individual 3 (\(D_3 = 1\)): the contribution is
\(\frac{1 \cdot 8}{0.5} - 0 = 16\).

For individual 4 (\(D_4 = 0\)): the contribution is
\(0 - \frac{5}{0.5} = -10\).

Adding these and dividing by \(N = 4\):
\(\widehat{\text{ATE}} = \frac{1}{4}(14 - 12 + 16 - 10) = \frac{8}{4} = 2\).

You can verify this equals the simple difference in means:
\(\overline{Y}^{\text{obs}}_{t} = \frac{7 + 8}{2} = 7.5\) and
\(\overline{Y}^{\text{obs}}_{c} = \frac{6 + 5}{2} = 5.5\), so
\(\widehat{\text{ATE}} = 7.5 - 5.5 = 2\).

In this expression

\begin{itemize}
\tightlist
\item
  \(Y_i(1)\) and \(Y_i(0)\) are fixed (the potential outcomes)
\item
  \(N\), \(N_t\), and \(N_c\) are fixed (determined by design)
\item
  \(D_i\) is random (the only source of uncertainty)
\end{itemize}

The subscript notation \(E_D[\cdot]\) indicates that we are taking
expectations with respect to the randomization distribution---the
distribution induced by the random assignment of individuals to
treatment and control.

\section{\texorpdfstring{Unbiasedness of
\(\widehat{\text{ATE}}\)}{Unbiasedness of \textbackslash widehat\{\textbackslash text\{ATE\}\}}}\label{unbiasedness-of-widehattextate}

We now prove the key result: \(\widehat{\text{ATE}}\) is an unbiased
estimator of \(\text{ATE}\). Informally, this means that our estimator
``gets it right on average''. It means that if we could repeat the
randomization many times, our estimates would exactly equal the ATE in
the long-run.

To state this result precisely, we need to introduce some notation. The
notation is tedious, but the underlying idea is straightforward,
important, and powerful.

\subsection{\texorpdfstring{The Expectation Operator
\(E[\cdot]\)}{The Expectation Operator E{[}\textbackslash cdot{]}}}\label{the-expectation-operator-ecdot}

The expectation operator \(E[\cdot]\) is just a way of writing
``long-run average.''

\marginnote{\begin{footnotesize}

The expectation of a random variable is its \emph{average value} across
all possible outcomes, weighted by how likely each outcome is. You can
think of \(E[X]\) as asking: ``If I could repeat this random process
infinitely many times, what would \(X\) average out to?''

For our purposes, we need four simple rules about expectations.

\textbf{Rule 1: The expectation of a constant is that constant.}

If \(c\) is a fixed number (not random), then \(E[c] = c\).

If something never changes, its ``average'' is just itself. For example,
\(E[5] = 5\).

\textbf{Rule 2: Constants can be pulled outside the expectation.}

If \(c\) is a constant and \(X\) is random, then
\(E[c \cdot X] = c \cdot E[X]\).

The average of ``5 times something random'' equals 5 times the average
of that random thing. If on average you earn \$100 per day, then on
average you earn \$500 per five-day week.

\textbf{Rule 3: The expectation of a sum is the sum of the
expectations.}

If \(X\) and \(Y\) are random variables, then\\
\[
E[X + Y] = E[X] + E[Y].
\]

The average of ``two random things added together'' equals the sum of
their averages. If on average you earn \$100 per day from your job and
\$20 per day from side work, then on average you earn \$120 per day
total. You do \textbf{not} need \(X\) and \(Y\) to be independent for
this rule to hold.

\textbf{Rule 4: The expectation of a binary (0/1) random variable equals
its probability of being 1.}

If \(X\) can only be 0 or 1, then \(E[X] = \Pr(X = 1)\).

This is the key rule for our proof. And it's intuitive. If you flip a
fair coin and let \(X = 1\) for heads and \(X = 0\) for tails, what's
the average value of \(X\)? Half the time you get 1, half the time you
get 0, so the average is \(\frac{1}{2}(1) + \frac{1}{2}(0) = 0.5\). And
\(\Pr(X = 1) = 0.5\). They match!

More generally, if you repeat the process many times, the fraction of
1's you observe will approach the probability of getting a 1. So the
average value equals the probability.

\marginnote{\begin{footnotesize}

Why does Rule 4 work? Well, remember that the expectation of a random
variable is defined as the sum of each possible value times its
probability, so that

\[
E[X] = \sum_{\text{all values } x} x \cdot \Pr(X = x).
\]

For a binary variable that can only be 0 or 1, then we have

\[
E[X] = 0 \cdot \Pr(X = 0) + 1 \cdot \Pr(X = 1) = \Pr(X = 1).
\]

The zero term disappears, and we're left with just the probability that
\(X\) equals 1.

\textbf{Example.} Suppose you roll a die and let \(X = 1\) if you roll a
6, and \(X = 0\) otherwise. Then \(\Pr(X = 1) = 1/6\), so
\(E[X] = 1/6 \approx 0.167\). If you rolled the die 600 times, you'd
expect about 100 sixes, and your average value of \(X\) would be about
\(100/600 = 0.167\).

\subsection{The Theorem}\label{the-theorem}

Now we can state the key result precisely. When we write
\(E_D[\widehat{\text{ATE}}]\), we mean ``the average value of our
estimate across all possible random assignments.'' (The subscript \(D\)
reminds us that the only randomness comes from treatment assignment.)

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Theorem}, breakable, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Under random assignment,
\(\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\)
is an unbiased estimator of the ATE. This mean that \[
E_D[\widehat{\text{ATE}}] = \text{ATE}.
\]

If we average our estimate over all the different ways we could have
randomly assigned individuals to treatment and control, we get exactly
the true ATE.

\subsection{The Proof}\label{the-proof}

\textbf{Proof.} We want to show that
\(E_D[\widehat{\text{ATE}}] = \text{ATE}\).

\textbf{Step 1: Write the estimator to separate random from fixed.}

First, we write \(\widehat{\text{ATE}}\) in a form that clearly
separates the random part (\(D_i\)) from the fixed parts (the potential
outcomes \(Y_i(1)\) and \(Y_i(0)\)). We borrow the representation above,
where

\[
\widehat{\text{ATE}} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{D_i \cdot Y_i(1)}{N_t/N} - \frac{(1 - D_i) \cdot Y_i(0)}{N_c/N} \right).
\]

This formula looks complicated, but it has two important features:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It equals the difference in the observed means (though this is only
  apparent after studying it carefully).
\item
  It highlights that \(D_i\) is the \emph{only} random thing.
\item
  It will be easy to use with the rules for expectations.
\end{enumerate}

\textbf{Step 2: Take the expectation.}

Now we ask: what is the average value of \(\widehat{\text{ATE}}\) across
all possible randomizations?

\[
E_D[\widehat{\text{ATE}}] = E_D\left[ \frac{1}{N} \sum_{i=1}^{N} \left( \frac{D_i \cdot Y_i(1)}{N_t/N} - \frac{(1 - D_i) \cdot Y_i(0)}{N_c/N} \right) \right]
\]

\textbf{Step 3: Move the expectation inside.}

Here's where the magic happens. Because the potential outcomes are
\emph{fixed} numbers and only \(D_i\) is random, we can move the
expectation operator inside the formula until it includes \emph{only}
\(D_i\) and \(1 - D_i\). Rule 2 above allows us to do this.

\marginnote{\begin{footnotesize}

Think of it this way: if you're computing the average of ``5 times
something random,'' you can compute it as ``5 times the average of that
random thing'' (that's Rule 2). The \(\frac{1}{N}\), the \(\sum\), the
potential outcomes \(Y_i(1)\) and \(Y_i(0)\), the fractions \(N_t/N\)
and \(N_c/N\) are all fixed numbers that pass right through the
expectation. The expectation only ``grabs onto'' the random part,
\(D_i\).

\[
E_D[\widehat{\text{ATE}}] = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{E_D[D_i] \cdot Y_i(1)}{N_t/N} - \frac{E_D[1 - D_i] \cdot Y_i(0)}{N_c/N} \right)
\]

\begin{align}
E_D[\widehat{\text{ATE}}]
&= E_D\!\left[
\frac{1}{N} \sum_{i=1}^{N}
\left(
\frac{D_i\, Y_i(1)}{N_t/N}
-
\frac{(1 - D_i)\, Y_i(0)}{N_c/N}
\right)
\right]
\qquad \text{(definition of the estimator)} \\[6pt]

&= \frac{1}{N}
E_D\!\left[
\sum_{i=1}^{N}
\left(
\frac{D_i\, Y_i(1)}{N_t/N}
-
\frac{(1 - D_i)\, Y_i(0)}{N_c/N}
\right)
\right]
\qquad \text{($\tfrac{1}{N}$ is constant; Rule 2)} \\[6pt]

&= \frac{1}{N}
\sum_{i=1}^{N}
E_D\!\left[
\frac{D_i\, Y_i(1)}{N_t/N}
-
\frac{(1 - D_i)\, Y_i(0)}{N_c/N}
\right]
\qquad \text{(expectation of asum; Rule 3)} \\[6pt]

&= \frac{1}{N}
\sum_{i=1}^{N}
\left(
\frac{Y_i(1)}{N_t/N} \, E_D[D_i]
-
\frac{Y_i(0)}{N_c/N} \, E_D[1 - D_i]
\right)
\qquad \text{(only $D_i$ is random; Rule 2)}.
\end{align} \}

\textbf{Step 4: Compute the expectation of \(D_i\).}

Now we need to find \(E_D[D_i]\). Remember that \(D_i\) is a binary
variable that equals 1 if individual \(i\) is assigned to treatment, and
0 if individual \(i\) is assigned to control. By Rule 4, the expectation
of a binary variable equals its probability of being 1, so that

\[
E_D[D_i] = \Pr(D_i = 1).
\]

Under completely randomized assignment, every individual has the same
chance of being assigned to treatment. If we're assigning \(N_t\)
individuals to treatment out of \(N\) total individuals, then each
individual has probability \(N_t/N\) of being treated so that

\[
E_D[D_i] = \Pr(D_i = 1) = \frac{N_t}{N}.
\]

\textbf{Step 5: Compute the expectation of \(1 - D_i\).}

Similarly, \(1 - D_i\) is also binary. It equals 1 when \(D_i = 0\)
(individual in control) and equals 0 when \(D_i = 1\) (individual in
treatment). Then we have

\[
E_D[1 - D_i] = \Pr(D_i = 0) = \frac{N_c}{N}.
\]

\textbf{Step 6: Substitute and simplify.}

Now we plug these values back in, we obtain

\[
\begin{align}
E_D[\widehat{\text{ATE}}] &= \frac{1}{N} \sum_{i=1}^{N} \left( \frac{(N_t/N) \cdot Y_i(1)}{N_t/N} - \frac{(N_c/N) \cdot Y_i(0)}{N_c/N} \right).
\end{align}
\] Then something magical happens. In the first term, we have \(N_t/N\)
in both the numerator and denominator, so they cancel out.

The same thing happens with \(N_c/N\) in the second term, so that

\[
\begin{align}
E_D[\widehat{\text{ATE}}] &= \frac{1}{N} \sum_{i=1}^{N} \left( Y_i(1) - Y_i(0) \right).
\end{align}
\]

This is just the average of individual treatment effects across all
individuals. This is exactly the definition of the ATE, so that

\[
E_D[\widehat{\text{ATE}}] = \text{ATE}.
\]

This completes the proof and shows that \(\widehat{\text{ATE}}\) is an
unbiased estimator of \(\widehat{\text{ATE}}\). \(\square\)

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, breakable, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The key insight is that the fractions \(N_t/N\) and \(N_c/N\) cancel
perfectly. This happens because (1) the probability of assignment to
treatment equals \(N_t/N\), and (2) we divide by \(N_t\) when averaging
over treated individuals. The ``selection'' into treatment is perfectly
balanced by the ``weighting'' in our estimator.

\subsection{What Does Unbiasedness
Mean?}\label{what-does-unbiasedness-mean}

Unbiasedness means that \emph{on average}, across all possible
randomizations, our estimator equals the true ATE. Any particular
estimate might be too high or too low, but there's no systematic
tendency to over- or under-estimate.

In our example, we got \(\widehat{\text{ATE}} = 2.6\) when the true ATE
is 1.5. This particular randomization overestimated. But if we had
assigned different individuals to treatment, we might have gotten a
different estimate. Some randomizations would overestimate, some would
underestimate, and on average we would get it right.

\subsection{No Model Required}\label{no-model-required}

Notice what we did \emph{not} assume in this proof:

\begin{itemize}
\tightlist
\item
  We did not assume any particular distribution for the outcomes
\item
  We did not assume the treatment effects are constant
\item
  We did not assume the outcomes are normally distributed
\item
  We did not assume any functional form for how treatment affects
  outcomes
\item
  We did not assume a ``large'' sample
\end{itemize}

The only assumption is that treatment was randomly assigned. This is why
randomized experiments are so powerful. Randomization provides unbiased
estimates (and other good things to be discussed later) without
requiring strong modeling assumptions.

\section{A Different Randomization}\label{a-different-randomization}

To reinforce that \(\widehat{\text{ATE}}\) depends on the randomization,
consider what happens with a different random assignment. Suppose
instead that individuals 2, 3, 6, 8, and 9 are assigned to treatment,
while individuals 1, 4, 5, 7, and 10 are assigned to control.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & ? & 0 & 4 \\
2 & ? & 5 & 1 & 5 \\
3 & ? & 5 & 1 & 5 \\
4 & 3 & ? & 0 & 3 \\
5 & 7 & ? & 0 & 7 \\
6 & ? & 4 & 1 & 4 \\
7 & 4 & ? & 0 & 4 \\
8 & ? & 7 & 1 & 7 \\
9 & ? & 3 & 1 & 3 \\
10 & 4 & ? & 0 & 4 \\
\end{longtable}

Now we compute

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{5}(5 + 5 + 4 + 7 + 3) = \frac{24}{5} = 4.8
\end{align}
\]

and

\[
\begin{align}
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{5}(4 + 3 + 7 + 4 + 4) = \frac{22}{5} = 4.4.
\end{align}
\]

Thus \(\widehat{\text{ATE}} = 4.8 - 4.4 = 0.4\).

This randomization gives us \(\widehat{\text{ATE}} = 0.4\), which
underestimates the true ATE of 1.5. The first randomization
overestimated (2.6), this one underestimates (0.4). Unbiasedness tells
us that if we averaged over \emph{all} possible randomizations, we would
get exactly 1.5.

\section{\texorpdfstring{Computing \(\widehat{\text{ATE}}\) in
R}{Computing \textbackslash widehat\{\textbackslash text\{ATE\}\} in R}}\label{computing-widehattextate-in-r}

So far, we have computed \(\widehat{\text{ATE}}\) by hand. In practice,
we use software. In this section, we walk through two approaches in R:
(1) using \texttt{mean()} to compute the difference in means directly,
and (2) using \texttt{lm()} to fit a linear regression. Both approaches
give the same answer, but \texttt{mean()} is a little more intuitive and
\texttt{lm()} is more useful in the practice.

\subsection{The Mock Rasinski Data}\label{the-mock-rasinski-data}

We collected data that mimics Rasinski's experiment. Each respondent was
randomly assigned to see either the ``welfare'' wording or the
``assistance to the poor'' wording. The respondent then indicated
whether they thought government spending on this program was ``Too
little,'' ``About right,'' or ``Too much.''

First, let's load the data and prepare it for analysis. For convenience,
I've written a little \texttt{tribble()} as a convenient way to hold the
data (rather than loading from a CSV).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tibble)}
\FunctionTok{library}\NormalTok{(forcats)}

\CommentTok{\# load the mock rasinski data}
\NormalTok{rasinski }\OtherTok{\textless{}{-}} \FunctionTok{tribble}\NormalTok{(}
  \SpecialCharTok{\textasciitilde{}}\NormalTok{desc,                     }\SpecialCharTok{\textasciitilde{}}\NormalTok{response,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too much"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too much"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"About right"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"assistance to the poor"}\NormalTok{,  }\StringTok{"Too little"}\NormalTok{,}
  \StringTok{"welfare"}\NormalTok{,                }\StringTok{"Too little"}
\NormalTok{)}

\CommentTok{\# take a quick look}
\FunctionTok{glimpse}\NormalTok{(rasinski)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 39
Columns: 2
$ desc     <chr> "assistance to the poor", "welfare", "assistance to the poor"~
$ response <chr> "Too little", "Too little", "Too little", "Too little", "Too ~
\end{verbatim}

The \texttt{desc} variable contains the experimental condition. It takes
on the value \texttt{"welfare"} (control) or
\texttt{"assistance\ to\ the\ poor"} (treatment). The \texttt{response}
variable contains the respondent's answer.

Following Rasinski's coding scheme from the previous chapter, we code
the outcome numerically:

\begin{itemize}
\tightlist
\item
  Too little = 1
\item
  About right = 2
\item
  Too much = 3
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# code the outcome variable numerically}
\NormalTok{rasinski }\OtherTok{\textless{}{-}}\NormalTok{ rasinski }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{outcome =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      response }\SpecialCharTok{==} \StringTok{"Too little"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{      response }\SpecialCharTok{==} \StringTok{"About right"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{      response }\SpecialCharTok{==} \StringTok{"Too much"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 39
Columns: 3
$ desc     <chr> "assistance to the poor", "welfare", "assistance to the poor"~
$ response <chr> "Too little", "Too little", "Too little", "Too little", "Too ~
$ outcome  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 3~
\end{verbatim}

We also need to create a treatment indicator variable. Following our
convention, treatment (\(D_i = 1\)) is the ``assistance to the poor''
wording and control (\(D_i = 0\)) is the ``welfare'' wording.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create treatment indicator}
\NormalTok{rasinski }\OtherTok{\textless{}{-}}\NormalTok{ rasinski }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{treatment =} \FunctionTok{if\_else}\NormalTok{(desc }\SpecialCharTok{==} \StringTok{"assistance to the poor"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 39
Columns: 4
$ desc      <chr> "assistance to the poor", "welfare", "assistance to the poor~
$ response  <chr> "Too little", "Too little", "Too little", "Too little", "Too~
$ outcome   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, ~
$ treatment <dbl> 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, ~
\end{verbatim}

\subsection{\texorpdfstring{Approach 1: Using
\texttt{mean()}}{Approach 1: Using mean()}}\label{approach-1-using-mean}

The difference-in-means estimator is

\[
\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}.
\]

We can compute this directly using R's \texttt{mean()} function. The key
is to subset the data to get the outcomes for each group separately.

\textbf{Step 1: Identify the treated and control observations.}

We use logical indexing to select rows where the treatment indicator
equals 1 (treated) or 0 (control).

We can write compactly in a single line.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# direct computation of difference in means}
\NormalTok{ate\_hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(rasinski}\SpecialCharTok{$}\NormalTok{outcome[rasinski}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{{-}}
  \FunctionTok{mean}\NormalTok{(rasinski}\SpecialCharTok{$}\NormalTok{outcome[rasinski}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}

\NormalTok{ate\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.2210526
\end{verbatim}

The estimated ATE is -0.221. Since lower values indicate more support
for spending (``Too little'' = 1), a negative estimate means the
``assistance to the poor'' wording increases support for spending
compared to the ``welfare'' wording.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{This Is Only an Estimate}, breakable, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Remember that \(\widehat{\text{ATE}}\) is an \emph{estimate} of the true
\(\text{ATE}\), not the \(\text{ATE}\) itself. If we had randomized
respondents differently---assigning different people to treatment and
control---we would have observed different outcomes and computed a
different estimate. Perhaps it would have been a bit larger, perhaps a
bit smaller, or perhaps even the opposite sign.

This is a fundamental feature of randomized experiments: the estimate we
get depends on the particular randomization we happened to draw. The
true \(\text{ATE}\) is fixed (it's determined by the potential outcomes
in the population), but our estimate of it varies with the
randomization.

In future chapters, we will address this uncertainty head-on. Our
framework allows us to say a great deal about \emph{how good} our
estimate is---that is, how close \(\widehat{\text{ATE}}\) is likely to
be to the true \(\text{ATE}\). For now, just keep in mind that the
number we computed is our best guess, but it comes with uncertainty that
we haven't yet quantified.

\subsection{\texorpdfstring{Approach 2: Using
\texttt{lm()}}{Approach 2: Using lm()}}\label{approach-2-using-lm}

\textbf{Setting Up the Model}

Linear regression with \texttt{lm()} provides another way to compute the
same estimate. This might seem surprising at first. After all,
regression is usually taught as a way to model \emph{relationships}
between variables. But it turns out that a simple regression of the
outcome on a treatment indicator gives us exactly the difference in
means.

When can fit the model

\[
Y_i = \beta_0 + \beta_1 D_i + \epsilon_i,
\]

where \(D_i\) is the treatment indicator (1 = treated, 0 = control).

The estimated coefficients have a direct interpretation:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}_0\) is the mean outcome in the control group
  (\(\overline{Y}^{\text{obs}}_{c}\))
\item
  \(\hat{\beta}_1\) is the difference in means
  (\(\overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\))
\end{itemize}

\textbf{Fitting the Model}

First, we need to ensure that ``welfare'' (control) is the reference
category for our treatment variable. When \texttt{lm()} encounters a
factor variable, it creates a dummy variable using the first level as
the reference. By default, R orders factor levels alphabetically, which
would make ``assistance to the poor'' the reference---the opposite of
what we want.

We use \texttt{fct\_relevel()} from the \texttt{forcats} package to set
``welfare'' as the reference category.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a factor with "welfare" as the reference category}
\NormalTok{rasinski }\OtherTok{\textless{}{-}}\NormalTok{ rasinski }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{condition =} \FunctionTok{fct\_relevel}\NormalTok{(desc, }\StringTok{"welfare"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# verify: "welfare" should be first}
\FunctionTok{levels}\NormalTok{(rasinski}\SpecialCharTok{$}\NormalTok{condition)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "welfare"                "assistance to the poor"
\end{verbatim}

Now we fit the linear model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit the model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition, }\AttributeTok{data =}\NormalTok{ rasinski)}

\CommentTok{\# extract the coefficients}
\FunctionTok{coef}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                    (Intercept) conditionassistance to the poor 
                      1.4210526                      -0.2210526 
\end{verbatim}

Let's verify that these coefficients match our manual calculations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the slope should equal the ATE estimate}
\FunctionTok{coef}\NormalTok{(fit)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
conditionassistance to the poor 
                     -0.2210526 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.2210526
\end{verbatim}

The values match exactly. \textbf{Using \texttt{mean()}} makes the
calculation transparent. You can see exactly what quantities are being
computed: the mean in each group and their difference. This is valuable
for understanding and teaching the concept. \textbf{Using \texttt{lm()}}
will be more valuable in applied work. We'll use \texttt{lm()} moving
forward.

\section{Key Terms}\label{key-terms-1}

\begin{itemize}
\tightlist
\item
  Completely randomized experiment
\item
  Observed outcome
\item
  Missing outcome
\item
  Difference-in-means estimator
\item
  Design-based inference
\item
  Unbiasedness
\end{itemize}

\section{Exercises}\label{exercises-1}

\subsection{Conceptual Understanding}\label{conceptual-understanding-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Observed vs.~Missing Outcomes}: In your own words, explain the
  difference between \(Y_i^{\text{obs}}\) and \(Y_i^{\text{mis}}\). Why
  is one observable and the other not?
\end{enumerate}

Solution

\(Y_i^{\text{obs}}\) is the \textbf{observed outcome}---the potential
outcome that corresponds to the treatment condition the individual
actually received. If individual \(i\) was assigned to treatment
(\(D_i = 1\)), then \(Y_i^{\text{obs}} = Y_i(1)\). If individual \(i\)
was assigned to control (\(D_i = 0\)), then
\(Y_i^{\text{obs}} = Y_i(0)\).

\(Y_i^{\text{mis}}\) is the \textbf{missing outcome}---the potential
outcome that corresponds to the treatment condition the individual did
\emph{not} receive. It is the counterfactual: what \emph{would have
happened} if the individual had been assigned to the other condition.

We can observe \(Y_i^{\text{obs}}\) because it is the outcome that
actually occurred. Once we assign individual \(i\) to treatment, we see
how they respond to treatment. But we cannot observe
\(Y_i^{\text{mis}}\) because we cannot simultaneously assign the same
individual to both treatment and control. The individual can only be in
one condition at a time, so we only ever see one of the two potential
outcomes.

This is the fundamental problem of causal inference applied to the
observed data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Design-Based Inference}: Explain what it means to say that
  inference in randomized experiments is ``design-based'' rather than
  ``model-based.'' What is the source of randomness in each approach?
\end{enumerate}

Solution

In \textbf{design-based inference}, the potential outcomes \(Y_i(0)\)
and \(Y_i(1)\) are treated as \emph{fixed} values---they are simply the
outcomes each individual would experience under each condition. The only
source of randomness comes from the treatment assignment \(D_i\).
Because \emph{we} designed the randomization mechanism, we know exactly
how this randomness works.

In \textbf{model-based inference}, the outcomes themselves are treated
as random draws from some probability distribution. The randomness comes
from imagining that the data could have been different if we had drawn a
different sample from the population, or from random variation in how
outcomes are generated.

The key difference is \emph{where the uncertainty comes from}:

\begin{itemize}
\item
  Design-based: Uncertainty comes from the \emph{random assignment}. The
  potential outcomes are fixed; we just don't know which ones we'll
  observe because we don't know the randomization in advance.
\item
  Model-based: Uncertainty comes from the \emph{data-generating
  process}. The outcomes themselves are random, drawn from some
  distribution with unknown parameters.
\end{itemize}

Design-based inference is powerful because we don't need to make
assumptions about how outcomes are generated. We only rely on the
randomization we controlled.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{The Meaning of Unbiasedness}: A student says, ``Since
  \(\widehat{\text{ATE}}\) is unbiased, my estimate must equal the true
  ATE.'' Explain why this is incorrect. What does unbiasedness actually
  guarantee?
\end{enumerate}

Solution

The student's statement is incorrect. Unbiasedness does \emph{not} mean
that any single estimate equals the true ATE.

Unbiasedness is a property about \emph{averages over repeated
randomizations}. Specifically, \(\widehat{\text{ATE}}\) is unbiased
means:

\[E_D[\widehat{\text{ATE}}] = \text{ATE}\]

In plain language: if we could repeat the randomization infinitely many
times and compute \(\widehat{\text{ATE}}\) each time, the \emph{average}
of all those estimates would equal the true ATE.

Any \emph{particular} estimate will almost certainly differ from the
true ATE. Some randomizations will, by chance, put more high-outcome
individuals in the treatment group, leading to overestimates. Other
randomizations will put more high-outcome individuals in control,
leading to underestimates. Unbiasedness tells us that these over- and
under-estimates balance out \emph{on average}---there is no systematic
tendency to err in one direction.

The chapter example illustrates this: one randomization gave
\(\widehat{\text{ATE}} = 2.6\) (overestimate), another gave
\(\widehat{\text{ATE}} = 0.4\) (underestimate), but the true ATE was
1.5. Neither estimate equaled the truth, but they averaged out
correctly.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Why Randomization Matters}: Suppose instead of randomly
  assigning treatment, we let participants choose whether to receive
  treatment. Why would
  \(\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\)
  no longer be unbiased for the ATE?
\end{enumerate}

Solution

If participants choose whether to receive treatment, the people who
select into treatment may be systematically different from those who
select into control. This is called \textbf{self-selection bias}.

For example, in a job training program where participants choose whether
to enroll:

\begin{itemize}
\tightlist
\item
  People who are more motivated, more confident, or have better
  prospects might be more likely to enroll
\item
  These same characteristics might also lead to better outcomes
  \emph{regardless} of training
\end{itemize}

In this case, the treatment group would have higher average outcomes
than the control group even if the training itself had no effect. The
difference in means would reflect both (1) any true effect of training
and (2) pre-existing differences between the groups.

Mathematically, the proof of unbiasedness relies on the fact that
\(E_D[D_i] = N_t/N\) for all individuals \(i\)---every individual has
the same probability of being treated. With self-selection, this no
longer holds. Individuals with higher potential outcomes might have
higher probabilities of selecting treatment, which means \(E_D[D_i]\)
would vary across individuals and would be correlated with potential
outcomes.

When treatment assignment is correlated with potential outcomes, the
difference-in-means estimator captures both the treatment effect and the
selection effect, making it biased for the ATE.

\subsection{Computational Practice}\label{computational-practice-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Computing the Estimate}: Consider the following observed data
  from a randomized experiment:

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Individual & \(D_i\) & \(Y_i^{\text{obs}}\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 1 & 12 \\
  2 & 0 & 8 \\
  3 & 1 & 15 \\
  4 & 0 & 7 \\
  5 & 0 & 9 \\
  6 & 1 & 11 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    What are \(N_t\) and \(N_c\)?
  \item
    Compute \(\overline{Y}^{\text{obs}}_{t}\) and
    \(\overline{Y}^{\text{obs}}_{c}\).
  \item
    Compute \(\widehat{\text{ATE}}\).
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Count the individuals in each group by looking at the
\(D_i\) column:

\begin{itemize}
\tightlist
\item
  Individuals with \(D_i = 1\) (treatment): Individuals 1, 3, and 6
\item
  Individuals with \(D_i = 0\) (control): Individuals 2, 4, and 5
\end{itemize}

So \(N_t = 3\) and \(N_c = 3\).

\textbf{Part b.} Compute the mean outcome in each group.

For the treatment group (individuals where \(D_i = 1\)):

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{N_t} \sum_{i:D_i=1} Y_i^{\text{obs}} \\
&= \frac{1}{3}(12 + 15 + 11) \\
&= \frac{38}{3} \\
&\approx 12.67
\end{align}
\]

For the control group (individuals where \(D_i = 0\)):

\[
\begin{align}
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{N_c} \sum_{i:D_i=0} Y_i^{\text{obs}} \\
&= \frac{1}{3}(8 + 7 + 9) \\
&= \frac{24}{3} \\
&= 8
\end{align}
\]

\textbf{Part c.} The difference-in-means estimator is:

\[
\begin{align}
\widehat{\text{ATE}} &= \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c} \\
&= 12.67 - 8 \\
&= 4.67
\end{align}
\]

Our estimate is that treatment increases outcomes by about 4.67 units on
average.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Different Randomizations}: Suppose the true potential outcomes
  for 4 individuals are:

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Individual & \(Y_i(0)\) & \(Y_i(1)\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 2 & 5 \\
  2 & 4 & 6 \\
  3 & 3 & 4 \\
  4 & 5 & 7 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute the true ATE.
  \item
    If individuals 1 and 3 are assigned to treatment (\(D_1 = D_3 = 1\))
    and individuals 2 and 4 are assigned to control (\(D_2 = D_4 = 0\)),
    compute \(\widehat{\text{ATE}}\).
  \item
    If individuals 2 and 4 are assigned to treatment and individuals 1
    and 3 are assigned to control, compute \(\widehat{\text{ATE}}\).
  \item
    Average your two estimates from (b) and (c). How does this compare
    to the true ATE?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} First, compute the individual treatment effects:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Individual & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i = Y_i(1) - Y_i(0)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 5 & \(5 - 2 = 3\) \\
2 & 4 & 6 & \(6 - 4 = 2\) \\
3 & 3 & 4 & \(4 - 3 = 1\) \\
4 & 5 & 7 & \(7 - 5 = 2\) \\
\end{longtable}

The true ATE is:

\[
\begin{align}
\text{ATE} &= \frac{1}{4} \sum_{i=1}^{4} \tau_i \\
&= \frac{1}{4}(3 + 2 + 1 + 2) \\
&= \frac{8}{4} \\
&= 2
\end{align}
\]

\textbf{Part b.} With individuals 1 and 3 treated, and individuals 2 and
4 in control:

\begin{itemize}
\tightlist
\item
  Treatment group observes: \(Y_1(1) = 5\) and \(Y_3(1) = 4\)
\item
  Control group observes: \(Y_2(0) = 4\) and \(Y_4(0) = 5\)
\end{itemize}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(5 + 4) = \frac{9}{2} = 4.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(4 + 5) = \frac{9}{2} = 4.5 \\
\widehat{\text{ATE}} &= 4.5 - 4.5 = 0
\end{align}
\]

\textbf{Part c.} With individuals 2 and 4 treated, and individuals 1 and
3 in control:

\begin{itemize}
\tightlist
\item
  Treatment group observes: \(Y_2(1) = 6\) and \(Y_4(1) = 7\)
\item
  Control group observes: \(Y_1(0) = 2\) and \(Y_3(0) = 3\)
\end{itemize}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(6 + 7) = \frac{13}{2} = 6.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(2 + 3) = \frac{5}{2} = 2.5 \\
\widehat{\text{ATE}} &= 6.5 - 2.5 = 4
\end{align}
\]

\textbf{Part d.} The average of the two estimates:

\[
\frac{0 + 4}{2} = 2
\]

This equals the true ATE exactly! The first randomization underestimated
(0 vs.~2), and the second overestimated (4 vs.~2), but they balanced
out. This illustrates unbiasedness: on average, the estimator gets it
right.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Verifying Unbiasedness}: Using the same potential outcomes
  from Exercise 6, list all possible ways to assign exactly 2
  individuals to treatment and 2 to control. For each, compute
  \(\widehat{\text{ATE}}\). Then compute the average of all these
  estimates and verify that it equals the true ATE.
\end{enumerate}

Solution

With 4 individuals and 2 assigned to treatment, there are
\(\binom{4}{2} = 6\) possible randomizations. Let's list each one and
compute \(\widehat{\text{ATE}}\).

Recall the potential outcomes:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Individual & \(Y_i(0)\) & \(Y_i(1)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 5 \\
2 & 4 & 6 \\
3 & 3 & 4 \\
4 & 5 & 7 \\
\end{longtable}

\textbf{Randomization 1: Individuals 1, 2 treated; Individuals 3, 4
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(5 + 6) = 5.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(3 + 5) = 4 \\
\widehat{\text{ATE}} &= 5.5 - 4 = 1.5
\end{align}
\]

\textbf{Randomization 2: Individuals 1, 3 treated; Individuals 2, 4
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(5 + 4) = 4.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(4 + 5) = 4.5 \\
\widehat{\text{ATE}} &= 4.5 - 4.5 = 0
\end{align}
\]

\textbf{Randomization 3: Individuals 1, 4 treated; Individuals 2, 3
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(5 + 7) = 6 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(4 + 3) = 3.5 \\
\widehat{\text{ATE}} &= 6 - 3.5 = 2.5
\end{align}
\]

\textbf{Randomization 4: Individuals 2, 3 treated; Individuals 1, 4
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(6 + 4) = 5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(2 + 5) = 3.5 \\
\widehat{\text{ATE}} &= 5 - 3.5 = 1.5
\end{align}
\]

\textbf{Randomization 5: Individuals 2, 4 treated; Individuals 1, 3
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(6 + 7) = 6.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(2 + 3) = 2.5 \\
\widehat{\text{ATE}} &= 6.5 - 2.5 = 4
\end{align}
\]

\textbf{Randomization 6: Individuals 3, 4 treated; Individuals 1, 2
control}

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{2}(4 + 7) = 5.5 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{2}(2 + 4) = 3 \\
\widehat{\text{ATE}} &= 5.5 - 3 = 2.5
\end{align}
\]

\textbf{Summary of all estimates:}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Randomization & Treated Individuals & \(\widehat{\text{ATE}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1, 2 & 1.5 \\
2 & 1, 3 & 0 \\
3 & 1, 4 & 2.5 \\
4 & 2, 3 & 1.5 \\
5 & 2, 4 & 4 \\
6 & 3, 4 & 2.5 \\
\end{longtable}

\textbf{Average of all estimates:}

\[
\begin{align}
E_D[\widehat{\text{ATE}}] &= \frac{1}{6}(1.5 + 0 + 2.5 + 1.5 + 4 + 2.5) \\
&= \frac{12}{6} \\
&= 2
\end{align}
\]

The average of all possible estimates equals 2, which is exactly the
true ATE we computed in Exercise 6. This confirms that
\(\widehat{\text{ATE}}\) is unbiased: averaged across all possible
randomizations, it equals the truth.

\subsection{Application to Rasinski's
Experiment}\label{application-to-rasinskis-experiment-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Rasinski's Experiment Revisited}: Return to the hypothetical
  Rasinski data from the previous chapter. Suppose we randomly assign 5
  respondents to the ``assistance to the poor'' wording (treatment) and
  5 to the ``welfare'' wording (control).

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Respondent & \(Y_i(0)\) (``welfare'') & \(Y_i(1)\) (``assistance'') \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 3 (Too much) & 1 (Too little) \\
  2 & 3 (Too much) & 2 (About right) \\
  3 & 2 (About right) & 1 (Too little) \\
  4 & 3 (Too much) & 1 (Too little) \\
  5 & 2 (About right) & 1 (Too little) \\
  6 & 3 (Too much) & 2 (About right) \\
  7 & 2 (About right) & 1 (Too little) \\
  8 & 3 (Too much) & 1 (Too little) \\
  9 & 2 (About right) & 2 (About right) \\
  10 & 3 (Too much) & 1 (Too little) \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    We know from the previous chapter that the true ATE is -1.3. If
    respondents 1, 3, 5, 7, and 9 are assigned to treatment, compute
    \(\widehat{\text{ATE}}\). How close is it to the true ATE?
  \item
    If respondents 2, 4, 6, 8, and 10 are assigned to treatment instead,
    compute \(\widehat{\text{ATE}}\). How close is it to the true ATE?
  \item
    Average your estimates from (a) and (b). What do you notice?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} With respondents 1, 3, 5, 7, and 9 assigned to
treatment:

\begin{itemize}
\tightlist
\item
  Treatment group (sees ``assistance to the poor''): Respondents 1, 3,
  5, 7, 9

  \begin{itemize}
  \tightlist
  \item
    Observed outcomes: \(Y_1(1) = 1\), \(Y_3(1) = 1\), \(Y_5(1) = 1\),
    \(Y_7(1) = 1\), \(Y_9(1) = 2\)
  \end{itemize}
\item
  Control group (sees ``welfare''): Respondents 2, 4, 6, 8, 10

  \begin{itemize}
  \tightlist
  \item
    Observed outcomes: \(Y_2(0) = 3\), \(Y_4(0) = 3\), \(Y_6(0) = 3\),
    \(Y_8(0) = 3\), \(Y_{10}(0) = 3\)
  \end{itemize}
\end{itemize}

Computing the means:

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{5}(1 + 1 + 1 + 1 + 2) = \frac{6}{5} = 1.2 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{5}(3 + 3 + 3 + 3 + 3) = \frac{15}{5} = 3 \\
\widehat{\text{ATE}} &= 1.2 - 3 = -1.8
\end{align}
\]

This estimate of \(-1.8\) is 0.5 units away from the true ATE of
\(-1.3\). It overestimates the magnitude of the effect.

\textbf{Part b.} With respondents 2, 4, 6, 8, and 10 assigned to
treatment:

\begin{itemize}
\tightlist
\item
  Treatment group: Respondents 2, 4, 6, 8, 10

  \begin{itemize}
  \tightlist
  \item
    Observed outcomes: \(Y_2(1) = 2\), \(Y_4(1) = 1\), \(Y_6(1) = 2\),
    \(Y_8(1) = 1\), \(Y_{10}(1) = 1\)
  \end{itemize}
\item
  Control group: Respondents 1, 3, 5, 7, 9

  \begin{itemize}
  \tightlist
  \item
    Observed outcomes: \(Y_1(0) = 3\), \(Y_3(0) = 2\), \(Y_5(0) = 2\),
    \(Y_7(0) = 2\), \(Y_9(0) = 2\)
  \end{itemize}
\end{itemize}

Computing the means:

\[
\begin{align}
\overline{Y}^{\text{obs}}_{t} &= \frac{1}{5}(2 + 1 + 2 + 1 + 1) = \frac{7}{5} = 1.4 \\
\overline{Y}^{\text{obs}}_{c} &= \frac{1}{5}(3 + 2 + 2 + 2 + 2) = \frac{11}{5} = 2.2 \\
\widehat{\text{ATE}} &= 1.4 - 2.2 = -0.8
\end{align}
\]

This estimate of \(-0.8\) is 0.5 units away from the true ATE of
\(-1.3\). It underestimates the magnitude of the effect.

\textbf{Part c.} The average of the two estimates:

\[
\frac{-1.8 + (-0.8)}{2} = \frac{-2.6}{2} = -1.3
\]

The average equals the true ATE exactly! This is a concrete illustration
of unbiasedness. The first randomization overestimated the effect (in
magnitude), and the second underestimated it by the same amount. When we
average them, we get exactly the truth.

Note: These two randomizations are not the only possible ones---there
are \(\binom{10}{5} = 252\) ways to assign 5 of 10 respondents to
treatment. If we computed \(\widehat{\text{ATE}}\) for all 252 and
averaged them, we would get exactly \(-1.3\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  \textbf{What the Researcher Sees}: In Rasinski's actual experiment,
  the researcher does not observe the potential outcomes table---they
  only observe the outcomes under the assigned condition.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Using the randomization from Exercise 8a (respondents 1, 3, 5, 7, 9
    assigned to treatment), write out the data table that the researcher
    would actually observe. Use ``?'' for unobserved potential outcomes.
  \item
    Can the researcher compute the true ATE from this observed data? Why
    or why not?
  \item
    What does the researcher compute instead, and why is this a
    reasonable substitute?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} With respondents 1, 3, 5, 7, 9 assigned to treatment,
the researcher observes:

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & ? & 1 & 1 & 1 \\
2 & 3 & ? & 0 & 3 \\
3 & ? & 1 & 1 & 1 \\
4 & 3 & ? & 0 & 3 \\
5 & ? & 1 & 1 & 1 \\
6 & 3 & ? & 0 & 3 \\
7 & ? & 1 & 1 & 1 \\
8 & 3 & ? & 0 & 3 \\
9 & ? & 2 & 1 & 2 \\
10 & 3 & ? & 0 & 3 \\
\end{longtable}

For each respondent, the researcher sees only one potential
outcome---the one corresponding to their assigned condition. The other
potential outcome is forever unknown.

\textbf{Part b.} No, the researcher cannot compute the true ATE from
this observed data.

The true ATE requires knowing \emph{all} potential outcomes:

\[\text{ATE} = \frac{1}{N}\sum_{i=1}^{N} Y_i(1) - \frac{1}{N}\sum_{i=1}^{N} Y_i(0)\]

But the researcher is missing \(Y_i(0)\) for treated respondents
(respondents 1, 3, 5, 7, 9) and \(Y_i(1)\) for control respondents
(respondents 2, 4, 6, 8, 10). Without these counterfactual outcomes, the
true ATE cannot be calculated.

\textbf{Part c.} The researcher computes the difference-in-means
estimator:

\[\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\]

This is a reasonable substitute because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It uses only observable quantities (no counterfactuals needed)
\item
  Under random assignment, it is \emph{unbiased} for the true ATE
\item
  The average of \(Y_i^{\text{obs}}\) among treated individuals
  estimates what the average of \(Y_i(1)\) would be for everyone
\item
  The average of \(Y_i^{\text{obs}}\) among control individuals
  estimates what the average of \(Y_i(0)\) would be for everyone
\end{enumerate}

Random assignment ensures that the treated and control groups are, on
average, comparable samples from the same population. This allows us to
use observed group means as stand-ins for the unobservable population
means.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  \textbf{Interpreting the Estimate in Context}: Using the estimate from
  Exercise 8a (\(\widehat{\text{ATE}} = -1.8\)):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Write a one-sentence interpretation of this estimate in the context
    of Rasinski's question-wording experiment. Be sure to explain what
    the negative sign means substantively.
  \item
    A colleague says, ``A change of 1.8 points on a 3-point scale is
    huge!'' Is this a fair characterization? What context would help
    evaluate the magnitude?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Based on this experiment, respondents gave answers that
were on average 1.8 points lower on the 1-to-3 scale (where 1 = ``Too
little'' and 3 = ``Too much'') when asked about ``assistance to the
poor'' compared to ``welfare,'' indicating that the ``assistance to the
poor'' framing substantially increases expressed support for government
spending.

\textbf{Part b.} The colleague's characterization captures the right
intuition---this is a large effect---but deserves more context:

\emph{What makes this effect large:}

\begin{itemize}
\tightlist
\item
  The outcome scale only has three categories (1, 2, 3), so the maximum
  possible effect is 2 points (from ``Too much'' to ``Too little'' or
  vice versa)
\item
  An effect of 1.8 points is 90\% of the maximum possible effect
\item
  Many respondents are shifting by a full category or more based solely
  on word choice
\end{itemize}

\emph{Context that helps evaluation:}

\begin{itemize}
\tightlist
\item
  The treatment is minimal---just changing one phrase in a question. The
  fact that word choice alone produces such a large shift suggests
  question wording is extremely consequential for survey results.
\item
  If this effect generalizes to the broader population, it means public
  opinion polls could show dramatically different levels of support for
  the same policy depending on how the question is worded.
\item
  In political terms, this could mean the difference between a policy
  appearing popular or unpopular.
\end{itemize}

So yes, the effect is substantively large, and the colleague's reaction
is warranted. Changing two words shifts responses by nearly the full
width of the scale.

\subsection{Critical Thinking}\label{critical-thinking-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\item
  \textbf{Unbiasedness vs.~Accuracy}: An estimator can be unbiased but
  still give estimates that are far from the true value.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    How is this possible? What determines how far individual estimates
    might be from the truth?
  \item
    What might make \(\widehat{\text{ATE}}\) more or less variable
    across different randomizations?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Unbiasedness only guarantees that the estimator is
correct \emph{on average}---it says nothing about any single estimate.

An unbiased estimator can produce estimates far from the truth because
of \emph{sampling variability}. Each randomization produces a different
assignment of individuals to treatment and control. Some randomizations
might, by chance, put individuals with unusually high potential outcomes
into the treatment group, leading to overestimates. Other randomizations
might do the opposite, leading to underestimates.

Think of it like flipping a fair coin 10 times. On average, you get 5
heads. But any particular sequence might give you 3 heads, 7 heads, or
even 0 heads. The ``estimator'' (count the heads) is unbiased, but
individual realizations vary.

What determines how far individual estimates might be from the truth:

\begin{itemize}
\tightlist
\item
  The variance of the estimator---how spread out the distribution of
  possible estimates is
\item
  This variance depends on the sample size, the variability in potential
  outcomes, and the proportion assigned to treatment vs.~control
\end{itemize}

\textbf{Part b.} Several factors affect how variable
\(\widehat{\text{ATE}}\) is across randomizations:

\emph{Factors that increase variability:}

\begin{itemize}
\tightlist
\item
  Smaller sample sizes (\(N\)): Fewer individuals means each
  randomization can produce more extreme group compositions
\item
  More heterogeneous potential outcomes: If individuals differ greatly
  in their outcomes, the groups can be quite different depending on
  which individuals land where
\item
  Unbalanced designs: Having very few individuals in one group (e.g.,
  \(N_t = 2\) out of \(N = 100\)) increases variability because the
  small group's mean is very sensitive to which specific individuals are
  included
\end{itemize}

\emph{Factors that decrease variability:}

\begin{itemize}
\tightlist
\item
  Larger sample sizes: With many individuals, the law of large numbers
  kicks in---each group is more likely to be representative of the
  population
\item
  More homogeneous potential outcomes: If all individuals have similar
  outcomes, it matters less which ones end up in which group
\item
  Balanced designs: Having roughly equal numbers in treatment and
  control (\(N_t \approx N_c\)) tends to minimize variance
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\item
  \textbf{Breaking Unbiasedness}: The proof of unbiasedness relies on
  the fact that \(E_D[D_i] = N_t/N\) for all individuals \(i\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Give an example of an assignment mechanism where this would not
    hold.
  \item
    If we used such a biased assignment mechanism, would
    \(\widehat{\text{ATE}}\) still be unbiased? Why or why not?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Here are examples of assignment mechanisms where
\(E_D[D_i] = N_t/N\) would not hold for all individuals:

\emph{Example 1: Selection based on a characteristic}

A researcher assigns older participants to treatment with probability
0.8 and younger participants with probability 0.2. If age is associated
with the outcome, \(E_D[D_i]\) differs across individuals based on their
age.

\emph{Example 2: Self-selection}

Participants choose whether to receive treatment. Motivated participants
might be more likely to choose treatment (\(E_D[D_i]\) is higher for
motivated people).

\emph{Example 3: Researcher discretion}

A researcher ``randomly'' assigns treatment but subconsciously tends to
assign sicker patients to the new treatment. Sicker patients have higher
\(E_D[D_i]\).

\emph{Example 4: Sequential assignment with learning}

A researcher stops assigning to treatment once they see a few bad
outcomes, making later individuals less likely to be treated.

\textbf{Part b.} No, \(\widehat{\text{ATE}}\) would generally not be
unbiased under such mechanisms.

The proof of unbiasedness relied on a key cancellation: the probability
of treatment (\(E_D[D_i] = N_t/N\)) canceled with the weighting in the
estimator (dividing by \(N_t\)). This worked because every individual
had the \emph{same} probability of treatment.

When treatment probabilities vary across individuals, this cancellation
breaks down. Individuals with higher probabilities of treatment
contribute more to the treatment group mean, and individuals with lower
probabilities contribute less. If these probabilities are correlated
with potential outcomes, the estimator becomes biased.

For example, if high-outcome individuals have higher treatment
probabilities:

\begin{itemize}
\tightlist
\item
  The treatment group will be enriched with high-outcome individuals
\item
  The control group will be enriched with low-outcome individuals
\item
  \(\overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\) will
  overestimate the true ATE
\end{itemize}

The estimator would be capturing both the treatment effect \emph{and}
the selection effect, leading to bias.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\item
  \textbf{Sample Size Considerations}: Consider two experiments
  estimating the same ATE. Experiment A has \(N = 10\) with \(N_t = 5\).
  Experiment B has \(N = 1000\) with \(N_t = 500\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Are both estimates unbiased for the ATE?
  \item
    In which experiment would you expect \(\widehat{\text{ATE}}\) to be
    closer to the true ATE? Why?
  \item
    What property of estimators (besides unbiasedness) captures this
    difference?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Yes, both estimates are unbiased for the ATE.

Unbiasedness depends only on the \emph{randomization procedure}, not on
the sample size. As long as treatment is randomly assigned (with each
individual having equal probability), the estimator is unbiased. This is
true whether \(N = 10\) or \(N = 1,000,000\).

The proof of unbiasedness holds for any \(N > 1\) with \(1 < N_t < N\).

\textbf{Part b.} We would expect \(\widehat{\text{ATE}}\) from
Experiment B (the larger experiment) to be closer to the true ATE.

With only 10 individuals, random chance can produce quite
unrepresentative groups. Maybe by luck, 4 of the 5 highest-outcome
individuals all end up in treatment, producing a large overestimate.
With 1000 individuals, such extreme imbalances are far less likely. The
law of large numbers ensures that with more individuals, each group's
mean converges toward the population mean of that potential outcome.

To use a simple analogy: if you flip a coin 10 times, getting 8 heads
(80\%) wouldn't be surprising. If you flip it 1000 times, getting 800
heads (80\%) would be extremely unlikely. Larger samples are more
stable.

\textbf{Part c.} The property that captures this difference is the
\textbf{variance} (or equivalently, the \textbf{standard error}) of the
estimator.

Both estimators are unbiased, but the estimator from the larger
experiment has smaller variance---its sampling distribution is more
concentrated around the true ATE. We often describe this by saying the
larger experiment is more \textbf{precise} or has more
\textbf{efficiency}.

Related concepts include:

\begin{itemize}
\tightlist
\item
  Standard error: The standard deviation of the sampling distribution of
  \(\widehat{\text{ATE}}\)
\item
  Efficiency: An estimator with smaller variance is more efficient
\item
  Consistency: As \(N \to \infty\), the estimator converges to the true
  value (related to variance shrinking to zero)
\end{itemize}

\subsection{R Practice}\label{r-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\item
  \textbf{Computing \(\widehat{\text{ATE}}\) with \texttt{mean()}}:
  Suppose you have a data frame called \texttt{experiment} with columns
  \texttt{treatment} (1 = treated, 0 = control) and \texttt{outcome} (a
  numeric outcome variable).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Write R code to compute \(\overline{Y}^{\text{obs}}_{t}\) using
    logical indexing.
  \item
    Write R code to compute \(\overline{Y}^{\text{obs}}_{c}\) using
    logical indexing.
  \item
    Write R code to compute \(\widehat{\text{ATE}}\) as the difference.
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} To compute \(\overline{Y}^{\text{obs}}_{t}\), the mean
outcome among treated individuals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_treated }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(experiment}\SpecialCharTok{$}\NormalTok{outcome[experiment}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

This works by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{experiment\$treatment\ ==\ 1} creates a logical vector that is
  \texttt{TRUE} for treated individuals
\item
  \texttt{experiment\$outcome{[}...{]}} subsets the outcome variable to
  only those rows
\item
  \texttt{mean(...)} computes the average of the subsetted outcomes
\end{enumerate}

\textbf{Part b.} To compute \(\overline{Y}^{\text{obs}}_{c}\), the mean
outcome among control individuals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_control }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(experiment}\SpecialCharTok{$}\NormalTok{outcome[experiment}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

The logic is identical, but we select rows where
\texttt{treatment\ ==\ 0}.

\textbf{Part c.} To compute \(\widehat{\text{ATE}}\) as the difference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate\_hat }\OtherTok{\textless{}{-}}\NormalTok{ mean\_treated }\SpecialCharTok{{-}}\NormalTok{ mean\_control}
\end{Highlighting}
\end{Shaded}

Or as a single line without intermediate variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate\_hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(experiment}\SpecialCharTok{$}\NormalTok{outcome[experiment}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{{-}}
           \FunctionTok{mean}\NormalTok{(experiment}\SpecialCharTok{$}\NormalTok{outcome[experiment}\SpecialCharTok{$}\NormalTok{treatment }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\item
  \textbf{Understanding \texttt{lm()} Output}: A researcher fits the
  model
  \texttt{lm(outcome\ \textasciitilde{}\ treatment,\ data\ =\ experiment)}
  and gets the following output:

\begin{verbatim}
(Intercept)    treatment
      4.200        1.350
\end{verbatim}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    What is the mean outcome in the control group?
  \item
    What is \(\widehat{\text{ATE}}\)?
  \item
    What is the mean outcome in the treatment group?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The mean outcome in the control group is
\textbf{4.200}.

The intercept represents the predicted outcome when
\texttt{treatment\ =\ 0}. Since the treatment variable is binary (0 or
1), plugging in 0 gives:

\[\hat{Y} = 4.200 + 1.350 \times 0 = 4.200\]

This is \(\overline{Y}^{\text{obs}}_{c}\).

\textbf{Part b.} The \(\widehat{\text{ATE}}\) is \textbf{1.350}.

The coefficient on \texttt{treatment} represents how much the predicted
outcome changes when treatment goes from 0 to 1. This is exactly the
difference in means:

\[\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c} = 1.350\]

\textbf{Part c.} The mean outcome in the treatment group is
\textbf{5.550}.

This is the predicted outcome when \texttt{treatment\ =\ 1}:

\[\hat{Y} = 4.200 + 1.350 \times 1 = 5.550\]

This equals
\(\overline{Y}^{\text{obs}}_{c} + \widehat{\text{ATE}} = 4.200 + 1.350 = 5.550\).

We can verify:
\(\overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c} = 5.550 - 4.200 = 1.350\),
which matches the coefficient.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\tightlist
\item
  \textbf{Reference Categories}: Explain why we used
  \texttt{fct\_relevel(desc,\ "welfare")} in the R code for the mock
  Rasinski data. What would happen if we had not done this? Would the
  \(\widehat{\text{ATE}}\) computed using \texttt{mean()} and
  \texttt{lm()} still match?
\end{enumerate}

Solution

We used \texttt{fct\_relevel(desc,\ "welfare")} to set ``welfare'' as
the \textbf{reference category} for the factor variable.

\textbf{Why this matters for \texttt{lm()}:}

When \texttt{lm()} encounters a factor variable, it creates dummy
(indicator) variables. R uses the first level of the factor as the
reference category---the category that gets coded as 0. By default, R
orders factor levels alphabetically. Since ``assistance to the poor''
comes before ``welfare'' alphabetically, it would be the reference
category by default.

With ``assistance to the poor'' as reference:

\begin{itemize}
\tightlist
\item
  The intercept would be the mean for ``assistance to the poor'' (the
  treatment group)
\item
  The coefficient would be the change when moving to ``welfare'' (the
  control group)
\item
  The coefficient would be
  \(\overline{Y}^{\text{obs}}_{c} - \overline{Y}^{\text{obs}}_{t}\),
  which is the \emph{negative} of our usual definition
\end{itemize}

By using \texttt{fct\_relevel(desc,\ "welfare")}, we make ``welfare''
the reference:

\begin{itemize}
\tightlist
\item
  The intercept is the mean for ``welfare'' (control group)
\item
  The coefficient is the change when moving to ``assistance to the
  poor'' (treatment)
\item
  The coefficient equals
  \(\overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\),
  matching our definition
\end{itemize}

\textbf{Would \texttt{mean()} and \texttt{lm()} still match?}

The magnitudes would still match, but the signs might differ depending
on how we set up the calculation:

\begin{itemize}
\tightlist
\item
  If we compute \texttt{mean()} as treatment minus control and
  \texttt{lm()} uses the alphabetical default, the coefficient would
  have the \emph{opposite sign} of our \texttt{mean()} calculation
\item
  The numerical values would still be correct in absolute value
\end{itemize}

In practice, for the two approaches to give the same number (not just
the same magnitude), we need to ensure the reference category in
\texttt{lm()} matches which group we subtract in the \texttt{mean()}
calculation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{16}
\tightlist
\item
  \textbf{Connecting \texttt{mean()} and \texttt{lm()}}: In your own
  words, explain why regressing the outcome on a binary treatment
  indicator produces a coefficient that equals the difference in means.
  What role does the intercept play in this relationship?
\end{enumerate}

Solution

When we regress outcome on a binary treatment indicator, OLS (ordinary
least squares) finds the coefficients that minimize the sum of squared
residuals---the differences between observed outcomes and predicted
outcomes.

\textbf{The key insight:}

For a binary predictor, the predictions can only take two values:

\begin{itemize}
\tightlist
\item
  When \(D_i = 0\): \(\hat{Y}_i = \hat{\beta}_0\)
\item
  When \(D_i = 1\): \(\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1\)
\end{itemize}

OLS chooses \(\hat{\beta}_0\) and \(\hat{\beta}_1\) to make predicted
values as close as possible to observed values. The best prediction for
a group of observations is their mean (this minimizes squared errors
within that group).

Therefore:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}_0\) is set to the mean outcome in the control group,
  because that's the best prediction for control individuals
\item
  \(\hat{\beta}_0 + \hat{\beta}_1\) is set to the mean outcome in the
  treatment group, because that's the best prediction for treated
  individuals
\end{itemize}

\textbf{Solving for the coefficient:}

\[\hat{\beta}_0 = \overline{Y}^{\text{obs}}_{c}\]
\[\hat{\beta}_0 + \hat{\beta}_1 = \overline{Y}^{\text{obs}}_{t}\]

Subtracting the first equation from the second:

\[\hat{\beta}_1 = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\]

This is exactly the difference in means---our \(\widehat{\text{ATE}}\).

\textbf{The role of the intercept:}

The intercept (\(\hat{\beta}_0\)) serves as the ``baseline''---the
predicted outcome for the reference group (control). It anchors the
predictions. The coefficient (\(\hat{\beta}_1\)) then represents how
much we add or subtract from this baseline when an individual is
treated. This additive structure means the coefficient must equal the
difference between the two group means.

\bookmarksetup{startatroot}

\chapter{\texorpdfstring{The Variance of
\(\widehat{\text{ATE}}\)}{The Variance of \textbackslash widehat\{\textbackslash text\{ATE\}\}}}\label{the-variance-of-widehattextate}

In the previous chapter, we showed that the difference-in-means
estimator
\(\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_{t} - \overline{Y}^{\text{obs}}_{c}\)
is an unbiased estimator of the average treatment effect. Unbiasedness
tells us that our estimator gets it right \emph{on average} in the
long-run.\footnotemark{}

But unbiasedness is only part of the story. An estimator can be unbiased
yet still produce estimates that are far from the true value in any
given experiment. What we need to understand is \emph{how much} our
estimate varies from one randomization to another. We want our procedure
to produce an estimate that is \emph{right on average} AND \emph{usually
close} to the ATE. The \textbf{variance} of the estimator tells us
\emph{how close} the estimate falls to the ATE in the long run.

In this chapter, we state the variance formula for
\(\widehat{\text{ATE}}\) under completely randomized experiments and
explain what it means. The proof, which involves very tedious algebra
(but \emph{only} tedious algebra), appears in Appendix A.

Understanding this variance formula will help us understand the
noisiness of our estimates and think carefully about how to design
experiments.

\section{\texorpdfstring{The Variance Operator
\(\text{Var}[\cdot]\)}{The Variance Operator \textbackslash text\{Var\}{[}\textbackslash cdot{]}}}\label{the-variance-operator-textvarcdot}

The variance operator \(\text{Var}[\cdot]\) is a way of measuring ``how
spread out'' a random variable is around its average value. The variance
of a random variable measures its typical \emph{squared} distance from
the mean.

\textbf{Rule: Variance is defined as the expected squared deviation from
the mean.}

If \(X\) is random, then

\[
\text{Var}(X) = E\left[(X - E[X])^2\right].
\]

Notice the logic. We're just computing the long-run average of
\((X - E[X])^2\), which is the \emph{squared} difference between \(X\)
and its own long-run average.

\marginnote{\begin{footnotesize}

It also turns out that

\[
\text{Var}(X) = E\left[X^2 \right] - \left(E[X]\right)^2, 
\]

which can be a little easier to work with, but is somewhat harder to
conceptualize.

A simulation helps illustrate. Suppose we sample once from the
collection -2, -1, 0, 1, 2. What's the variance of this sample?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# a collection of numbers (that we\textquotesingle{}ll sample from below)}
\NormalTok{collection }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\CommentTok{\# take one sample from the collection}
\FunctionTok{sample}\NormalTok{(collection, }\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -1
\end{verbatim}

We might wonder---what's the variance of this sample?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# take and store many (approximating long{-}run) samples from the collection (with replacement)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(collection, }\AttributeTok{size =} \DecValTok{100000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# compute variance in several steps }
\NormalTok{avg }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{sq\_dev }\OtherTok{\textless{}{-}}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ avg)}\SpecialCharTok{\^{}}\DecValTok{2}
\FunctionTok{mean}\NormalTok{(sq\_dev)  }\CommentTok{\# avg squared deviation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.994083
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute in one step}
\FunctionTok{mean}\NormalTok{((x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.994083
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{We don't need a computer to find this, though!}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

We don't need a computer to approximate the variance; we can use our
knowledge of expected values to find it exactly. We can compute it most
easily using the definition
\(\text{Var}(X) = E\left[X^2 \right] - \left(E[X]\right)^2\) mentioned
in the aside above.

Let \(X\) be a random variable that takes values in \(\{-2,-1,0,1,2\}\)
with equal probability \(1/5\). This is what the code is doing when it
samples \texttt{x}.

We need to compute both \(E\left[X^2 \right]\) and
\(\left(E[X]\right)^2\). Let's start with the second, because it's
easier.

First compute \(E[X]\). This is easy.

\[
E[X] = \frac{1}{5}(-2 - 1 + 0 + 1 + 2) = 0.
\]

Since \(E[X] = 0\), \(E[X]^2 = 0\) and the second term on the right-hand
side drops out.

Next, compute \(E\left[X^2 \right]\).

\[
E[X^2]
= \frac{1}{5}\left( (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 \right)
= \frac{10}{5}
= 2.
\]

Therefore, \[
\text{Var}(X) = E\left[X^2 \right] - \left(E[X]\right)^2 = 2 - 0 = 2.
\]

\section{The Main Theorem}\label{the-main-theorem}

Now that we understand how variance measures the spread of a random
variable, we can state the variance of our estimator
\(\widehat{\text{ATE}}\). Remember, because we are choosing to
\emph{randomly} assign participants to treatment and control, our
procedure produces a \emph{random} estimate.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Theorem: Variance of the Difference-in-Means Estimator}, breakable, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Under completely randomized assignment, the variance of
\(\widehat{\text{ATE}}\) is

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N},
\]

where \(S_t^2\) is defined as

\[
S_t^2 = \frac{1}{N - 1} \sum_{i=1}^{N} (Y_i(1) - \overline{Y}(1))^2 \quad \text{and},
\]

\(S_c^2\) is defined as

\[
 S_c^2 = \frac{1}{N - 1} \sum_{i=1}^{N} (Y_i(0) - \overline{Y}(0))^2,
\]

and \(S_{tc}^2\) is defined as

\[
S_{tc}^2 = \frac{1}{N - 1} \sum_{i=1}^{N} \left(\tau_i - \text{ATE}\right)^2.
\]

\marginnote{\begin{footnotesize}

Somewhat (or perhaps \emph{very}) confusingly, \(S_t^2\), \(S_c^2\), and
\(S_{tc}^2\) are also called ``variance.'' We use the variance
\(\text{Var}(X) = E\left[(X - E[X])^2\right]\) to describe the spread of
random numbers (like noisy estimates) and the variances \(S_t^2\),
\(S_c^2\), and \(S_{tc}^2\) to describe the spread of a fixed collection
of numbers (like potential outcomes under treatment). Statisticians
sometimes call the latter \textbf{finite population variances} to
distinguish them.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{On the \(1/(N-1)\) Convention}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

You may notice that \(S_t^2\), \(S_c^2\), and \(S_{tc}^2\) are defined
with \(1/(N-1)\) in the denominator rather than \(1/N\). This follows
the convention used by Cochran (1977) and later adopted by Imbens and
Rubin (2015).

This choice differs from the arguably more ``elementary'' variance

\[
V_N = \frac{1}{N} \sum_{i=1}^N (x_i - \bar{x})^2,
\]

which is often introduced in introductory texts and interpretable as the
average squared deviation from the mean.

The \(1/(N-1)\) convention has a key advantage: it aligns the definition
of the population variance with the sample variance estimator, so that
sample variances are unbiased estimators of population variances. This
simplifies the algebra of variance estimation in randomized experiments.
While readers accustomed to the \(1/N\) normalization may find this
convention unfamiliar, for design-based inference---where randomness
arises from treatment assignment---the \(1/(N-1)\) normalization offers
conceptual and algebraic advantages.

Let's unpack this formula.

\subsection{Understanding the Variance
Formula}\label{understanding-the-variance-formula}

Before interpreting the formula, let's understand what each quantity
measures. \(S_t^2\) measures how spread out the potential outcomes under
treatment are across individuals. \(S_c^2\) measures the same for
control. And \(S_{tc}^2\) measures how spread out the individual
treatment effects are---that is, how much the treatment effect varies
from person to person.

The variance formula has three terms.

\textbf{Term 1: \(\frac{S_t^2}{N_t}\)} This is the variance of the
treatment group mean. If potential outcomes under treatment vary a lot
across individuals (large \(S_t^2\)), our estimate of the treatment
group mean will be more variable. Dividing by \(N_t\) reflects that
larger treatment groups give us more precise estimates.

\textbf{Term 2: \(\frac{S_c^2}{N_c}\)} Similarly, this is the variance
of the control group mean. More variability in control potential
outcomes or fewer control individuals increases variance.

\textbf{Term 3: \(-\frac{S_{tc}^2}{N}\)} This is the interesting (and
difficult) term. Importantly, it \emph{shrinks} the variance. The
quantity \(S_{tc}^2\) measures how variable the individual treatment
effects are. If everyone has the same treatment effect (constant
effects), then \(S_{tc}^2 = 0\) and this term disappears.

\subsection{The Special Case of Constant Treatment
Effects}\label{the-special-case-of-constant-treatment-effects}

When treatment effects are constant (every individual experiences the
same effect \(\tau\)), we have \(\tau_i = \tau\) for all \(i\). In this
case, something remarkable happens: \(S_{tc}^2 = 0\). In this case, the
variance simplifies to

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c}.
\]

\marginnote{\begin{footnotesize}

Moreover, when effects are constant, \(S_t^2 = S_c^2 = S^2\) (the
variance of potential outcomes is the same under treatment and control),
so

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = S^2 \left(\frac{1}{N_t} + \frac{1}{N_c}\right).
\]

This is the familiar variance formula you might have seen in an
introductory statistics course for the difference of two means.

\section{Intuition}\label{intuition}

The algebraic proof of the variance formula is long, but the intuition
of the result is straightforward.

The variance of \(\widehat{\text{ATE}}\) reflects how much our estimate
would change if we repeated the \emph{same} experiment many
times.\footnotemark{}

\subsection{Two Obvious Sources of
Variation}\label{two-obvious-sources-of-variation}

There are two obvious reasons why \(\widehat{\text{ATE}}\) varies across
repetitions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Randomness in the treatment-group mean.} Different
  randomizations place different individuals into the treatment group.
  Since individuals have different potential outcomes under treatment,
  the treatment-group mean \(\overline{Y}^{\text{obs}}_t\) will not be
  exactly the same every time. The amount it varies depends on the
  spread of the potential outcomes (\(S_t^2\)) and the number of
  individuals assigned to treatment (\(N_t\)). This produces the term
  \(\frac{S_t^2}{N_t}\).
\item
  \textbf{Randomness in the control-group mean.} The same logic applies
  to the control group. Different randomizations create different
  control groups, so the control-group mean also varies in the same way
  as the treatment group.
\end{enumerate}

If these two sources of variation were unrelated, the variance of
\(\widehat{\text{ATE}}\) would simply be the sum of these two terms.
However, they are not unrelated. They are related in a subtle,
interesting, and important way.

\subsection{Why the Third Term
Matters}\label{why-the-third-term-matters}

Each individual can be in \textbf{only one} group. If someone is
assigned to treatment, they cannot be assigned to control. This creates
a link between the two group means.

To understand why this matters, focus on an individual with an
\textbf{unusually large treatment effect}. For this person, the
difference between their treated outcome \(Y_i(1)\) and their control
outcome \(Y_i(0)\) is especially large.\footnotemark{}

Now consider what happens when this individual is assigned to the
treatment group. Their treated outcome \(Y_i(1)\) is relatively high, so
their presence pushes the treatment-group mean upward.

But assigning them to treatment also means they are \emph{missing} from
the control group. Because their treatment effect is unusually large,
their control outcome \(Y_i(0)\) is relatively low. Had they been
assigned to control, their low outcome would have pulled the mean of the
control-group downward. Since they are not in the control group, that
downward pull doesn't happen.

As a result, two things happen.

\begin{itemize}
\tightlist
\item
  The treatment-group mean is \textbf{higher than usual}, because this
  individual is included.
\item
  The control-group mean is \textbf{also higher than usual}, because
  this individual is missing.
\end{itemize}

These effects partially cancel out when we take the difference between
the treatment and control group.

This stabilizing effect comes specifically from individuals with
\textbf{unusually large (or unusually small) treatment effects}.
Individuals with ``typical'' treatment effects do not create this
cancelling dynamic. The more treatment effects vary across people, the
stronger this stabilizing effect becomes. That variation is exactly what
\(S_{tc}^2\) measures.

This is why the variance formula includes the negative term
\(-\frac{S_{tc}^2}{N}\).

And then, if treatment effects are constant, then no one has unusually
large (or unusually small) treatment effect and the cancelling dynamic
is absent.

\section{Numerical Illustration}\label{numerical-illustration}

Let's verify the variance formula with a simulation. We'll create a
small population of 10 individuals with known potential outcomes,
compute the theoretical variance using the formula, and then simulate
1,000 randomizations to see if the simulated variance matches.

\subsection{Step 1: Create the Potential
Outcomes}\label{step-1-create-the-potential-outcomes}

First, we create potential outcomes for 10 individuals. We'll use the
same hypothetical data from the previous chapters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# create potential outcomes for 10 individuals}
\NormalTok{po }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
  \AttributeTok{Y0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{),  }\CommentTok{\# potential outcome under control}
  \AttributeTok{Y1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{)  }\CommentTok{\# potential outcome under treatment}
\NormalTok{)}

\CommentTok{\# compute individual treatment effects}
\NormalTok{po }\OtherTok{\textless{}{-}}\NormalTok{ po }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tau =}\NormalTok{ Y1 }\SpecialCharTok{{-}}\NormalTok{ Y0)}

\CommentTok{\# view the data}
\NormalTok{po}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 4
       i    Y0    Y1   tau
   <int> <dbl> <dbl> <dbl>
 1     1     4     7     3
 2     2     6     5    -1
 3     3     5     5     0
 4     4     3     9     6
 5     5     7    10     3
 6     6     5     4    -1
 7     7     4     8     4
 8     8     6     7     1
 9     9     5     3    -2
10    10     4     6     2
\end{verbatim}

\subsection{Step 2: Compute the True ATE and Variance
Components}\label{step-2-compute-the-true-ate-and-variance-components}

Now we compute the quantities we need for the variance formula.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sample size and group sizes}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \DecValTok{5}

\CommentTok{\# compute means of potential outcomes}
\NormalTok{Y1\_bar }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(po}\SpecialCharTok{$}\NormalTok{Y1)}
\NormalTok{Y0\_bar }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(po}\SpecialCharTok{$}\NormalTok{Y0)}

\CommentTok{\# compute the true ATE}
\NormalTok{ATE }\OtherTok{\textless{}{-}}\NormalTok{ Y1\_bar }\SpecialCharTok{{-}}\NormalTok{ Y0\_bar}
\NormalTok{ATE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.5
\end{verbatim}

Next, we compute \(S_t^2\), \(S_c^2\), and \(S_{tc}^2\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# variance of Y(1)}
\NormalTok{S\_t\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(po}\SpecialCharTok{$}\NormalTok{Y1)}
\NormalTok{S\_t\_sq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.933333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# variance of Y(0)}
\NormalTok{S\_c\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(po}\SpecialCharTok{$}\NormalTok{Y0)}
\NormalTok{S\_c\_sq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.433333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# variance of treatment effects}
\NormalTok{S\_tc\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(po}\SpecialCharTok{$}\NormalTok{tau)}
\NormalTok{S\_tc\_sq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.5
\end{verbatim}

\subsection{Step 3: Compute the Theoretical
Variance}\label{step-3-compute-the-theoretical-variance}

Using the variance formula, we compute the theoretical variance of
\(\widehat{\text{ATE}}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# theoretical variance using the formula}
\NormalTok{theoretical\_var }\OtherTok{\textless{}{-}}\NormalTok{ S\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ S\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c }\SpecialCharTok{{-}}\NormalTok{ S\_tc\_sq }\SpecialCharTok{/}\NormalTok{ N}
\NormalTok{theoretical\_var}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6233333
\end{verbatim}

\subsection{Step 4: Simulate Many
Randomizations}\label{step-4-simulate-many-randomizations}

Now we simulate 1,000 randomizations and compute
\(\widehat{\text{ATE}}\) for each one. We use a simple for-loop to make
the logic clear.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# number of simulations}
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# create a vector to store the estimates}
\NormalTok{ate\_estimates }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_sims)}

\CommentTok{\# run the simulation}
\ControlFlowTok{for}\NormalTok{ (sim }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims) \{}

 \CommentTok{\# randomly assign 5 individuals to treatment}
\NormalTok{  treated }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{N, }\AttributeTok{size =}\NormalTok{ N\_t, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}

  \CommentTok{\# compute observed outcomes}
\NormalTok{  Y\_obs\_t }\OtherTok{\textless{}{-}}\NormalTok{ po}\SpecialCharTok{$}\NormalTok{Y1[treated]       }\CommentTok{\# treated individuals show Y(1)}
\NormalTok{  Y\_obs\_c }\OtherTok{\textless{}{-}}\NormalTok{ po}\SpecialCharTok{$}\NormalTok{Y0[}\SpecialCharTok{{-}}\NormalTok{treated]      }\CommentTok{\# control individuals show Y(0)}

  \CommentTok{\# compute the estimate for this randomization}
\NormalTok{  ate\_estimates[sim] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(Y\_obs\_t) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Y\_obs\_c)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Step 5: Compare Theoretical and Simulated
Variance}\label{step-5-compare-theoretical-and-simulated-variance}

Finally, we compare the theoretical variance to the variance of our
simulated estimates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simulated variance}
\NormalTok{simulated\_var }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(ate\_estimates)}

\CommentTok{\# compare the two}
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{theoretical\_variance =}\NormalTok{ theoretical\_var,}
  \AttributeTok{simulated\_variance =}\NormalTok{ simulated\_var,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  theoretical_variance simulated_variance
                 <dbl>              <dbl>
1                0.623              0.552
\end{verbatim}

The simulated variance is very close to the theoretical variance. The
small difference is due to simulation error. With more simulations, the
two values would converge.

\marginnote{\begin{footnotesize}

We can also visualize the distribution of estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{ate =}\NormalTok{ ate\_estimates), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{fill =} \StringTok{"grey80"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ ATE) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{ch/var_files/figure-pdf/unnamed-chunk-9-1.pdf}}

}

\caption{Distribution of \(\widehat{\text{ATE}}\) across 1,000
randomizations. The vertical dashed line shows the true ATE. The
histogram shows that the estimates are centered around the true ATE
(confirming unbiasedness) and spread out according to the variance we
computed. Some randomizations produce estimates close to the true ATE,
while others are further away. But on average, we get it right.}

\end{figure}%

\section{Key Terms}\label{key-terms-2}

\begin{itemize}
\tightlist
\item
  Variance operator (\(\text{Var}[\cdot]\))
\item
  Variance of an estimator
\item
  Finite population variance
\item
  Variance of potential outcomes (\(S_t^2\), \(S_c^2\))
\item
  Variance of treatment effects (\(S_{tc}^2\))
\item
  Heterogeneous treatment effects
\end{itemize}

\section{Exercises}\label{exercises-2}

\subsection{Conceptual Understanding}\label{conceptual-understanding-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Variance vs.~Bias.} In your own words, explain the difference
  between an estimator being unbiased and an estimator having low
  variance. Can an estimator be unbiased but still give poor estimates
  in practice?
\end{enumerate}

Solution

\textbf{Unbiasedness} means that the estimator is correct \emph{on
average}. If we could repeat the experiment many times with different
randomizations, the average of all our estimates would equal the true
ATE. But any single estimate might be far from the truth.

\textbf{Low variance} means that the estimates are tightly clustered
around their average value. Different randomizations produce similar
estimates.

Yes, an estimator can be unbiased but still give poor estimates in
practice. Imagine an estimator that, across many randomizations,
produces estimates of \(-100\) half the time and \(+102\) half the time
(when the true ATE is 1). This estimator is unbiased (the average is 1),
but any single estimate is wildly wrong.

In practice, we want both properties: an estimator that is unbiased
\emph{and} has low variance. The ideal estimator hits close to the
target on average (unbiased) and doesn't scatter too much around that
average (low variance).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{The Three Terms.} The variance formula has three terms:
  \(S_t^2/N_t\), \(S_c^2/N_c\), and \(-S_{tc}^2/N\). Explain in plain
  language what each term represents and why the third term is negative.
\end{enumerate}

Solution

\textbf{First term (\(S_t^2/N_t\))}: This represents the uncertainty in
estimating the average treatment group outcome. \(S_t^2\) measures how
much the potential outcomes under treatment vary across individuals.
When there's more variation, the particular individuals who happen to
land in the treatment group could have very different outcomes from
another randomization. Dividing by \(N_t\) reflects that larger
treatment groups give more stable estimates.

\textbf{Second term (\(S_c^2/N_c\))}: Similarly, this represents the
uncertainty in estimating the average control group outcome. It depends
on how variable the control potential outcomes are and how many
individuals are in the control group.

\textbf{Third term (\(-S_{tc}^2/N\))}: This term \emph{reduces} the
variance, which seems surprising at first. Here \(S_{tc}^2\) is the
variance of the individual-level treatment effects
\(\tau_i = Y_i(1) - Y_i(0)\). It measures how much treatment effects
vary across individuals. Formally,

\[
S_{tc}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (\tau_i - \text{ATE})^2.
\]

The third term captures the fact that treatment and control group
assignments are not independent. They are negatively correlated. If one
individual goes to treatment, another must go to control.

The negative sign arises because treatment and control assignments are
linked. When a high-treatment-effect individual is assigned to
treatment, they are necessarily \emph{missing} from control. This
creates a stabilizing dynamic: the treatment mean goes up (because they
have high \(Y(1)\)), but so does the control mean (because their low
\(Y(0)\) is absent). The difference between the means is more stable
than either mean alone. This stabilizing effect is stronger when
treatment effects are more heterogeneous (larger \(S_{tc}^2\)). When
treatment effects are constant (\(S_{tc}^2 = 0\)), this stabilizing
effect disappears entirely.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Constant Treatment Effects.} Suppose the treatment effect is
  the same for every individual (\(\tau_i = \tau\) for all \(i\)).
  Explain what happens to \(S_{tc}^2\). What does the variance formula
  simplify to?
\end{enumerate}

Solution

If the treatment effect is constant, meaning \(\tau_i = \tau\) for all
individuals, then every individual's treatment effect equals the ATE.
The deviations \(\tau_i - \text{ATE}\) are all zero, so

\[
S_{tc}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (\tau_i - \text{ATE})^2 = \frac{1}{N-1} \sum_{i=1}^{N} 0 = 0.
\]

The variance formula simplifies to

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c}.
\]

Moreover, when treatment effects are constant, the variance of potential
outcomes under treatment equals the variance under control:
\(S_t^2 = S_c^2 = S^2\). This is because \(Y_i(1) = Y_i(0) + \tau\), so
both sets of potential outcomes are just shifted versions of each other,
with the same spread.

In this case, the formula further simplifies to

\[
\text{Var}_D(\widehat{\text{ATE}}) = S^2 \left(\frac{1}{N_t} + \frac{1}{N_c}\right),
\]

which is the familiar formula for the variance of a difference of two
sample means.

\subsection{Computational Practice}\label{computational-practice-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Computing \(S_t^2\), \(S_c^2\), and \(S_{tc}^2\).} Consider
  the following potential outcomes for 4 individuals.

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Individual & \(Y_i(0)\) & \(Y_i(1)\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 2 & 5 \\
  2 & 4 & 6 \\
  3 & 3 & 4 \\
  4 & 5 & 7 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute \(\overline{Y}(0)\) and \(\overline{Y}(1)\).
  \item
    Compute \(S_c^2\), the variance of \(Y_i(0)\).
  \item
    Compute \(S_t^2\), the variance of \(Y_i(1)\).
  \item
    Compute the individual treatment effects \(\tau_i\) and the ATE.
  \item
    Compute \(S_{tc}^2\), the variance of the treatment effects.
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The means of the potential outcomes are

\[
\begin{align}
\overline{Y}(0) &= \frac{1}{4}(2 + 4 + 3 + 5) = \frac{14}{4} = 3.5 \\
\overline{Y}(1) &= \frac{1}{4}(5 + 6 + 4 + 7) = \frac{22}{4} = 5.5
\end{align}
\]

\textbf{Part b.} To compute \(S_c^2\), we first find the squared
deviations from the mean \(\overline{Y}(0) = 3.5\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3375}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3875}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Individual
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(0) - \overline{Y}(0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((Y_i(0) - \overline{Y}(0))^2\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & \(-1.5\) & \(2.25\) \\
2 & 4 & \(0.5\) & \(0.25\) \\
3 & 3 & \(-0.5\) & \(0.25\) \\
4 & 5 & \(1.5\) & \(2.25\) \\
\end{longtable}

\[
S_c^2 = \frac{1}{4-1}(2.25 + 0.25 + 0.25 + 2.25) = \frac{5}{3} \approx 1.67
\]

\textbf{Part c.} Similarly, for \(S_t^2\) using
\(\overline{Y}(1) = 5.5\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3375}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3875}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Individual
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(1)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Y_i(1) - \overline{Y}(1)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((Y_i(1) - \overline{Y}(1))^2\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 5 & \(-0.5\) & \(0.25\) \\
2 & 6 & \(0.5\) & \(0.25\) \\
3 & 4 & \(-1.5\) & \(2.25\) \\
4 & 7 & \(1.5\) & \(2.25\) \\
\end{longtable}

\[
S_t^2 = \frac{1}{3}(0.25 + 0.25 + 2.25 + 2.25) = \frac{5}{3} \approx 1.67
\]

\textbf{Part d.} The individual treatment effects are shown in the
following table.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Individual & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i = Y_i(1) - Y_i(0)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 5 & 3 \\
2 & 4 & 6 & 2 \\
3 & 3 & 4 & 1 \\
4 & 5 & 7 & 2 \\
\end{longtable}

\[
\text{ATE} = \frac{1}{4}(3 + 2 + 1 + 2) = \frac{8}{4} = 2
\]

\textbf{Part e.} For \(S_{tc}^2\), we compute deviations from
\(\text{ATE} = 2\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1389}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Individual
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tau_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tau_i - \text{ATE}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((\tau_i - \text{ATE})^2\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 3 & 1 & 1 \\
2 & 2 & 0 & 0 \\
3 & 1 & \(-1\) & 1 \\
4 & 2 & 0 & 0 \\
\end{longtable}

\[
S_{tc}^2 = \frac{1}{3}(1 + 0 + 1 + 0) = \frac{2}{3} \approx 0.67
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Computing the Variance.} Using the values you computed in
  Exercise 4, suppose we conduct an experiment with \(N_t = 2\) and
  \(N_c = 2\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute the variance of \(\widehat{\text{ATE}}\) using the formula.
  \item
    In Exercise 7 of the previous chapter, you computed
    \(\widehat{\text{ATE}}\) for all 6 possible randomizations. Verify
    that the variance of those 6 estimates matches your answer to part
    (a).
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} Using the variance formula with \(N = 4\), \(N_t = 2\),
\(N_c = 2\), we have

\[
\begin{align}
\text{Var}_D(\widehat{\text{ATE}}) &= \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N} \\
&= \frac{5/3}{2} + \frac{5/3}{2} - \frac{2/3}{4} \\
&= \frac{5}{6} + \frac{5}{6} - \frac{2}{12} \\
&= \frac{10}{6} - \frac{1}{6} \\
&= \frac{9}{6} = 1.5
\end{align}
\]

\textbf{Part b.} From Exercise 7 of the previous chapter, the 6 possible
estimates were as follows.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Randomization & \(\widehat{\text{ATE}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1, 2 treated & 1.5 \\
1, 3 treated & 0 \\
1, 4 treated & 2.5 \\
2, 3 treated & 1.5 \\
2, 4 treated & 4 \\
3, 4 treated & 2.5 \\
\end{longtable}

The mean of these estimates is
\(\frac{1.5 + 0 + 2.5 + 1.5 + 4 + 2.5}{6} = \frac{12}{6} = 2\), which
equals the true ATE (confirming unbiasedness).

The variance is

\[
\begin{align}
\text{Var} &= \frac{1}{6}\left[(1.5-2)^2 + (0-2)^2 + (2.5-2)^2 + (1.5-2)^2 + (4-2)^2 + (2.5-2)^2\right] \\
&= \frac{1}{6}\left[0.25 + 4 + 0.25 + 0.25 + 4 + 0.25\right] \\
&= \frac{9}{6} = 1.5
\end{align}
\]

This matches our answer from part (a).

\subsection{Application to Rasinski's
Experiment}\label{application-to-rasinskis-experiment-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Variance in the Rasinski Context.} Return to the hypothetical
  Rasinski data from the previous chapters.

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Respondent & \(Y_i(0)\) & \(Y_i(1)\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 3 & 1 \\
  2 & 3 & 2 \\
  3 & 2 & 1 \\
  4 & 3 & 1 \\
  5 & 2 & 1 \\
  6 & 3 & 2 \\
  7 & 2 & 1 \\
  8 & 3 & 1 \\
  9 & 2 & 2 \\
  10 & 3 & 1 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Use R to compute \(S_c^2\), \(S_t^2\), and \(S_{tc}^2\) for this
    population.
  \item
    Suppose we run an experiment with \(N_t = 5\) and \(N_c = 5\). Use R
    to compute the variance of \(\widehat{\text{ATE}}\).
  \item
    How would the variance change if we instead used \(N_t = 3\) and
    \(N_c = 7\)?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} First, we create the data and compute the variance
components using R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create the potential outcomes}
\NormalTok{Y0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Y1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{tau }\OtherTok{\textless{}{-}}\NormalTok{ Y1 }\SpecialCharTok{{-}}\NormalTok{ Y0}

\CommentTok{\# compute the variances}
\NormalTok{S\_c\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(Y0)}
\NormalTok{S\_t\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(Y1)}
\NormalTok{S\_tc\_sq }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(tau)}

\CommentTok{\# display the results}
\NormalTok{S\_c\_sq   }\CommentTok{\# approximately 0.267}
\NormalTok{S\_t\_sq   }\CommentTok{\# approximately 0.233}
\NormalTok{S\_tc\_sq  }\CommentTok{\# approximately 0.446}
\end{Highlighting}
\end{Shaded}

\textbf{Part b.} With \(N = 10\), \(N_t = 5\), \(N_c = 5\), we compute
the variance using the formula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \DecValTok{5}

\CommentTok{\# compute variance using the formula}
\NormalTok{variance\_balanced }\OtherTok{\textless{}{-}}\NormalTok{ S\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ S\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c }\SpecialCharTok{{-}}\NormalTok{ S\_tc\_sq }\SpecialCharTok{/}\NormalTok{ N}
\NormalTok{variance\_balanced  }\CommentTok{\# approximately 0.0554}
\end{Highlighting}
\end{Shaded}

\textbf{Part c.} With \(N_t = 3\) and \(N_c = 7\), we have

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \DecValTok{7}

\CommentTok{\# compute variance with unbalanced design}
\NormalTok{variance\_unbalanced }\OtherTok{\textless{}{-}}\NormalTok{ S\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ S\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c }\SpecialCharTok{{-}}\NormalTok{ S\_tc\_sq }\SpecialCharTok{/}\NormalTok{ N}
\NormalTok{variance\_unbalanced  }\CommentTok{\# approximately 0.0712}
\end{Highlighting}
\end{Shaded}

The variance is higher (0.0712 vs.~0.0554) with the unbalanced design.
Balanced designs (\(N_t = N_c\)) generally minimize variance for a given
total sample size.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  \textbf{Interpreting the Variance.} Based on your calculations in
  Exercise 6, answer the following.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    The standard deviation is the square root of the variance. The
    variance with the balanced design (\(N_t = N_c = 5\)) is about
    0.055. What is the standard deviation of \(\widehat{\text{ATE}}\)?
  \item
    If the true ATE is \(-1.3\), and the standard deviation is about
    0.24, roughly what range of estimates would you expect to see across
    different randomizations? (Hint: remember the normal table.)
  \item
    How does this compare to the estimates we computed in the previous
    chapter (one randomization gave \(-1.8\), another gave \(-0.8\))?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The standard deviation is the square root of the
variance.

\[
\text{SD}(\widehat{\text{ATE}}) = \sqrt{0.0554} \approx 0.235
\]

\textbf{Part b.} A common rule of thumb is that most estimates (roughly
95\%) fall within about 2 standard deviations of the mean. With the true
ATE of \(-1.3\) and standard deviation of about 0.24, we can compute the
following.

\begin{itemize}
\tightlist
\item
  Most estimates would fall between \(-1.3 - 2(0.24) = -1.78\) and
  \(-1.3 + 2(0.24) = -0.82\)
\item
  The typical estimate would be within about 0.24 units of the true
  value
\end{itemize}

\textbf{Part c.} In the previous chapter, we computed the following
estimates.

\begin{itemize}
\tightlist
\item
  One randomization: \(\widehat{\text{ATE}} = -1.8\) (deviation of
  \(-0.5\) from truth)
\item
  Another randomization: \(\widehat{\text{ATE}} = -0.8\) (deviation of
  \(+0.5\) from truth)
\end{itemize}

Both of these estimates fall within 2 standard deviations of the true
ATE, which is exactly what we'd expect. The estimate of \(-1.8\) is
about 2 standard deviations below the true value, and \(-0.8\) is about
2 standard deviations above. These are at the edges of the typical range
but not implausible.

This illustrates how the variance formula helps us understand and
calibrate our expectations for how far estimates might be from the
truth.

\subsection{Critical Thinking}\label{critical-thinking-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{The Negative Term.} A colleague claims, ``More heterogeneous
  treatment effects lead to higher variance in our estimates.'' Based on
  the variance formula, explain why this claim needs to be stated more
  carefully.
\end{enumerate}

Solution

The colleague's claim is not quite right. The variance formula is given
by

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}.
\]

More heterogeneous treatment effects means larger \(S_{tc}^2\). But
\(S_{tc}^2\) enters with a \emph{negative} sign! So increasing
\(S_{tc}^2\) actually \emph{decreases} the variance, all else equal.

However, ``all else equal'' is the key caveat. Treatment effect
heterogeneity is linked to the variances of potential outcomes.
Specifically, we have

\[
S_{tc}^2 = S_t^2 + S_c^2 - 2\text{Cov}(Y_i(1), Y_i(0))
\]

If treatment effects become more heterogeneous because the potential
outcomes become more variable (larger \(S_t^2\) and \(S_c^2\)), the
first two terms of the variance formula increase, which could outweigh
the reduction from the third term.

A more careful statement would be the following. ``Holding the variances
of potential outcomes fixed, more heterogeneous treatment effects lead
to \emph{lower} variance in our estimates, because of the negative term
in the variance formula. But if treatment effect heterogeneity arises
because potential outcomes themselves are more variable, the net effect
on variance is ambiguous.''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  \textbf{Design Choices.} Suppose you are designing an experiment with
  a fixed budget that allows you to include \(N = 100\) individuals. You
  can choose how many to assign to treatment (\(N_t\)) and how many to
  control (\(N_c = 100 - N_t\)).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    If you knew that \(S_t^2 = S_c^2\), what allocation would minimize
    variance?
  \item
    If you knew that \(S_t^2 = 4\) and \(S_c^2 = 1\), would you still
    use the same allocation? Why or why not?
  \item
    In practice, you don't know \(S_t^2\) and \(S_c^2\) before running
    the experiment. What allocation would you recommend, and why?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} If \(S_t^2 = S_c^2 = S^2\), the variance (ignoring the
\(S_{tc}^2\) term for simplicity) is

\[
\text{Var} \approx S^2 \left(\frac{1}{N_t} + \frac{1}{N_c}\right) = S^2 \left(\frac{1}{N_t} + \frac{1}{100 - N_t}\right)
\]

Taking the derivative with respect to \(N_t\) and setting it to zero:

\[
\frac{d}{dN_t}\left(\frac{1}{N_t} + \frac{1}{100 - N_t}\right) = -\frac{1}{N_t^2} + \frac{1}{(100-N_t)^2} = 0
\]

This gives \(N_t = 100 - N_t\), so \(N_t = 50\). A balanced design
minimizes variance when variances are equal.

\textbf{Part b.} If \(S_t^2 = 4\) and \(S_c^2 = 1\), we would want to
allocate more individuals to the group with higher variance (treatment).
The optimal allocation satisfies:

\[
\frac{N_t}{N_c} = \sqrt{\frac{S_t^2}{S_c^2}} = \sqrt{\frac{4}{1}} = 2
\]

So we'd want \(N_t = 2 N_c\), which with \(N = 100\) gives
\(N_t \approx 67\) and \(N_c \approx 33\).

The intuition is that we need more observations from the noisier group
to get the same precision in estimating that group's mean.

\textbf{Part c.} In practice, I would recommend a balanced design
(\(N_t = N_c = 50\)) for several reasons.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robustness.} Without knowing the true variances, a balanced
  design is a safe choice that performs well across many scenarios.
\item
  \textbf{Near-optimal.} Even when variances differ, the balanced design
  is often close to optimal unless the variance ratio is extreme.
\item
  \textbf{Simplicity.} A balanced design is easier to implement and
  explain.
\end{enumerate}

If there's strong prior information suggesting very different variances,
one could consider an unbalanced design, but for most applications a
50-50 split is a sensible default.

\bookmarksetup{startatroot}

\chapter{Estimating the Variance}\label{estimating-the-variance}

In the previous chapter, we derived the variance of
\(\widehat{\text{ATE}}\) under completely randomized experiments. We
showed that

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N},
\]

where \(S_t^2\) and \(S_c^2\) are the variances of the potential
outcomes and \(S_{tc}^2\) is the variance of the individual treatment
effects.

But there's a problem. This formula depends on quantities we cannot
observe.

\begin{itemize}
\tightlist
\item
  We defined \(S_t^2\) as the variance of \emph{all} individuals'
  potential outcomes under treatment, but we only observe \(Y_i(1)\) for
  the treated individuals.
\item
  Similarly, we only observe \(Y_i(0)\) for the control individuals.
\item
  And \(S_{tc}^2\) depends on the individual treatment effects
  \(\tau_i = Y_i(1) - Y_i(0)\), which we can never observe for any
  individual.
\end{itemize}

To make inferences about the ATE, we need to \emph{estimate} the
variance using only the observed data. This chapter addresses that
challenge.

\section{The Problem: We Can't Observe What We
Need}\label{the-problem-we-cant-observe-what-we-need}

Let's be explicit about what we can and cannot observe. Recall our
running example with 10 respondents. Here are the full potential
outcomes (which only we, as omniscient author and reader, can see).

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 7 & 3 \\
2 & 6 & 5 & -1 \\
3 & 5 & 5 & 0 \\
4 & 3 & 9 & 6 \\
5 & 7 & 10 & 3 \\
6 & 5 & 4 & -1 \\
7 & 4 & 8 & 4 \\
8 & 6 & 7 & 1 \\
9 & 5 & 3 & -2 \\
10 & 4 & 6 & 2 \\
\end{longtable}

From this complete table, we computed in the previous chapter that
\(S_t^2 = 4.49\), \(S_c^2 = 1.33\), and \(S_{tc}^2 = 5.17\). With
\(N_t = N_c = 5\), the true variance of the estimator is

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{4.49}{5} + \frac{1.33}{5} - \frac{5.17}{10} = 0.898 + 0.266 - 0.517 = 0.647.
\]

But in practice, after running the experiment, we only see the observed
outcomes. Suppose respondents 1, 4, 5, 7, and 10 are assigned to
treatment. Here's what we actually observe.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Respondent & \(Y_i(0)\) & \(Y_i(1)\) & \(D_i\) & \(Y_i^{\text{obs}}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & ? & 7 & 1 & 7 \\
2 & 6 & ? & 0 & 6 \\
3 & 5 & ? & 0 & 5 \\
4 & ? & 9 & 1 & 9 \\
5 & ? & 10 & 1 & 10 \\
6 & 5 & ? & 0 & 5 \\
7 & ? & 8 & 1 & 8 \\
8 & 6 & ? & 0 & 6 \\
9 & 5 & ? & 0 & 5 \\
10 & ? & 6 & 1 & 6 \\
\end{longtable}

The question marks represent counterfactual outcomes we cannot observe.
We have no way to compute \(S_t^2\), \(S_c^2\), or \(S_{tc}^2\) from
this data because we don't observe all the potential outcomes.

\section{\texorpdfstring{Estimating \(S_t^2\) and
\(S_c^2\)}{Estimating S\_t\^{}2 and S\_c\^{}2}}\label{estimating-s_t2-and-s_c2}

Even though we cannot compute \(S_t^2\) exactly, we can \emph{estimate}
it using the observed outcomes in the treatment group. Similarly, we can
estimate \(S_c^2\) using the observed outcomes in the control group.

\subsection{The Sample Variance in the Treatment
Group}\label{the-sample-variance-in-the-treatment-group}

We define the \textbf{sample variance in the treatment group} as

\[
s_t^2 = \frac{1}{N_t - 1} \sum_{i:D_i=1} \left( Y^{\text{obs}}_i - \overline{Y}^{\text{obs}}_t \right)^2.
\]

This is just the ordinary sample variance formula applied to the
observed outcomes of treated individuals.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Formula}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The formula computes the variance of the observed outcomes among treated
individuals. The subscript \(i:D_i=1\) means ``only sum over individuals
with \(D_i = 1\)'' (i.e., the treated individuals).

We divide by \(N_t - 1\) rather than \(N_t\) to get an unbiased estimate
of the variance. This is the standard ``degrees of freedom'' correction
you may have seen in introductory statistics.

\textbf{Example.} In our experiment, the treated individuals (1, 4, 5,
7, 10) have observed outcomes 7, 9, 10, 8, 6. The treatment group mean
is

\[
\overline{Y}^{\text{obs}}_t = \frac{7 + 9 + 10 + 8 + 6}{5} = \frac{40}{5} = 8.
\]

The sample variance is

\[
s_t^2 = \frac{1}{5-1}\left[(7-8)^2 + (9-8)^2 + (10-8)^2 + (8-8)^2 + (6-8)^2\right] = \frac{1 + 1 + 4 + 0 + 4}{4} = \frac{10}{4} = 2.5.
\]

\subsection{The Sample Variance in the Control
Group}\label{the-sample-variance-in-the-control-group}

Similarly, we define the \textbf{sample variance in the control group}
as

\[
s_c^2 = \frac{1}{N_c - 1} \sum_{i:D_i=0} \left( Y^{\text{obs}}_i - \overline{Y}^{\text{obs}}_c \right)^2.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Formula}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

This is the same idea, applied to the control group. We compute the
variance of the observed outcomes among control individuals.

\textbf{Example.} The control individuals (2, 3, 6, 8, 9) have observed
outcomes 6, 5, 5, 6, 5. The control group mean is

\[
\overline{Y}^{\text{obs}}_c = \frac{6 + 5 + 5 + 6 + 5}{5} = \frac{27}{5} = 5.4.
\]

The sample variance is

\[
s_c^2 = \frac{1}{4}\left[(6-5.4)^2 + (5-5.4)^2 + (5-5.4)^2 + (6-5.4)^2 + (5-5.4)^2\right] = \frac{0.36 + 0.16 + 0.16 + 0.36 + 0.16}{4} = \frac{1.2}{4} = 0.3.
\]

\subsection{Are These Good Estimates?}\label{are-these-good-estimates}

A natural question is whether \(s_t^2\) and \(s_c^2\) are good estimates
of \(S_t^2\) and \(S_c^2\). They are! Under random assignment, these
sample variances are unbiased estimators of the population variances.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Unbiasedness of Sample Variances}, breakable, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Under completely randomized assignment, we have

\[
E_D[s_t^2] = S_t^2 \quad \text{and} \quad E_D[s_c^2] = S_c^2.
\]

That is, on average across all possible randomizations, the sample
variances equal the population variances.

The intuition is straightforward. Random assignment ensures that the
treated individuals are a representative sample of the full population.
So the variance of the observed outcomes among treated individuals
should, on average, equal the variance of potential outcomes under
treatment for the full population. The same logic applies to the control
group.

\section{\texorpdfstring{The Problem with
\(S_{tc}^2\)}{The Problem with S\_\{tc\}\^{}2}}\label{the-problem-with-s_tc2}

We can estimate \(S_t^2\) and \(S_c^2\) from observed data. But what
about \(S_{tc}^2\), the variance of the individual treatment effects?

Here's a \textbf{big problem}: \(S_{tc}^2\) is defined as

\[
S_{tc}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (\tau_i - \text{ATE})^2,
\]

where \(\tau_i = Y_i(1) - Y_i(0)\). This requires knowing \emph{both}
potential outcomes for every individual. But we only observe one
potential outcome per individual. We can never compute \(\tau_i\) for
any individual, let alone its variance across individuals.

This is a fundamental limitation. The quantity \(S_{tc}^2\) is
inherently unobservable. \textbf{We cannot even \emph{estimate} it from
the observed data.}

\section{Neyman's Variance Estimator}\label{neymans-variance-estimator}

In 1923, Jerzy Neyman proposed a solution to this problem (Neyman 1923).
Rather than trying to estimate \(S_{tc}^2\), we simply \textbf{drop the
third term} from the variance formula. This gives what we call
\textbf{Neyman's variance estimator}.

\[
\widehat{\text{Var}}(\widehat{\text{ATE}}) = \frac{s_t^2}{N_t} + \frac{s_c^2}{N_c}.
\]

This estimator uses only observable quantities: the sample variances in
the treatment and control groups.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Estimator}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Neyman's variance estimator replaces the population variances \(S_t^2\)
and \(S_c^2\) with their sample counterparts \(s_t^2\) and \(s_c^2\),
and ignores the \(-S_{tc}^2/N\) term entirely.

\textbf{Example.} Using our earlier calculations where \(s_t^2 = 2.5\)
and \(s_c^2 = 0.3\) with \(N_t = N_c = 5\), Neyman's variance estimator
is

\[
\widehat{\text{Var}}(\widehat{\text{ATE}}) = \frac{2.5}{5} + \frac{0.3}{5} = 0.5 + 0.06 = 0.56.
\]

Recall that the true variance (computed from the full potential
outcomes) was 0.647. Our estimate of 0.56 is reasonably close.

But why is it safe to drop this third term? Let's consider three
scenarios.

\subsection{Case 1: The Sharp Null
Hypothesis}\label{case-1-the-sharp-null-hypothesis}

Under the \textbf{sharp null hypothesis}, the treatment has no effect on
any individual. That is, \(Y_i(1) = Y_i(0)\) for all \(i\), which means
\(\tau_i = 0\) for all \(i\).

If all treatment effects are zero, then the variance of the treatment
effects is also zero. We have \(S_{tc}^2 = 0\). What happens to the true
variance formula? The third term vanishes, and the formula simplifies to

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c}.
\]

But this is exactly what Neyman's estimator estimates! Under the sharp
null, Neyman's estimator is unbiased.

\subsection{Case 2: Constant Treatment
Effects}\label{case-2-constant-treatment-effects}

Under \textbf{constant treatment effects}, every individual experiences
the same treatment effect. That is, \(\tau_i = \tau\) for some constant
\(\tau\) and all individuals \(i\).

If treatment effects are constant, then there is no variance in the
treatment effects. Every \(\tau_i\) equals \(\tau\), which equals the
ATE. So \(S_{tc}^2 = 0\). Once again, the third term vanishes, and the
true variance formula simplifies to

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c}.
\]

Neyman's estimator is unbiased in this case, too.

\subsection{Case 3: Non-Constant Treatment
Effects}\label{case-3-non-constant-treatment-effects}

What if treatment effects are neither zero nor constant? What if they
vary across individuals, as they almost certainly do in practice?

This is where things get interesting. Recall the true variance formula

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}.
\]

When treatment effects vary, \(S_{tc}^2 > 0\). Critically, the third
term is \emph{always negative}. It \emph{shrinks} the variance. So what
happens when Neyman's estimator drops this term?

The estimator becomes too large. It overestimates the true variance.

More formally, since \(S_{tc}^2 \geq 0\), we have

\[
\frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} \geq \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N} = \text{Var}_D(\widehat{\text{ATE}}).
\]

And since \(E_D[s_t^2] = S_t^2\) and \(E_D[s_c^2] = S_c^2\), the
expected value of Neyman's estimator is

\[
E_D\left[\widehat{\text{Var}}(\widehat{\text{ATE}})\right] = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} \geq \text{Var}_D(\widehat{\text{ATE}}).
\]

Here is the surprising result. We might have worried that dropping an
unobservable term would cause serious problems. Instead, it makes our
variance estimate \emph{conservative}. We err on the side of
overestimating uncertainty rather than underestimating it.

In practice, having a conservative variance estimator is often
acceptable. If we use an overestimated variance to construct confidence
intervals or conduct hypothesis tests, our inferences will be
conservative.

\begin{itemize}
\tightlist
\item
  Confidence intervals will be wider than necessary, so they will cover
  the true ATE \emph{at least as often as advertised} (95\% \emph{or
  more}).
\item
  Hypothesis tests will reject the null hypothesis less often than they
  ``should,'' so the actual Type I error rate will be \emph{at most} the
  nominal level (5\% (or less*)).
\end{itemize}

\section{Computing the Variance Estimate in
R}\label{computing-the-variance-estimate-in-r}

Let's compute Neyman's variance estimator using R. We'll use the same
hypothetical experiment from above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# create the observed data}
\NormalTok{obs\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{respondent =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
  \AttributeTok{D =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),  }\CommentTok{\# treatment assignment}
  \AttributeTok{Y\_obs =} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)  }\CommentTok{\# observed outcomes}
\NormalTok{)}

\NormalTok{obs\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 3
   respondent     D Y_obs
        <int> <dbl> <dbl>
 1          1     1     7
 2          2     0     6
 3          3     0     5
 4          4     1     9
 5          5     1    10
 6          6     0     5
 7          7     1     8
 8          8     0     6
 9          9     0     5
10         10     1     6
\end{verbatim}

\subsection{Computing the Sample
Variances}\label{computing-the-sample-variances}

We compute the sample variances in each group.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sample variance in treatment group}
\NormalTok{s\_t\_sq }\OtherTok{\textless{}{-}}\NormalTok{ obs\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{var}\NormalTok{()}

\NormalTok{s\_t\_sq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sample variance in control group}
\NormalTok{s\_c\_sq }\OtherTok{\textless{}{-}}\NormalTok{ obs\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{var}\NormalTok{()}

\NormalTok{s\_c\_sq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3
\end{verbatim}

\subsection{Computing Neyman's Variance
Estimator}\label{computing-neymans-variance-estimator}

Now we can compute Neyman's variance estimator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# group sizes}
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(obs\_data}\SpecialCharTok{$}\NormalTok{D)}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ obs\_data}\SpecialCharTok{$}\NormalTok{D)}

\CommentTok{\# Neyman\textquotesingle{}s variance estimator}
\NormalTok{var\_hat }\OtherTok{\textless{}{-}}\NormalTok{ s\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ s\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c}
\NormalTok{var\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.56
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# standard error is a common conversion (square root of variance)}
\NormalTok{se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(var\_hat)}
\NormalTok{se\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7483315
\end{verbatim}

The estimated variance is 0.56 and the estimated standard error is
0.748.

\subsection{Using OLS with HC2 Standard
Errors}\label{using-ols-with-hc2-standard-errors}

We can also compute Neyman's variance estimator in R using
\texttt{lm()}. As we showed in an earlier chapter, the
difference-in-means estimator equals the coefficient from a regression
of \(Y_i^{\text{obs}}\) on \(D_i\). But the default standard errors from
\texttt{lm()} (as reported by \texttt{summary()} for example) assume
homoscedasticity, which is not appropriate here.

Samii and Aronow (2012) show that \textbf{HC2 robust standard errors}
are exactly equivalent to Neyman's variance estimator for completely
randomized experiments. The HC2 estimator is one of several
``heteroscedasticity-consistent'' variance estimators developed in
econometrics. Samii and Aronow's result means we can obtain the Neyman
standard error directly from regression output by requesting HC2
standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the sandwich package for robust standard errors}
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}

\CommentTok{\# fit the regression}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_obs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ D, }\AttributeTok{data =}\NormalTok{ obs\_data)}

\CommentTok{\# print estimates and HC2 SEs}
\NormalTok{var\_hat\_hc2 }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"HC2"}\NormalTok{)}
\FunctionTok{coeftest}\NormalTok{(fit, }\AttributeTok{vcov. =}\NormalTok{ var\_hat\_hc2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

t test of coefficients:

            Estimate Std. Error t value  Pr(>|t|)    
(Intercept)  5.40000    0.24495 22.0454 1.893e-08 ***
D            2.60000    0.74833  3.4744  0.008388 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract variance estimates using HC2}
\FunctionTok{diag}\NormalTok{(var\_hat\_hc2)[}\DecValTok{2}\NormalTok{] }\CommentTok{\# variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   D 
0.56 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(var\_hat\_hc2)[}\DecValTok{2}\NormalTok{]) }\CommentTok{\# SE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        D 
0.7483315 
\end{verbatim}

The coefficient on \texttt{D} is 2.6, which is our
\(\widehat{\text{ATE}}\). The HC2 standard error is 0.748, which matches
the Neyman standard error we computed by hand (0.748).

\section{Key Terms}\label{key-terms-3}

\begin{itemize}
\tightlist
\item
  Sample variance (\(s_t^2\), \(s_c^2\))
\item
  Neyman's variance estimator
\item
  Conservative variance estimation
\item
  Standard error
\end{itemize}

\section{Exercises}\label{exercises-3}

\subsection{Conceptual Understanding}\label{conceptual-understanding-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Observable vs.~Unobservable.} Explain why we can estimate
  \(S_t^2\) and \(S_c^2\) from observed data, but we cannot estimate
  \(S_{tc}^2\). What fundamental feature of experiments makes
  \(S_{tc}^2\) unobservable?
\end{enumerate}

Solution

We can estimate \(S_t^2\) because random assignment ensures that the
treated individuals are a representative sample of the full population.
The observed outcomes \(Y_i^{\text{obs}}\) for treated individuals are a
random sample of all the potential outcomes \(Y_i(1)\). So the sample
variance of observed outcomes in the treatment group is an unbiased
estimate of the population variance of potential outcomes under
treatment. The same logic applies to \(S_c^2\) and the control group.

We cannot estimate \(S_{tc}^2\) because it measures the variance of
\(\tau_i = Y_i(1) - Y_i(0)\), the individual-level treatment effect. To
compute \(\tau_i\) for any individual, we would need to observe
\emph{both} \(Y_i(1)\) and \(Y_i(0)\). But the fundamental problem of
causal inference is that we can only observe one potential outcome per
individual. Since we can never observe \(\tau_i\) for any individual, we
cannot compute its variance across individuals.

This is a fundamental limitation of experiments. We can learn about
average effects, but the individual-level effects remain hidden.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Why Conservative?} Explain in plain language why Neyman's
  variance estimator is conservative (i.e., why it tends to overestimate
  the true variance). Under what conditions is it exactly unbiased?
\end{enumerate}

Solution

The true variance of \(\widehat{\text{ATE}}\) is

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}.
\]

Neyman's variance estimator only uses the first two terms. It ignores
the third term, \(-S_{tc}^2/N\), which is negative (or zero). By
ignoring a negative term, we end up with a larger estimate than the true
value.

Intuitively, the negative term reflects the fact that treatment and
control assignments are negatively correlated. When an individual with a
large \(Y_i(1)\) ends up in treatment, they're not in control, which
partially offsets the effect on the estimate. Neyman's estimator ignores
this offsetting effect, so it overestimates how much the estimate
varies.

Neyman's estimator is exactly unbiased when \(S_{tc}^2 = 0\). This
happens in two cases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sharp null.} If the treatment has no effect on anyone
  (\(\tau_i = 0\) for all \(i\)), then all treatment effects are zero,
  and their variance is zero.
\item
  \textbf{Constant effects.} If everyone experiences the same treatment
  effect (\(\tau_i = \tau\) for all \(i\)), then there's no variation in
  treatment effects, and their variance is zero.
\end{enumerate}

In any other case, where treatment effects vary across individuals,
Neyman's estimator overestimates the true variance.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Conservative Inference.} If you use a conservative variance
  estimator to construct a 95\% confidence interval, will the actual
  coverage probability be greater than, less than, or equal to 95\%?
  Explain.
\end{enumerate}

Solution

The actual coverage probability will be \textbf{greater than} 95\%.

A conservative variance estimator overestimates the variance, which
means it also overestimates the standard error (the square root of the
variance). When we construct a confidence interval, we compute

\[
\widehat{\text{ATE}} \pm 1.96 \times \widehat{\text{SE}}.
\]

If \(\widehat{\text{SE}}\) is too large, the confidence interval will be
wider than necessary. A wider interval is more likely to contain the
true ATE, so the actual coverage probability exceeds the nominal 95\%.

This is why we call it ``conservative.'' The inference errs on the side
of caution. We're more likely to say ``we're uncertain'' than to make a
confident but wrong claim.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Sharp Null and Constant Effects.} The sharp null hypothesis
  states that \(\tau_i = 0\) for all individuals, while constant effects
  means \(\tau_i = \tau\) for some constant \(\tau\). Explain why both
  assumptions lead to \(S_{tc}^2 = 0\), even though they seem quite
  different.
\end{enumerate}

Solution

Both assumptions lead to \(S_{tc}^2 = 0\) because both eliminate
\emph{variation} in the treatment effects, even though they imply
different average treatment effects.

\(S_{tc}^2\) measures how much the individual treatment effects vary
around their mean (the ATE). The formula is

\[
S_{tc}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (\tau_i - \text{ATE})^2.
\]

Under the \textbf{sharp null}, every \(\tau_i = 0\), so the ATE is also
0. Every deviation \(\tau_i - \text{ATE} = 0 - 0 = 0\), so
\(S_{tc}^2 = 0\).

Under \textbf{constant effects}, every \(\tau_i = \tau\), so the ATE is
also \(\tau\). Every deviation
\(\tau_i - \text{ATE} = \tau - \tau = 0\), so \(S_{tc}^2 = 0\).

The key insight is that \(S_{tc}^2\) measures \emph{heterogeneity} in
treatment effects, not the level of the effect. Both the sharp null
(zero effects for everyone) and constant effects (the same nonzero
effect for everyone) eliminate heterogeneity, just in different ways.

\subsection{Computational Practice}\label{computational-practice-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Computing Sample Variances.} Consider an experiment with 6
  individuals. After randomization, you observe the following.

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Individual & \(D_i\) & \(Y_i^{\text{obs}}\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 1 & 8 \\
  2 & 0 & 4 \\
  3 & 1 & 6 \\
  4 & 0 & 5 \\
  5 & 1 & 10 \\
  6 & 0 & 3 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute the treatment group mean \(\overline{Y}^{\text{obs}}_t\) and
    control group mean \(\overline{Y}^{\text{obs}}_c\).
  \item
    Compute the sample variance in the treatment group, \(s_t^2\).
  \item
    Compute the sample variance in the control group, \(s_c^2\).
  \item
    Compute Neyman's variance estimator
    \(\widehat{\text{Var}}(\widehat{\text{ATE}})\).
  \item
    Compute the estimated standard error of \(\widehat{\text{ATE}}\).
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The treated individuals (1, 3, 5) have outcomes 8, 6,
10. The control individuals (2, 4, 6) have outcomes 4, 5, 3.

\[
\overline{Y}^{\text{obs}}_t = \frac{8 + 6 + 10}{3} = \frac{24}{3} = 8
\]

\[
\overline{Y}^{\text{obs}}_c = \frac{4 + 5 + 3}{3} = \frac{12}{3} = 4
\]

\textbf{Part b.} The sample variance in the treatment group is

\[
s_t^2 = \frac{1}{3-1}\left[(8-8)^2 + (6-8)^2 + (10-8)^2\right] = \frac{0 + 4 + 4}{2} = \frac{8}{2} = 4.
\]

\textbf{Part c.} The sample variance in the control group is

\[
s_c^2 = \frac{1}{3-1}\left[(4-4)^2 + (5-4)^2 + (3-4)^2\right] = \frac{0 + 1 + 1}{2} = \frac{2}{2} = 1.
\]

\textbf{Part d.} With \(N_t = 3\) and \(N_c = 3\), Neyman's variance
estimator is

\[
\widehat{\text{Var}}(\widehat{\text{ATE}}) = \frac{s_t^2}{N_t} + \frac{s_c^2}{N_c} = \frac{4}{3} + \frac{1}{3} = \frac{5}{3} \approx 1.67.
\]

\textbf{Part e.} The estimated standard error is

\[
\widehat{\text{SE}}(\widehat{\text{ATE}}) = \sqrt{\widehat{\text{Var}}(\widehat{\text{ATE}})} = \sqrt{\frac{5}{3}} \approx 1.29.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Comparing Estimates.} Using the data from Exercise 5:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute \(\widehat{\text{ATE}}\).
  \item
    Construct an approximate 95\% confidence interval for the ATE using
    \(\widehat{\text{ATE}} \pm 1.96 \times \widehat{\text{SE}}\).
  \item
    Does this confidence interval include zero? What does this suggest
    about whether the treatment has an effect?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The estimated ATE is

\[
\widehat{\text{ATE}} = \overline{Y}^{\text{obs}}_t - \overline{Y}^{\text{obs}}_c = 8 - 4 = 4.
\]

\textbf{Part b.} Using \(\widehat{\text{SE}} \approx 1.29\) from
Exercise 5, the approximate 95\% confidence interval is

\[
4 \pm 1.96 \times 1.29 = 4 \pm 2.53 = (1.47, 6.53).
\]

\textbf{Part c.} The confidence interval does not include zero. It
ranges from about 1.47 to 6.53, all positive values. This suggests that
the treatment has a positive effect. We can be reasonably confident (at
the 95\% level) that the true ATE is positive.

However, we should be cautious with such a small sample (\(N = 6\)). The
normal approximation that justifies using 1.96 may not be accurate. In
practice, we might use a \(t\)-distribution with fewer degrees of
freedom, which would give a wider interval.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  \textbf{True vs.~Estimated Variance.} Suppose the full potential
  outcomes for the 6 individuals in Exercise 5 are as follows.

  \begin{longtable}[]{@{}llll@{}}
  \toprule\noalign{}
  Individual & \(Y_i(0)\) & \(Y_i(1)\) & \(\tau_i\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 5 & 8 & 3 \\
  2 & 4 & 7 & 3 \\
  3 & 4 & 6 & 2 \\
  4 & 5 & 9 & 4 \\
  5 & 6 & 10 & 4 \\
  6 & 3 & 5 & 2 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute \(S_t^2\), \(S_c^2\), and \(S_{tc}^2\) using the full
    potential outcomes.
  \item
    Compute the true variance of \(\widehat{\text{ATE}}\) using the
    formula with \(N_t = N_c = 3\).
  \item
    Compare the true variance to Neyman's estimate from Exercise 5. Is
    Neyman's estimate conservative, as expected?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} First, compute the means of the potential outcomes.

\[
\overline{Y}(1) = \frac{8 + 7 + 6 + 9 + 10 + 5}{6} = \frac{45}{6} = 7.5
\]

\[
\overline{Y}(0) = \frac{5 + 4 + 4 + 5 + 6 + 3}{6} = \frac{27}{6} = 4.5
\]

For \(S_t^2\), we compute the variance of \(Y_i(1)\).

\[
S_t^2 = \frac{1}{5}\left[(8-7.5)^2 + (7-7.5)^2 + (6-7.5)^2 + (9-7.5)^2 + (10-7.5)^2 + (5-7.5)^2\right]
\]

\[
= \frac{0.25 + 0.25 + 2.25 + 2.25 + 6.25 + 6.25}{5} = \frac{17.5}{5} = 3.5
\]

For \(S_c^2\), we compute the variance of \(Y_i(0)\).

\[
S_c^2 = \frac{1}{5}\left[(5-4.5)^2 + (4-4.5)^2 + (4-4.5)^2 + (5-4.5)^2 + (6-4.5)^2 + (3-4.5)^2\right]
\]

\[
= \frac{0.25 + 0.25 + 0.25 + 0.25 + 2.25 + 2.25}{5} = \frac{5.5}{5} = 1.1
\]

For \(S_{tc}^2\), we first note that the treatment effects are 3, 3, 2,
4, 4, 2, with ATE = 3. Then

\[
S_{tc}^2 = \frac{1}{5}\left[(3-3)^2 + (3-3)^2 + (2-3)^2 + (4-3)^2 + (4-3)^2 + (2-3)^2\right]
\]

\[
= \frac{0 + 0 + 1 + 1 + 1 + 1}{5} = \frac{4}{5} = 0.8
\]

\textbf{Part b.} The true variance is

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N} = \frac{3.5}{3} + \frac{1.1}{3} - \frac{0.8}{6}
\]

\[
= 1.167 + 0.367 - 0.133 = 1.4
\]

\textbf{Part c.} Neyman's estimate from Exercise 5 was approximately
1.67. The true variance is 1.4. Since 1.67 \textgreater{} 1.4, Neyman's
estimate is indeed conservative, as expected. It overestimates the true
variance by about 0.27 (or about 19\%).

The difference is the dropped term \(S_{tc}^2/N = 0.8/6 \approx 0.133\).
Since we drop a positive quantity that's being subtracted, we end up
with a larger estimate.

\subsection{Application to Rasinski's
Experiment}\label{application-to-rasinskis-experiment-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Estimating the Standard Error.} Return to the hypothetical
  Rasinski experiment from the previous chapters. Suppose after
  randomization with \(N_t = 5\) and \(N_c = 5\), you observe the
  following.

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Respondent & \(D_i\) & \(Y_i^{\text{obs}}\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1 & 1 & 1 \\
  2 & 0 & 3 \\
  3 & 1 & 1 \\
  4 & 0 & 3 \\
  5 & 1 & 1 \\
  6 & 0 & 3 \\
  7 & 0 & 2 \\
  8 & 1 & 1 \\
  9 & 0 & 2 \\
  10 & 1 & 2 \\
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Compute \(\widehat{\text{ATE}}\).
  \item
    Compute \(s_t^2\) and \(s_c^2\).
  \item
    Compute Neyman's variance estimator and the estimated standard
    error.
  \item
    Construct an approximate 95\% confidence interval for the ATE. Does
    it include zero?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} The treated individuals (1, 3, 5, 8, 10) have outcomes
1, 1, 1, 1, 2. The control individuals (2, 4, 6, 7, 9) have outcomes 3,
3, 3, 2, 2.

\[
\overline{Y}^{\text{obs}}_t = \frac{1 + 1 + 1 + 1 + 2}{5} = \frac{6}{5} = 1.2
\]

\[
\overline{Y}^{\text{obs}}_c = \frac{3 + 3 + 3 + 2 + 2}{5} = \frac{13}{5} = 2.6
\]

\[
\widehat{\text{ATE}} = 1.2 - 2.6 = -1.4
\]

\textbf{Part b.} For \(s_t^2\), we have

\[
s_t^2 = \frac{1}{4}\left[(1-1.2)^2 + (1-1.2)^2 + (1-1.2)^2 + (1-1.2)^2 + (2-1.2)^2\right]
\]

\[
= \frac{0.04 + 0.04 + 0.04 + 0.04 + 0.64}{4} = \frac{0.8}{4} = 0.2
\]

For \(s_c^2\), we have

\[
s_c^2 = \frac{1}{4}\left[(3-2.6)^2 + (3-2.6)^2 + (3-2.6)^2 + (2-2.6)^2 + (2-2.6)^2\right]
\]

\[
= \frac{0.16 + 0.16 + 0.16 + 0.36 + 0.36}{4} = \frac{1.2}{4} = 0.3
\]

\textbf{Part c.} Neyman's variance estimator is

\[
\widehat{\text{Var}}(\widehat{\text{ATE}}) = \frac{0.2}{5} + \frac{0.3}{5} = 0.04 + 0.06 = 0.1
\]

The estimated standard error is

\[
\widehat{\text{SE}}(\widehat{\text{ATE}}) = \sqrt{0.1} \approx 0.316
\]

\textbf{Part d.} The approximate 95\% confidence interval is

\[
-1.4 \pm 1.96 \times 0.316 = -1.4 \pm 0.62 = (-2.02, -0.78)
\]

The confidence interval does not include zero. The entire interval is
negative, suggesting that the ``welfare'' wording (treatment) leads to
lower support than the ``assistance to the poor'' wording (control).
This is consistent with Rasinski's finding that question wording affects
measured public opinion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  \textbf{Interpreting the Results.} Based on your calculations in
  Exercise 8, write a one-paragraph interpretation of the results that
  would be understandable to someone who has not taken a statistics
  course. Be sure to explain what the estimate, standard error, and
  confidence interval tell us about the effect of question wording.
\end{enumerate}

Solution

In this experiment, we asked half of the respondents about ``welfare''
and the other half about ``assistance to the poor.'' On a scale where
higher numbers mean more support, respondents who heard ``welfare'' gave
an average rating of 1.2, while those who heard ``assistance to the
poor'' gave an average rating of 2.6. The difference is -1.4 points,
meaning that using the word ``welfare'' reduced support by about 1.4
points on average.

How confident should we be in this finding? The standard error of 0.32
tells us that if we repeated this experiment many times with different
random assignments, we'd expect our estimates to typically vary by about
a third of a point from one experiment to another. The 95\% confidence
interval runs from -2.02 to -0.78. This means we can be fairly confident
that the true effect is negative and somewhere in this range.
Importantly, the confidence interval doesn't include zero, which
suggests the effect is real, not just random noise.

In plain terms, this experiment provides evidence that how you ask the
question matters. Calling a program ``welfare'' versus ``assistance to
the poor'' changes how favorably people respond, even though both
phrases describe the same thing.

\subsection{Critical Thinking}\label{critical-thinking-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  \textbf{When Conservation Matters.} Consider two scenarios.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    You're testing a new drug and want to determine if it has \emph{any}
    effect (positive or negative). Is using a conservative variance
    estimator problematic here? Why or why not?
  \item
    You're evaluating a policy and want to determine if it has a
    \emph{large enough} positive effect to justify its cost. Is using a
    conservative variance estimator problematic here? Why or why not?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} When testing whether a drug has any effect, using a
conservative variance estimator is not very problematic. If anything, it
makes you more cautious. A conservative estimator leads to wider
confidence intervals and higher p-values. This means you're less likely
to claim an effect exists when it doesn't (Type I error). You might fail
to detect a real effect (Type II error), but you're less likely to make
false positive claims.

In drug testing, this caution is often appropriate. The cost of claiming
a drug works when it doesn't (leading to widespread use of an
ineffective or harmful drug) is usually greater than the cost of failing
to detect a genuine effect (which can be addressed with follow-up
studies).

\textbf{Part b.} When evaluating whether a policy effect is large enough
to justify costs, conservative variance estimation can be more
problematic. A conservative estimator makes your confidence interval
wider, which pushes the lower bound of the interval closer to zero (or
even negative). This makes it harder to demonstrate that the effect is
``large enough.''

Suppose the true effect is exactly large enough to justify the policy.
With a conservative variance estimator, you might conclude that the
effect is uncertain and could be too small, leading you to reject a
worthwhile policy. This is a Type II error, and it's more costly in
contexts where the bar for action is ``is the effect big enough?''
rather than ``is there any effect at all?''

In cost-benefit analysis contexts, researchers sometimes prefer more
precise variance estimators or use other methods to avoid being overly
conservative.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  \textbf{Sample Size and the Dropped Term.} The term we drop from the
  variance formula is \(S_{tc}^2/N\). As sample size \(N\) increases,
  what happens to this term? What does this imply about the bias of
  Neyman's estimator for large samples versus small samples?
\end{enumerate}

Solution

As \(N\) increases, the term \(S_{tc}^2/N\) decreases. Since
\(S_{tc}^2\) is a fixed population quantity (it doesn't depend on sample
size), dividing by a larger \(N\) makes this term smaller.

This implies that \textbf{Neyman's estimator becomes less biased for
larger samples}. For small \(N\), the dropped term \(S_{tc}^2/N\) can be
relatively large compared to the other terms, so the conservative bias
is more pronounced. For large \(N\), the dropped term approaches zero,
and Neyman's estimator is nearly unbiased.

To see this more concretely, consider that the relative bias is
approximately

\[
\frac{\text{Bias}}{\text{True Variance}} = \frac{S_{tc}^2/N}{\frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}}.
\]

For large \(N\) (with fixed proportions \(N_t/N\) and \(N_c/N\)), all
terms shrink proportionally with \(1/N\), so the relative bias stays
roughly constant. But the \emph{absolute} bias (the dropped term itself)
goes to zero.

In practice, for large experiments, the conservative bias is typically
negligible. For small experiments, it can matter more, though the
direction of the bias (overestimating variance) is still ``safe'' for
inference.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\item
  \textbf{Heterogeneous Effects and Bias.} Suppose you run an experiment
  and find that \(\widehat{\text{ATE}} = 5\) with Neyman's standard
  error of 2. A colleague points out that treatment effects might be
  heterogeneous, meaning \(S_{tc}^2 > 0\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Does the possibility of heterogeneous effects change your point
    estimate of the ATE? Why or why not?
  \item
    Does it change your confidence interval? If so, in what direction?
  \item
    What would you say to your colleague about the implications of
    heterogeneous effects for your conclusions?
  \end{enumerate}
\end{enumerate}

Solution

\textbf{Part a.} No, the possibility of heterogeneous effects does not
change the point estimate. The difference-in-means estimator
\(\widehat{\text{ATE}}\) is unbiased regardless of whether treatment
effects are homogeneous or heterogeneous. The estimate of 5 is our best
guess of the average treatment effect either way.

\textbf{Part b.} Yes, it could change the confidence interval, but only
to make it \emph{narrower}, not wider. The true variance is

\[
\text{Var}_D(\widehat{\text{ATE}}) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}.
\]

If \(S_{tc}^2 > 0\), then the true variance is smaller than what
Neyman's estimator suggests. Our standard error of 2 is an overestimate,
and our confidence interval is wider than necessary.

\textbf{Part c.} I would tell my colleague the following. ``Good point!
If treatment effects are heterogeneous, then our standard error is
conservative, which means our confidence interval is wider than it needs
to be. This makes our inference cautious but still valid. Our confidence
interval will cover the true ATE at least 95\% of the time, possibly
more.

The key point is that heterogeneous effects don't threaten the validity
of our inference. They just make it more conservative. If we found a
significant effect with a conservative standard error, we can be even
more confident that the effect is real. If we failed to find
significance, it's possible we would have found it with a more precise
variance estimator, but we can't know for sure without more information
about \(S_{tc}^2\), which is fundamentally unobservable.''

\subsection{R Practice}\label{r-practice-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  \textbf{Computing with R.} Write R code to compute Neyman's variance
  estimator and the estimated standard error from the data in Exercise
  5. Verify that your code produces the same answers.
\end{enumerate}

Solution

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create the data from Exercise 5}
\NormalTok{exercise\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{individual =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,}
  \AttributeTok{D =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
  \AttributeTok{Y\_obs =} \FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# view the data}
\NormalTok{exercise\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  individual     D Y_obs
       <int> <dbl> <dbl>
1          1     1     8
2          2     0     4
3          3     1     6
4          4     0     5
5          5     1    10
6          6     0     3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute sample variances}
\NormalTok{s\_t\_sq }\OtherTok{\textless{}{-}}\NormalTok{ exercise\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{var}\NormalTok{()}

\NormalTok{s\_c\_sq }\OtherTok{\textless{}{-}}\NormalTok{ exercise\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{var}\NormalTok{()}

\CommentTok{\# display sample variances}
\FunctionTok{cat}\NormalTok{(}\StringTok{"s\_t\^{}2 ="}\NormalTok{, s\_t\_sq, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
s_t^2 = 4 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"s\_c\^{}2 ="}\NormalTok{, s\_c\_sq, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
s_c^2 = 1 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute group sizes}
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(exercise\_data}\SpecialCharTok{$}\NormalTok{D)}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ exercise\_data}\SpecialCharTok{$}\NormalTok{D)}

\CommentTok{\# Neyman\textquotesingle{}s variance estimator}
\NormalTok{var\_hat }\OtherTok{\textless{}{-}}\NormalTok{ s\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ s\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Variance estimate ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(var\_hat, }\DecValTok{3}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Variance estimate = 1.667 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# standard error}
\NormalTok{se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(var\_hat)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Standard error ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(se\_hat, }\DecValTok{3}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Standard error = 1.291 
\end{verbatim}

The code produces \(s_t^2 = 4\), \(s_c^2 = 1\), variance estimate
\(\approx 1.667\), and standard error \(\approx 1.291\), which match our
hand calculations in Exercise 5.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{13}
\tightlist
\item
  \textbf{Confidence Interval in R.} Write R code that takes data in the
  format of Exercise 5 and computes a 95\% confidence interval for the
  ATE. Apply your code to verify your answer to Exercise 6.
\end{enumerate}

Solution

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function to compute 95\% confidence interval}
\NormalTok{compute\_ci }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  \CommentTok{\# compute group means}
\NormalTok{  Y\_bar\_t }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{()}
\NormalTok{  Y\_bar\_c }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{()}

  \CommentTok{\# compute ATE estimate}
\NormalTok{  ate\_hat }\OtherTok{\textless{}{-}}\NormalTok{ Y\_bar\_t }\SpecialCharTok{{-}}\NormalTok{ Y\_bar\_c}

  \CommentTok{\# compute sample variances}
\NormalTok{  s\_t\_sq }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{var}\NormalTok{()}
\NormalTok{  s\_c\_sq }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{var}\NormalTok{()}

  \CommentTok{\# compute group sizes}
\NormalTok{  N\_t }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{D)}
\NormalTok{  N\_c }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{D)}

  \CommentTok{\# Neyman\textquotesingle{}s variance estimator and standard error}
\NormalTok{  var\_hat }\OtherTok{\textless{}{-}}\NormalTok{ s\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ s\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c}
\NormalTok{  se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(var\_hat)}

  \CommentTok{\# 95\% confidence interval}
\NormalTok{  ci\_lower }\OtherTok{\textless{}{-}}\NormalTok{ ate\_hat }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_hat}
\NormalTok{  ci\_upper }\OtherTok{\textless{}{-}}\NormalTok{ ate\_hat }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_hat}

  \CommentTok{\# return results}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{ate\_hat =}\NormalTok{ ate\_hat,}
    \AttributeTok{se\_hat =}\NormalTok{ se\_hat,}
    \AttributeTok{ci\_lower =}\NormalTok{ ci\_lower,}
    \AttributeTok{ci\_upper =}\NormalTok{ ci\_upper}
\NormalTok{  )}
\NormalTok{\}}

\CommentTok{\# apply to Exercise 5 data}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{compute\_ci}\NormalTok{(exercise\_data)}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
  ate_hat se_hat ci_lower ci_upper
    <dbl>  <dbl>    <dbl>    <dbl>
1       4   1.29     1.47     6.53
\end{verbatim}

The code produces \(\widehat{\text{ATE}} = 4\), standard error
\(\approx 1.29\), and 95\% CI of approximately \((1.47, 6.53)\). This
matches our calculations in Exercise 6.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{14}
\item
  \textbf{Using HC2 Standard Errors.} The chapter shows that HC2 robust
  standard errors from \texttt{lm()} are equivalent to Neyman's variance
  estimator. Using the data from Exercise 5, write R code that:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Fits a regression of \texttt{Y\_obs} on \texttt{D} using
    \texttt{lm()}.
  \item
    Uses the \texttt{sandwich} and \texttt{lmtest} packages to compute
    HC2 standard errors.
  \item
    Verifies that the HC2 standard error for the treatment coefficient
    matches the Neyman standard error you computed in Exercise 13.
  \end{enumerate}
\end{enumerate}

Solution

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}

\CommentTok{\# data from Exercise 5}
\NormalTok{exercise\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{individual =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,}
  \AttributeTok{D =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
  \AttributeTok{Y\_obs =} \FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Part a: fit the regression}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_obs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ D, }\AttributeTok{data =}\NormalTok{ exercise\_data)}

\CommentTok{\# Part b: compute HC2 standard errors}
\NormalTok{var\_hat\_hc2 }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"HC2"}\NormalTok{)}
\FunctionTok{coeftest}\NormalTok{(fit, }\AttributeTok{vcov. =}\NormalTok{ var\_hat\_hc2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

t test of coefficients:

            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  4.00000    0.57735  6.9282 0.002278 **
D            4.00000    1.29099  3.0984 0.036278 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Part c: extract the HC2 standard error for the D coefficient}
\NormalTok{se\_hc2 }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(var\_hat\_hc2)[}\DecValTok{2}\NormalTok{])}
\FunctionTok{cat}\NormalTok{(}\StringTok{"HC2 standard error:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(se\_hc2, }\DecValTok{3}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
HC2 standard error: 1.291 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compare to Neyman SE computed manually}
\NormalTok{s\_t\_sq }\OtherTok{\textless{}{-}}\NormalTok{ exercise\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{var}\NormalTok{()}
\NormalTok{s\_c\_sq }\OtherTok{\textless{}{-}}\NormalTok{ exercise\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(D }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(Y\_obs) }\SpecialCharTok{|\textgreater{}} \FunctionTok{var}\NormalTok{()}
\NormalTok{N\_t }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(exercise\_data}\SpecialCharTok{$}\NormalTok{D)}
\NormalTok{N\_c }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ exercise\_data}\SpecialCharTok{$}\NormalTok{D)}
\NormalTok{se\_neyman }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(s\_t\_sq }\SpecialCharTok{/}\NormalTok{ N\_t }\SpecialCharTok{+}\NormalTok{ s\_c\_sq }\SpecialCharTok{/}\NormalTok{ N\_c)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Neyman standard error:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(se\_neyman, }\DecValTok{3}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Neyman standard error: 1.291 
\end{verbatim}

The HC2 standard error equals the Neyman standard error (both
approximately 1.291). This confirms the result from Samii and Aronow
(2012) that HC2 robust standard errors are exactly equivalent to
Neyman's variance estimator for completely randomized experiments.

The coefficient on \texttt{D} (which equals 4) is the
\(\widehat{\text{ATE}}\), matching our earlier calculations. The
\texttt{coeftest()} output also provides a t-statistic and p-value,
which we could use for hypothesis testing.

\part{Appendices}

\chapter{Appendix A: Proof of the Variance
Formula}\label{appendix-a-proof-of-the-variance-formula}

This appendix provides a complete proof of the variance formula for the
difference-in-means estimator, stated in the chapter on the variance of
\(\widehat{\text{ATE}}\). The proof involves only algebra, but there are
many steps and some tricks are helpful.

\section{Setting Up the Problem}\label{setting-up-the-problem}

Recall from the previous chapter that we can write the
difference-in-means estimator as

\[
\widehat{\text{ATE}} = \frac{1}{N_t} \sum_{i=1}^{N} D_i \cdot Y^{\text{obs}}_i - \frac{1}{N_c} \sum_{i=1}^{N} (1 - D_i) \cdot Y^{\text{obs}}_i,
\]

where \(D_i\) is the treatment assignment indicator for individual \(i\)
(1 if treated, 0 if control).

To compute the variance, we need to understand how this estimator
behaves across all possible randomizations. The subscript \(D\) in
\(\text{Var}_D(\cdot)\) and \(E_D[\cdot]\) reminds us that the
randomness comes entirely from the treatment assignment \(D_i\).

\section{\texorpdfstring{The Variance Operator
\(\text{Var}[\cdot]\)}{The Variance Operator \textbackslash text\{Var\}{[}\textbackslash cdot{]}}}\label{the-variance-operator-textvarcdot-1}

The variance operator \(\text{Var}[\cdot]\) is a way of measuring ``how
spread out'' a random variable is around its average value. The variance
of a random variable measures its typical \emph{squared} distance from
the mean. If we take the You can think of \(\text{Var}(X)\) as asking:
``If I could repeat this random process infinitely many times, how far
would \(X\) typically be from its average?''

\end{tcolorbox}

For our purposes, we need five rules about variance. These rules play
the same role that the expectation rules played in the previous chapter.
They tell us which algebraic moves are allowed.

\textbf{Rule 1: Variance is defined as the expected squared deviation
from the mean.}

If \(X\) is random, then

\[
\text{Var}(X) = E\left[(X - E[X])^2\right].
\]

We square the deviation so that negative and positive deviations do not
cancel out. If on average you earn \$100 per day, but some days you earn
\$80 and some days you earn \$120, the deviations are \(-20\) and
\(+20\). Without squaring, they would cancel to zero, hiding the
variability.

\textbf{Rule 2: Adding a constant does not change variance.}

If \(c\) is a constant, then \(\text{Var}(X + c) = \text{Var}(X)\).

Shifting a random variable up or down changes its average value, but
does not change how spread out it is. If your daily earnings go from
averaging \$100 to averaging \$200 (because of a \$100 raise), the
\emph{spread} around that average stays the same.

\textbf{Rule 3: Multiplying by a constant scales variance by the
square.}

If \(c\) is a constant, then \(\text{Var}(cX) = c^2 \text{Var}(X)\).

This rule often surprises people because it differs from expectation.
For expectation, constants come out unchanged
(\(E[cX] = c \cdot E[X]\)). For variance, constants come out
\emph{squared}. If your earnings double, the \emph{spread} of your
earnings quadruples.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Rule}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

Why does the constant get squared? Variance measures squared deviations.
If we double \(X\), the deviations from the mean also double. But since
we square those deviations, doubling the deviation means quadrupling the
squared deviation.

\textbf{Example.} Suppose \(X\) can be either 1 or 3, each with
probability 1/2. Then \(E[X] = 2\), and

\[
\text{Var}(X) = \frac{1}{2}(1 - 2)^2 + \frac{1}{2}(3 - 2)^2 = \frac{1}{2}(1) + \frac{1}{2}(1) = 1.
\]

Now consider \(2X\), which can be either 2 or 6. Then \(E[2X] = 4\), and

\[
\text{Var}(2X) = \frac{1}{2}(2 - 4)^2 + \frac{1}{2}(6 - 4)^2 = \frac{1}{2}(4) + \frac{1}{2}(4) = 4.
\]

The variance is \(4 = 2^2 \times 1 = c^2 \times \text{Var}(X)\).

\end{tcolorbox}

\textbf{Rule 4: The variance of a sum includes cross-terms.}

If \(X\) and \(Y\) are random, then

\[
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y),
\]

where \(\text{Cov}(X, Y)\) is the \textbf{covariance} between \(X\) and
\(Y\). The covariance measures how \(X\) and \(Y\) move together. If
\(X\) and \(Y\) tend to be high at the same time, their covariance is
positive. If one tends to be high when the other is low, their
covariance is negative.

This rule matters because our estimator is built from sums. When we take
the variance of a sum, we do not simply get ``variance of the first part
plus variance of the second part.'' We also get cross-terms.

In our setting, these cross-terms are especially important. Under
completely randomized assignment, treatment indicators for different
individuals are \emph{not} independent. If one individual is assigned to
treatment, that makes it slightly less likely that another individual is
assigned to treatment (because there are only \(N_t\) slots). This
creates negative covariance between treatment assignments.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding Where the Covariance Comes From}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The covariance term appears because we square a sum. Think back to
algebra: \((a + b)^2 = a^2 + 2ab + b^2\). The ``cross-term'' \(2ab\) is
what becomes the covariance.

Start from the definition, so that

\[
\text{Var}(X + Y) = E\left[\left((X + Y) - E[X + Y]\right)^2\right].
\]

Rewrite the mean as \(E[X + Y] = E[X] + E[Y]\), so that

\[
\text{Var}(X + Y) = E\left[\left((X - E[X]) + (Y - E[Y])\right)^2\right].
\]

Expand the square, which gives

\[
\text{Var}(X + Y) =
E\left[(X - E[X])^2\right]
+ E\left[(Y - E[Y])^2\right]
+ 2E\left[(X - E[X])(Y - E[Y])\right].
\]

The first term is \(\text{Var}(X)\). The second term is
\(\text{Var}(Y)\). The last term is \(2\text{Cov}(X, Y)\) by definition.

\textbf{Special case: independence.} If \(X\) and \(Y\) are independent,
their covariance is zero, and the variance of the sum is just the sum of
the variances. But in our setting, treatment assignments are \emph{not}
independent, so we cannot ignore the covariance.

\end{tcolorbox}

\textbf{Rule 5: If the mean is zero, variance equals the expected
square.}

If \(E[X] = 0\), then

\[
\text{Var}(X) = E[X^2].
\]

This rule is a useful shortcut. By definition,
\(\text{Var}(X) = E[X^2] - (E[X])^2\). If \(E[X] = 0\), the second term
vanishes, and we are left with just \(E[X^2]\).

We will use this simplification later by rewriting expressions so that
they have mean zero.

\section{Some Preliminary Results}\label{some-preliminary-results}

Before tackling the variance formula, we need to establish some basic
facts about the treatment indicators \(D_i\) under completely randomized
assignment.

\subsection{\texorpdfstring{Facts About
\(D_i\)}{Facts About D\_i}}\label{facts-about-d_i}

In a completely randomized experiment with \(N\) individuals where
\(N_t\) are assigned to treatment, each individual has the same
probability of being treated. We have

\[
E_D[D_i] = \Pr(D_i = 1) = \frac{N_t}{N}.
\]

Since \(D_i\) is binary (either 0 or 1), we also have \(D_i^2 = D_i\)
(squaring a 0 or 1 gives you the same number). This means

\[
E_D[D_i^2] = E_D[D_i] = \frac{N_t}{N}.
\]

From these facts, we can compute the variance of \(D_i\), so that

\[
\text{Var}_D(D_i) = E_D[D_i^2] - (E_D[D_i])^2 = \frac{N_t}{N} - \left(\frac{N_t}{N}\right)^2 = \frac{N_t}{N} \left( 1 - \frac{N_t}{N} \right) = \frac{N_t \cdot N_c}{N^2}.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Calculation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The variance formula \(\text{Var}(X) = E[X^2] - (E[X])^2\) is a standard
result. Let's verify it with a concrete example.

Suppose \(N = 10\) and \(N_t = 4\). Then \(E_D[D_i] = 4/10 = 0.4\). The
variance is

\[
\text{Var}_D(D_i) = \frac{4}{10} \left(1 - \frac{4}{10}\right) = 0.4 \times 0.6 = 0.24.
\]

This is the variance of a Bernoulli random variable with probability
\(p = 0.4\), which is \(p(1-p) = 0.24\).

\end{tcolorbox}

\subsection{The Correlation Between Treatment
Assignments}\label{the-correlation-between-treatment-assignments}

Here's where things get interesting. In a completely randomized
experiment, the treatment assignments \(D_i\) and \(D_j\) for different
individuals are \emph{not} independent. If one individual is assigned to
treatment, that makes it slightly less likely that another individual is
assigned to treatment (because there are only \(N_t\) treatment slots to
fill).

For \(i \neq j\), we need to compute \(E_D[D_i \cdot D_j]\). This equals
the probability that \emph{both} individuals \(i\) and \(j\) are
assigned to treatment.

We can compute this using conditional probability, so that

\[
E_D[D_i \cdot D_j] = \Pr(D_i = 1) \cdot \Pr(D_j = 1 \mid D_i = 1) = \frac{N_t}{N} \cdot \frac{N_t - 1}{N - 1}.
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Calculation}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

The probability that individual \(i\) is treated is \(N_t/N\). Given
that individual \(i\) is treated, there are now \(N_t - 1\) remaining
treatment slots and \(N - 1\) remaining individuals. So the probability
that individual \(j\) is also treated is \((N_t - 1)/(N - 1)\).

\textbf{Example.} Suppose \(N = 5\) and \(N_t = 2\). The probability
both individuals 1 and 2 are treated is

\[
\frac{2}{5} \cdot \frac{1}{4} = \frac{2}{20} = 0.1.
\]

We can verify this by counting. There are \(\binom{5}{2} = 10\) ways to
choose 2 individuals for treatment. Only 1 of these ways assigns both
individuals 1 and 2 to treatment. So the probability is \(1/10 = 0.1\).

\end{tcolorbox}

\section{A Helpful Transformation}\label{a-helpful-transformation}

The algebra becomes cleaner if we transform our treatment indicator.
Instead of working directly with \(D_i\), we define a \textbf{centered
treatment indicator} \(\tilde{D}_i\), so that

\[
\tilde{D}_i = D_i - \frac{N_t}{N}.
\]

This transformation shifts \(D_i\) so that its mean is zero. When
\(D_i = 1\) (treated), we have \(\tilde{D}_i = 1 - N_t/N = N_c/N\). When
\(D_i = 0\) (control), we have \(\tilde{D}_i = 0 - N_t/N = -N_t/N\).

In summary, the centered indicator is

\[
\tilde{D}_i = \begin{cases}
\frac{N_c}{N} & \text{if } D_i = 1 \text{ (treatment)}, \\[6pt]
-\frac{N_t}{N} & \text{if } D_i = 0 \text{ (control)}.
\end{cases}
\]

\subsection{\texorpdfstring{Properties of
\(\tilde{D}_i\)}{Properties of \textbackslash tilde\{D\}\_i}}\label{properties-of-tilded_i}

The centered indicator \(\tilde{D}_i\) has two key properties that make
it useful.

\textbf{Property 1: Zero mean.} By construction,
\(E_D[\tilde{D}_i] = E_D[D_i] - N_t/N = N_t/N - N_t/N = 0\).

\textbf{Property 2: Known variance.} Since \(\tilde{D}_i\) is just a
shifted version of \(D_i\), its variance is the same, so that

\[
\text{Var}_D(\tilde{D}_i) = \text{Var}_D(D_i) = \frac{N_t \cdot N_c}{N^2}.
\]

And because \(E_D[\tilde{D}_i] = 0\), we have
\(E_D[\tilde{D}_i^2] = \text{Var}_D(\tilde{D}_i) = \frac{N_t \cdot N_c}{N^2}\).

\subsection{\texorpdfstring{The Cross-Product
\(\tilde{D}_i \cdot \tilde{D}_j\)}{The Cross-Product \textbackslash tilde\{D\}\_i \textbackslash cdot \textbackslash tilde\{D\}\_j}}\label{the-cross-product-tilded_i-cdot-tilded_j}

For the variance calculation, we also need
\(E_D[\tilde{D}_i \cdot \tilde{D}_j]\) when \(i \neq j\). The product
\(\tilde{D}_i \cdot \tilde{D}_j\) can take three possible values
depending on whether both individuals are treated, both are in control,
or one of each.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(D_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(D_j\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tilde{D}_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tilde{D}_j\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\tilde{D}_i \cdot \tilde{D}_j\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & \(N_c/N\) & \(N_c/N\) & \(N_c^2/N^2\) \\
1 & 0 & \(N_c/N\) & \(-N_t/N\) & \(-N_t N_c/N^2\) \\
0 & 1 & \(-N_t/N\) & \(N_c/N\) & \(-N_t N_c/N^2\) \\
0 & 0 & \(-N_t/N\) & \(-N_t/N\) & \(N_t^2/N^2\) \\
\end{longtable}

The probabilities of each case are:

\begin{itemize}
\tightlist
\item
  Both treated: \(\frac{N_t(N_t - 1)}{N(N-1)}\)
\item
  Both control: \(\frac{N_c(N_c - 1)}{N(N-1)}\)
\item
  One each: \(\frac{2 N_t N_c}{N(N-1)}\)
\end{itemize}

After computing the expected value (a homework exercise), we obtain

\[
E_D[\tilde{D}_i \cdot \tilde{D}_j] = -\frac{N_t \cdot N_c}{N^2 \cdot (N - 1)} \quad \text{for } i \neq j.
\]

Notice this is \emph{negative}. This reflects the negative correlation
between treatment assignments: if individual \(i\) is more likely to be
treated (higher \(\tilde{D}_i\)), individual \(j\) is slightly less
likely to be treated (lower \(\tilde{D}_j\)).

\section{Rewriting the Estimator}\label{rewriting-the-estimator}

Using the centered indicator \(\tilde{D}_i\), we can rewrite the
estimator in a form that separates the fixed and random components.
After algebraic manipulation (shown in the proof below), we obtain

\[
\widehat{\text{ATE}} = \underbrace{\frac{1}{N} \sum_{i=1}^{N} (Y_i(1) - Y_i(0))}_{\text{fixed: the true ATE}} + \underbrace{\frac{1}{N} \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+}_{\text{random: depends on } \tilde{D}_i},
\]

where we define

\[
Y_i^+ = \frac{N}{N_t} Y_i(1) + \frac{N}{N_c} Y_i(0).
\]

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Understanding This Decomposition}, breakable, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, coltitle=black, leftrule=.75mm, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, toprule=.15mm, arc=.35mm, bottomtitle=1mm, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, colback=white]

This decomposition is powerful. It says that our estimate equals the
true ATE plus a ``noise'' term that depends on the randomization.

The first term, \(\frac{1}{N} \sum_{i=1}^{N} (Y_i(1) - Y_i(0))\), is the
true ATE. It doesn't depend on the randomization at all---it's the same
no matter which individuals we assign to treatment.

The second term, \(\frac{1}{N} \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+\),
is the deviation of our estimate from the truth. This term:

\begin{itemize}
\tightlist
\item
  Has expected value zero (because \(E_D[\tilde{D}_i] = 0\))
\item
  Varies depending on the randomization
\item
  Has variance that we want to compute
\end{itemize}

The quantity \(Y_i^+\) is a weighted combination of individual \(i\)'s
potential outcomes. It captures how much that individual's outcomes
contribute to the variability of the estimator.

\end{tcolorbox}

This decomposition immediately shows that \(\widehat{\text{ATE}}\) is
unbiased. Since \(E_D[\tilde{D}_i] = 0\), we have

\[
E_D[\widehat{\text{ATE}}] = \frac{1}{N} \sum_{i=1}^{N} (Y_i(1) - Y_i(0)) + 0 = \text{ATE}.
\]

\section{The Proof}\label{the-proof-1}

The proof involves careful algebraic manipulation. We break it into
labeled steps.

\textbf{Step 1: Start with the decomposition.}

We established that

\[
\widehat{\text{ATE}} = \frac{1}{N} \sum_{i=1}^{N} (Y_i(1) - Y_i(0)) + \frac{1}{N} \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+.
\]

Since the first term is constant (not random), the variance is

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = \text{Var}_D \left( \frac{1}{N} \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+ \right) = \frac{1}{N^2} \text{Var}_D \left( \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+ \right).
\]

\textbf{Step 2: Use the variance-of-a-sum formula.}

Since \(E_D\left[\sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+\right] = 0\), we
have

\[
\text{Var}_D \left( \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+ \right) = E_D \left[ \left( \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+ \right)^2 \right].
\]

\textbf{Step 3: Expand the square.}

Expanding the squared sum gives

\[
E_D \left[ \left( \sum_{i=1}^{N} \tilde{D}_i \cdot Y_i^+ \right)^2 \right] = E_D \left[ \sum_{i=1}^{N} \sum_{j=1}^{N} \tilde{D}_i \tilde{D}_j Y_i^+ Y_j^+ \right] = \sum_{i=1}^{N} \sum_{j=1}^{N} E_D[\tilde{D}_i \tilde{D}_j] \cdot Y_i^+ \cdot Y_j^+.
\]

We can separate this into diagonal terms (\(i = j\)) and off-diagonal
terms (\(i \neq j\)), so that

\[
= \sum_{i=1}^{N} E_D[\tilde{D}_i^2] \cdot (Y_i^+)^2 + \sum_{i=1}^{N} \sum_{j \neq i} E_D[\tilde{D}_i \tilde{D}_j] \cdot Y_i^+ \cdot Y_j^+.
\]

\textbf{Step 4: Substitute the expectations.}

Using our earlier results:

\begin{itemize}
\tightlist
\item
  \(E_D[\tilde{D}_i^2] = \frac{N_t N_c}{N^2}\)
\item
  \(E_D[\tilde{D}_i \tilde{D}_j] = -\frac{N_t N_c}{N^2(N-1)}\) for
  \(i \neq j\)
\end{itemize}

This gives

\[
= \frac{N_t N_c}{N^2} \sum_{i=1}^{N} (Y_i^+)^2 - \frac{N_t N_c}{N^2(N-1)} \sum_{i=1}^{N} \sum_{j \neq i} Y_i^+ Y_j^+.
\]

\textbf{Step 5: Simplify using a variance identity.}

There's a useful algebraic identity: for any values \(z_1, \ldots, z_N\)
with mean \(\bar{z}\),

\[
(N-1) \sum_{i=1}^{N} (z_i - \bar{z})^2 = N \sum_{i=1}^{N} z_i^2 - \sum_{i=1}^{N} \sum_{j \neq i} z_i z_j.
\]

Applying this identity (with some additional algebra that we omit), we
can show that

\[
\sum_{i=1}^{N} (Y_i^+)^2 - \frac{1}{N-1} \sum_{i=1}^{N} \sum_{j \neq i} Y_i^+ Y_j^+ = \frac{N}{N-1} \sum_{i=1}^{N} (Y_i^+ - \overline{Y}^+)^2.
\]

\textbf{Step 6: Substitute the definition of \(Y_i^+\).}

After substitution and considerable algebraic manipulation, we can show
that

\[
\frac{1}{N-1} \sum_{i=1}^{N} (Y_i^+ - \overline{Y}^+)^2 = \frac{N^2}{N_t^2} S_t^2 + \frac{N^2}{N_c^2} S_c^2 + \frac{2N^2}{N_t N_c} \text{Cov}(Y_i(1), Y_i(0)),
\]

where \(\text{Cov}(Y_i(1), Y_i(0))\) is the covariance between the
potential outcomes.

\textbf{Step 7: Relate to \(S_{tc}^2\).}

The variance of treatment effects can be written as

\[
S_{tc}^2 = S_t^2 + S_c^2 - 2 \text{Cov}(Y_i(1), Y_i(0)).
\]

This allows us to substitute for the covariance term.

\textbf{Step 8: Combine and simplify.}

Putting everything together and simplifying (which involves careful
bookkeeping of the various terms), we arrive at

\[
\text{Var}_D \left(\widehat{\text{ATE}}\right) = \frac{S_t^2}{N_t} + \frac{S_c^2}{N_c} - \frac{S_{tc}^2}{N}.
\]

This completes the proof. \(\square\)

\end{tcolorbox}

\end{tcolorbox}

\end{tcolorbox}

\end{footnotesize}}

\end{footnotesize}}

\end{tcolorbox}

\end{footnotesize}}

\end{tcolorbox}

\end{tcolorbox}

\end{footnotesize}}

\end{tcolorbox}

\end{tcolorbox}

\end{footnotesize}}

\end{tcolorbox}

\end{footnotesize}}

\end{footnotesize}}

\end{tcolorbox}

\end{footnotesize}}

\end{tcolorbox}

\end{tcolorbox}

\end{tcolorbox}

\end{tcolorbox}

\end{footnotesize}}

\end{tcolorbox}

\footnotetext{Respondents were randomly assigned to one of two
conditions that asked about spending on programs for low-income
individuals, but used different labels to describe these programs. The
experiment was conducted across three years (1984, 1985, and 1986) and
consistently found that far more respondents said the government was
spending ``too little'' when the program was described as ``assistance
to the poor'' (around 63-65\%) compared to when it was described as
``welfare'' (around 20-25\%). This represents a dramatic difference of
approximately 40 percentage points due solely to the choice of label
used to describe the same underlying government program.}

\footnotetext{Remember that ``long run'' means that we are repeatedly
re-randomizing many, many times. Each randomization gives us a different
collection of observed outcomes (i.e., one per respondent).}

\footnotetext{Remember all that changes across these many repetitions is
\emph{who is assigned to treatment and control}. Every randomization
produces a slightly different treatment group and a slightly different
control group, and therefore a slightly different estimate of the ATE.}

\footnotetext{To make this even more concrete, we might imagine this
person's potential outcomes are among the largest under treatment and
among the smallest under control.}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-Neyman1923}
Neyman, Jerzy. 1923. {``On the Application of Probability Theory to
Agricultural Experiments: Essay on Principles, Section 9.''}
\emph{Statistical Science} 5 (4): 465--72.

\bibitem[\citeproctext]{ref-rasinski1989}
Rasinski, Kenneth A. 1989. {``The Effect of Question Wording on Public
Support for Government Spending.''} \emph{Public Opinion Quarterly} 53
(3): 388--94.

\bibitem[\citeproctext]{ref-SamiiAronow2012}
Samii, Cyrus, and Peter M. Aronow. 2012. {``On Equivalencies Between
Design-Based and Regression-Based Variance Estimators for Randomized
Experiments.''} \emph{Statistics and Probability Letters} 82 (2):
365--70.

\end{CSLReferences}


\backmatter


\end{document}
